<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>视觉SLAM十四讲 第三章</title>
    <url>/2020/04/29/%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    <content><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2>
<p>作者： 高翔 | 张涛</p>
<h3 id="第三章-三维空间刚体运动">第三章 三维空间刚体运动</h3>
<p>本章将介绍SLAM的基本问题之一：<strong>一个刚体在三维空间中的运动是如何描述的</strong>。</p>
<h4 id="旋转矩阵">3.1 旋转矩阵</h4>
<h5 id="点和向量坐标系">3.1.1 点和向量，坐标系</h5>
<p>　　向量：它是线性空间中的一个元素，可以把它想象成从原点指向某处的一个箭头。对于<span class="math inline">\(a,b∈R^3\)</span>，内积可以写为：<span class="math display">\[a·b=a^Tb=\sum_{i=1}^3a_ib_i=|a||b|cos&lt;a,b&gt;\]</span> 而外积可以写成：<span class="math display">\[a×b=a\hat{}b=|a||b|sin&lt;a,b&gt;\]</span> 外积只对三维向量存在定义，我们还能用外积表示向量的旋转。</p>
<h5 id="坐标系间的欧氏变换">3.1.2 坐标系间的欧氏变换</h5>
<p>　　与向量间的旋转类似，我们同样可以描述两个坐标系之间的旋转关系，再加上平移，统称为坐标系之间的变换关系。在运动过程中，通常设定惯性坐标系<span class="math inline">\(p_w\)</span>（世界坐标系）和移动坐标系<span class="math inline">\(p_c\)</span>（相机或机器人），变换关系如图： 　　<img src="https://img-blog.csdnimg.cn/20200429153715435.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 　　相机运动是一个刚体运动，它保证了同一个向量在各个坐标系下的长度和夹角都不会发生变化，这种变换称为欧氏变换。这里有一个旋转矩阵<span class="math inline">\(R\)</span>的概念：这个矩阵由两组基之间的内积组成，刻 画了旋转前后同一个向量的坐标变换关系，换句话说，旋转矩阵可以描述相机的旋转。除了旋转，还有一个平移<span class="math inline">\(t\)</span>，世界坐标系中的向量<span class="math inline">\(a\)</span>，经过一次旋转（用<span class="math inline">\(R\)</span>描述）和一次平移<span class="math inline">\(t\)</span>后，得到了<span class="math inline">\(a&#39;\)</span>，于是有：<span class="math inline">\(a&#39;=Ra+t\)</span>。</p>
<h5 id="变换矩阵与齐次坐标">3.1.3 变换矩阵与齐次坐标</h5>
<p>　　当世界坐标系中的向量经过多次变换后，其变换后向量的表达形式会很复杂，引入齐次坐标和变换矩阵重写： 　　<span class="math display">\[ \begin{bmatrix} a&#39;  \\ 1 \\ \end{bmatrix}=\begin{bmatrix} R&amp;t \\ 0^T&amp;1 \\ \end{bmatrix} \begin{bmatrix} a  \\ 1 \\ \end{bmatrix} = T \begin{bmatrix} a  \\ 1 \\ \end{bmatrix}\]</span> 这是一个数学技巧：把一个三维向量<span class="math inline">\(a\)</span>的末尾添加1，变成了四维向量，称为齐次 坐标。对于这个四维向量，我们可以把旋转和平移写在一个矩阵里面，使得整个关系变成了线性关系其中<span class="math inline">\(T\)</span>为变换矩阵。在齐次坐标中，某个点 x 的每个分量同乘一个非零常数 k 后，仍然表示的是同一个点。因此，一个点的具体坐标值不是唯一的，但当最后一项不为零时，总可以把所有坐标除以最后一项，强制最后一项为 1，从而得到一个点唯一的坐标表示（也就是转换成非齐次坐标），即 <span class="math display">\[\tilde{x}=[x,y,x,w]^T=[x/w,y/w,z/w,1]^T\]</span> 然后忽略掉最后为1的项，这个点的坐标和欧氏空间就是一样的了。那么惯性坐标系向量<span class="math inline">\(a\)</span>经两次变换累加的形式可以写为：<span class="math display">\[\tilde{b}=T_1\tilde{a}，\tilde{c}=T_2\tilde{b}  \Rightarrow \tilde{c}=T_1T_2\tilde{a}\]</span> 其中<span class="math inline">\(\tilde{a}\)</span> 表示<span class="math inline">\(a\)</span>的齐次坐标。</p>
<h4 id="实践eigen">3.2 实践：Eigen</h4>
<p><strong>Eigen</strong>是一个 C++ 开源线性代数库。它提供了快速的有关矩阵的线性代数运算，还包括解方程等功能。许多上层的软件库也使用 Eigen 进行矩阵运算，包括 g2o、Sophus等。 #### 3.3 旋转向量和欧拉角</p>
<h3 id="四元数">3.4 四元数</h3>
<h3 id="相似仿射射影变换">3.5 相似、仿射、射影变换</h3>
<p>在3D变换中，欧氏变换保持了向量的长度和夹角，相当于我们把一个刚体原封不动地进行了移动或旋转，不改变它自身的样子，而其他几种变换则会改变它的外形，它们都拥有类似的矩阵表示。</p>
<p><strong>1、 相似变换</strong> 　相似变换比欧氏变换多了一个自由度，它允许物体进行均匀的缩放，其矩阵表示为： <span class="math display">\[T_s= \begin{bmatrix} sR&amp;t  \\ 0^T&amp;1 \\ \end{bmatrix}\]</span> 缩放因子 s 表示在对向量旋转之后，可以在 <span class="math inline">\(x,y,z\)</span> 三个坐标上进行均匀的缩放，相似变换不再保持图形的面积不变。</p>
<p><strong>2、仿射变换</strong> 　其矩阵表示为： <span class="math display">\[T_A= \begin{bmatrix} A&amp;t  \\ 0^T&amp;1 \\ \end{bmatrix}\]</span> 仿射变换只要求 A 是一个可逆矩阵，而不必是正交矩阵，这与欧式变换不同，仿射变换也叫正交投影。经过仿射变换之后，立方体就不再是方的了，但是各个面仍然是平行四边形。</p>
<p><strong>3、射影变换</strong> 　其矩阵表示为： <span class="math display">\[T_P= \begin{bmatrix} A&amp;t  \\ a^T&amp;v \\ \end{bmatrix}\]</span> 它左上角为可逆矩阵 A，右上为平移 t，左下缩放<span class="math inline">\(a^T\)</span>。由于采用齐坐标，当 v ≠ 0 时，可以对整个矩阵除以 v 得到一个右下角为 1 的矩阵；否则，得到右下角 为 0 的矩阵。从真实世界到相机照片的变换可以看成一个射影变换，也就是说物体的形状会发生不规则变化。其实如果相机的焦距为无穷远，那么这个变换则为仿射变换。</p>
<h3 id="实践eigen几何模块">3.6 实践：Eigen几何模块</h3>
<h3 id="可视化演示">3.7 可视化演示</h3>
]]></content>
      <categories>
        <category>work</category>
        <category>视觉SLAM十四讲</category>
        <category>第三章</category>
      </categories>
      <tags>
        <tag>视觉SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title> 视觉SLAM十四讲 第二章</title>
    <url>/2020/04/28/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
    <content><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2>
<p>作者：高翔 | 张涛 <br> 最后更新于：2017.03.31</p>
<h3 id="第二章-初识slam">第二章 初识SLAM</h3>
<h4 id="小萝卜例子">2. 1 小萝卜例子</h4>
<p><img src="https://img-blog.csdnimg.cn/20200428103415425.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p>
<p>小萝卜是一个组装机器人，设备有相机、轮子、笔记本，我们希望小萝卜有自主运动能力，它至少要知道两件事： <br> 1. 我在什么地方——定位。 <br> 2. 周围环境是什么样——建图。 　　</p>
<p>　　有些传感器可以解决定位问题了，而有些不能，分为两种：一类是在机器人人体上的（轮式编码器、相机、激光、惯性测量单元IMU等，所测得数据是间接物理量，例如轮子转动角度、角速度和加速度等）；另一类是安装于环境中的（导轨、二维码标志等）可直接测量机器人位置信息，即解决定位问题，<font color="red">但不能在所有环境中都安装</font>。所以没有GPS的地方定位就是接下来要解决的问题。下面说说视觉SLAM中的视觉，即小萝卜的眼睛——相机。 <br> 　　SLAM的相机与昂贵的单反不同。按工作方式，分为：单目(Monocular)、双目(Stereo)、深度相机(RGB-D)。从图中来看，小萝卜用的是双目相机。这几类相机用来做SLAM时各有特点： <br> * <strong>单目相机</strong> 单目相机的数据：照片，它以二维的形式反映了三维的世界，它少了场景的一个维度：深度（或距离）。我们的眼睛在多数情况下可以判断出物体的远近，当然也有时效的时候，由于单目相机是二维的，所以想要恢复三维结构，就必须移动相机的视角，通过视差定量地判断物体的远近，但这只是一个相对的距离，无法确定物体的具体尺度，称之为<font color="maroon">尺度不确定性</font>。 <br> * <strong>双目相机和深度相机</strong> 双目相机和深度相机的目的，在于通过某种手段测量物体离我们的距离，克服单目无法知道距离的缺点。双目相机由两个单目相机组成，但这两个相机之间的距离（称为基线 Baseline）是已知的，通过这个基线来估计每个像素的空间位置——这和人眼非常相似。双目相机的距离估计是比较左右眼的图像获得的，并不依赖其他传感设备，所以它既可以应用在室内，亦可应用于室外。<font color="red">但是其缺点是配置与标定均较为复杂，其深度量程和精度受双目的基线与分辨率限制，而且视差的计算非常消耗计算资源，需要使用 GPU 和 FPGA 设备加速后，才能实时输出整张图像的距离信息，因此在现有的条件下，计算量是双目的主要问题之一。</font> 深度相机（又称 RGB-D 相机），它最大的特点是可以通过红外结构光或 Time-of-Flight（ToF）原理，像激光传感器那样，通过主动向物体发射光并接收返回的光，测出物体离相机的距离。其计算量小于双目，但现在多数 RGB-D 相机还存在<font color="maroon">测量范围窄、噪声大、视野小、易受日光干扰、无法测量透射材质</font>等诸多问题，在 SLAM 方面，主要用于室内 SLAM，室外则较难应用。</p>
<h4 id="经典视觉slam框架">2.2 经典视觉SLAM框架</h4>
<p>下图是SLAM的框架： <img src="https://img-blog.csdnimg.cn/20200429111630228.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p>
<p>其流程主要分为以下几步： <br> * <font color="red">传感器信息读取</font>。在视觉 SLAM 中主要为<strong>相机图像信息的读取和预处理</strong>。 <br> * <font color="red">视觉里程计</font> (Visual Odometry, VO，又称为前端（Front End）)。视觉里程计任务是<strong>估算相邻图像间相机的运动</strong>， <strong>以及局部地图的样子</strong>。 <br> * <font color="red">后端优化</font>（Optimization，在VO之后又称为后端（Back End））。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行<strong>优化</strong>，得到全局一致的<strong>轨迹和地图</strong>。 <br> * <font color="red">回环检测</font>（Loop Closing）。回环检测<strong>判断机器人是否曾经到达过先前的位置</strong>，如果检测到回环，它会把信息提供给后端进行处理。 <br> * <font color="red">建图</font>（Mapping）。它根据估计的轨迹，<strong>建立与任务要求对应的地图</strong>。</p>
<p>下面针对一些模块做简要介绍：</p>
<h5 id="视觉里程计">2.2.1 视觉里程计</h5>
<p><strong>VO知识铺垫</strong>：VO 能够通过相邻帧间的图像估计相机运动，并恢复场景的空间结构。叫它为“里程计”是因为它和实际的里程计一样，只计算相邻时刻的运动，而和再往前的过去的信息没有关联。 <br> 　　这里要解决的问题是：计算机是如何通过图像确定相机的运动呢？在视觉SLAM中，计算机中只有像素点，知道它们是某些空间点在相机的成像平面上投影的结果。要定量估计相机的运动就必须先知道相机与空间点的几何关系。 <br> 　　假设VO已经估计了两张图像间的相机运动，所以只要将每两个相邻时刻的运动串起来，就构成机器人的运动轨迹，即解决了定位问题。另一方面根据每个时刻相机的定位，计算出各像素点对应的空间位置，就得到了地图。但是由于运动估计误差的积累，将出现<font color="red">累计漂移（Accumulating Drift）</font>，它将导致我们无法建立一致的地图，为了解决漂移问题，我们还需要两种技术：后端优化和回环检测。</p>
<h5 id="后端优化">2.2.2 后端优化</h5>
<p>　后端优化主要处理SLAM过程中的噪声问题。后端优化要考虑的问题， 就是如何从这些带有噪声的数据中，估计整个系统的状态，以及这个状态估计的不确定性有多大——<font color="red">这称为最大后验概率估计（Maximum-a-Posteriori，MAP）</font>。这里的状态既包括机器人自身的轨迹，也包含地图。为解决SLAM对运动主体自身和周围环境空间不确定性的估计，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，去估计状态的均值和不确定性（方差）。</p>
<h5 id="回环检测">2.2.3 回环检测</h5>
<p>　　回环检测，又称闭环检测（Loop Closure Detection），主要解决位置估计随时间漂移 的问题。通俗一点来解释：就是通过某种方法，让机器人知道“回到原点”这件事，或是把原点识别出来，再把位置的估计值“拉”过去，就可以消除漂移了，即所谓的回环检测。为了实现回环检测，我们需要让机器人具有识别曾到达过的场景的能力。可以在环境中安装传感器，类似在环境中的一种标志，只要看到了标志就知道自己回到了原点，但更希望机器人自身携带传感器——图像。可以判断图像间的相似性（图像匹配）完成闭环检测。</p>
<h5 id="建图">2.2.4 建图</h5>
<p>　　建图（Mapping）是指构建地图的过程。地图是对环境的描述，但这个描述并不是固定的，需要视 SLAM 的应用而定。地图类型如下： <img src="https://img-blog.csdnimg.cn/20200429130401375.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 大体上讲，它们可以分为度量地图（Metric Map）与拓扑地图（Topological Map）两种： <br> * <strong>度量地图</strong> 度量地图强调精确地表示地图中物体的位置关系，通常用稀疏（Sparse）与稠密 （Dense）对它们进行分类。稀疏地图进行了一定程度的抽象，并不需要表达所有的物体，例如，只选择一部分具有代表意义的东西，称之为路标（Landmark），那么一张稀疏地图就是由路标组成的地图，而不是路标的部分就可以忽略掉。相对的，稠密地图着重于建模所有看到的东西。对于定位来说，稀疏路标地图就足够了，导航时则需要稠密地图。其中稠密地图有许多小块组成，小块的状态有占据、空闲和未知来表达格子（Grid）内是否有物体，通过查询告知的方式判断是否可以通过该格。但这样需要大量存储空间，并且大规模的度量地图易出现一致性问题，很小的一点转向误差，可能会导致两间屋子的墙出现重叠，使得地图失效。 <br> * <strong>拓扑地图</strong> 相比于度量地图的精确性，拓扑地图则更强调地图元素之间的关系。拓扑地图是一个图（Graph），由节点和边组成，只考虑节点间的连通性。它不注重地图对精确位置的需要，去掉地图的细节问题， 是一种更为紧凑的表达方式。然而，拓扑地图不擅长表达具有复杂结构的地图，如何对地图进行分割形成结点与边，又如何使用拓扑地图进行导航与路径规划，仍是有待研究的问题。</p>
<h4 id="slam问题的数学表述">2.3 SLAM问题的数学表述</h4>
<p>　　假设小萝卜携带着某种传感器在未知环境里运动。 首先，由于相机通常是在某些时刻采集数据的，所以我们也只关心这些时刻的位置和地图。 因此把一段连续时间的运动变成了离散时刻 <span class="math inline">\(t = 1,...,K\)</span> 当中发生的事情，各时刻的位置就记为 <span class="math inline">\(x_1,...,x_K\)</span>，它们构成了小萝卜的轨迹。地图方面，设地图是由许多个路标（Landmark）组成的，而每个时刻，传感器 会得到一部分路标点的观测数据，设路标点一共有 N 个，用<span class="math inline">\(y_1,...,y_N\)</span>表示它们。 小萝卜的运动由运动和观测来描述：一是从k-1时刻到k时刻位置x的变化；二是在k时刻的<span class="math inline">\(x_k\)</span>处探测出路标<span class="math inline">\(y_j\)</span>，这一事情用数学语言来描述。因此可以描述为两个方程：<span class="math display">\[x_k=f(x_{k-1},u_k,w_k)\]</span> <span class="math display">\[z_{k,j}=h(y_j,x_k,v_{k,j})\]</span>第一个是运动方程，其中<span class="math inline">\(u_k\)</span>是运动传感器的读数（有时也叫输入)，<span class="math inline">\(w_k\)</span>为噪声。用一个一 般函数 f 来描述这个过程，整个函数可以指代任意的运动传感器。第二个是观测方程，程描述的是当小萝卜在<span class="math inline">\(x_k\)</span>位置上 看到某个路标点<span class="math inline">\(y_j\)</span>，产生了一个观测数据<span class="math inline">\(z_{k,j}\)</span>，这里<span class="math inline">\(v_{k,j}\)</span>是这次观测里的噪声。由于观测所用的传感器形式更多，这里的观测数据 z 以及观测方程 h 也许多不同的形式。考虑视觉 SLAM 时，传感器是相机，那么观测方程就是<font color="blue">“对路标点拍摄后，得到了图 像中的像素”</font>的过程。 <br> 　 　需要说明一点，针对不同的传感器，两个方程都有不同的参数化形式，涉及到<span class="math inline">\(x,y,z\)</span>的参数化，这里不赘述了。但是这两个方程描述了最基本的 SLAM 问题：当知道运动传感器的读数 u，以及观测传感器的读数 z 后，把 SLAM 问题建模成了一个<strong>状态估计问题</strong>：如何通过带有噪声的测量数据估计内部的、隐藏着的状态变量？更具体的估计的求解与噪声性质 (线性/非线性和高斯/非高斯)有关，并且涉及三维空间中六自由度位姿的表达和优化，以及观测方程如何参数化 (即空间中的路标点是如何投影到一张照片上)。</p>
<h4 id="实践编程基础">2.4 实践：编程基础</h4>
<p>这本书的所有变成都是在Linux操作系统上完成的，程序将主要以 Linux 上的 C++ 程序为主。作为windows用户，我在电脑上装了一个Ubuntu的子系统。编程部分就自己悄悄地完成了~</p>
]]></content>
      <categories>
        <category>work</category>
        <category>视觉SLAM</category>
        <category>第二章</category>
      </categories>
      <tags>
        <tag>视觉SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title> 学习规划</title>
    <url>/2020/04/27/%E4%B9%A0%E8%A7%84%E5%88%92/</url>
    <content><![CDATA[<h5 id="迷茫的我接下来该干什么">迷茫的我接下来该干什么</h5>
<hr>
<p>记录一下我从充满热情到颓废无比的前半段研究生状态。</p>
<p>从去年10月份开始接触CV，慢慢的接触到图像匹配辅助定位方向，并为之看了将近20篇文章，并产生了两个idea，但是最后被否决了。为什么这个否定不来得早一些，偏偏在这个时候，我感觉真的对生活和学习都失去了兴趣，我真的完蛋了吗。</p>
<p>我不想，于是我打算梳理一下我做过的事学过的东西，并尝试着寻求下一条出路：</p>
<ul>
<li>Tools： python（tensorflow、pytorch）</li>
<li>深度网络： AlexNet、ResNet、VGG、RNN(LSTM)、GoogleNet、GAN、encoder-decoder</li>
<li>Others： attention mechanism</li>
<li>Papr reading：GAN交叉视图生成、cross-view geo-localization、depth estimation、attention module</li>
</ul>
<p>那么现在我的cross-view geo-localization方向要暂且放一放了，这其中学到的值得用的大概就是他们处理feature的方法、设计的Loss、几个特别的网络以及加入注意力机制的概念。 为了不让工作停滞，现在为自己规划一下接下来的工作：</p>
<ul>
<li>学技术：C++、keras、JAVA（每天学一些） <br></li>
<li>跑程序：训练GAN交叉视图的生成(CVUSA&amp;VHdataset)+attention moduel <br></li>
<li>其他：啃完《SLAM十四讲》（每天都要看一些并做笔记） <br></li>
<li>方向：继续看顶会上各个方向的文章（Long-term、多传感器融合等） 找找灵感或者感兴趣方向，记录想法，切记多看。</li>
</ul>
<p>以上工作或需要多线程进行了，成功的第一步大概就是走出舒适圈吧~</p>
<hr>
<p>小声逼逼：我不是这么轻易放弃的人，路还很长，梦想还是很遥远。</p>
]]></content>
      <categories>
        <category>life</category>
        <category>planning</category>
      </categories>
  </entry>
  <entry>
    <title>GAN跑mnist数据集</title>
    <url>/2020/04/27/AN%E8%B7%91mnist%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    <content><![CDATA[<h2 id="gan网络在手写数字数据集mnist上的运用">GAN网络在手写数字数据集mnist上的运用</h2>
<p>前言：最近学了几种GAN网络，想自己动手跑一跑，就搭了个很简单的普通GAN网络。</p>
<p><strong>一、网络架构</strong><br>
GAN的原理就不详细说明了，有很多很好资源都可以学习到，在这里定义的GAN网络结构如下（图来自灵魂画手<a href="https://zhuanlan.zhihu.com/p/85908702" target="_blank" rel="noopener">知乎</a>）： <img src="https://img-blog.csdnimg.cn/20200316163116180.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="framework"> <font color="red">结构</font> ：input --&gt; 全连接层 --&gt; leak Relu --&gt; dropout20% --&gt; 全连接层 --&gt; tanh --&gt;output。（生成器和判别器的网络架构一样） <br> <font color="red">判别器的目的是</font>：1. 对于真实图片，D要为其打上标签1；对于生成图片，D要为其打上标签0。 <br> <font color="red">生成器的目的是 </font>：对于生成的图片，G希望D打上标签1。</p>
<p><strong>二、源代码</strong><br>
part1，import~~</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> tensorflow <span class="im">as</span> tf</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> pickle</a>
<a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">from</span> tensorflow.examples.tutorials.mnist <span class="im">import</span> input_data</a></code></pre></div>
<p>part2,定义generator 和 discriminator * Generator</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> generator(noise_img,reuse):</a>
<a class="sourceLine" id="cb2-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co">    生成器</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4"><span class="co">    :param noise_img:</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5"><span class="co">    :return:</span></a>
<a class="sourceLine" id="cb2-6" data-line-number="6"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">    alpha <span class="op">=</span> <span class="fl">0.01</span>      <span class="co"># parameter of leaky ReLU</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8">    out_dim <span class="op">=</span> <span class="dv">784</span>     <span class="co"># the output size of the generator, MNIST images are 28*28, into a length of 784 array.</span></a>
<a class="sourceLine" id="cb2-9" data-line-number="9">    g_units <span class="op">=</span> <span class="dv">128</span>    <span class="co"># hidden layer nodes</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10"></a>
<a class="sourceLine" id="cb2-11" data-line-number="11">    <span class="cf">with</span> tf.variable_scope(<span class="st">&quot;generator&quot;</span>, reuse<span class="op">=</span>reuse):</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">        <span class="co"># hidden layer</span></a>
<a class="sourceLine" id="cb2-13" data-line-number="13">        hidden1 <span class="op">=</span> tf.layers.dense(noise_img, g_units)</a>
<a class="sourceLine" id="cb2-14" data-line-number="14"></a>
<a class="sourceLine" id="cb2-15" data-line-number="15">        <span class="co"># leaky ReLU</span></a>
<a class="sourceLine" id="cb2-16" data-line-number="16">        hidden1 <span class="op">=</span> tf.maximum(alpha <span class="op">*</span> hidden1, hidden1)</a>
<a class="sourceLine" id="cb2-17" data-line-number="17"></a>
<a class="sourceLine" id="cb2-18" data-line-number="18">        <span class="co"># dropout</span></a>
<a class="sourceLine" id="cb2-19" data-line-number="19">        hidden1 <span class="op">=</span> tf.layers.dropout(hidden1, rate<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb2-20" data-line-number="20"></a>
<a class="sourceLine" id="cb2-21" data-line-number="21">        <span class="co"># logits &amp; outputs,tf.layers.dense——full connection</span></a>
<a class="sourceLine" id="cb2-22" data-line-number="22">        logits <span class="op">=</span> tf.layers.dense(hidden1, out_dim)</a>
<a class="sourceLine" id="cb2-23" data-line-number="23">        <span class="co"># the pixel range of MNIST  dataset is 0-1, the generated image range is (-1,1).</span></a>
<a class="sourceLine" id="cb2-24" data-line-number="24">        outputs <span class="op">=</span> tf.tanh(logits)</a>
<a class="sourceLine" id="cb2-25" data-line-number="25"></a>
<a class="sourceLine" id="cb2-26" data-line-number="26">    <span class="cf">return</span> logits, outputs</a></code></pre></div>
<ul>
<li>Discriminator</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">def</span>  discriminator(img,reuse):</a>
<a class="sourceLine" id="cb3-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="co">    discriminator</span></a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="co">    :param img:</span></a>
<a class="sourceLine" id="cb3-5" data-line-number="5"><span class="co">    :return:</span></a>
<a class="sourceLine" id="cb3-6" data-line-number="6"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">    alpha<span class="op">=</span><span class="fl">0.01</span>      </a>
<a class="sourceLine" id="cb3-8" data-line-number="8">    d_units<span class="op">=</span><span class="dv">128</span>    </a>
<a class="sourceLine" id="cb3-9" data-line-number="9"></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">    <span class="cf">with</span> tf.variable_scope(<span class="st">&quot;discriminator&quot;</span>, reuse<span class="op">=</span>reuse):</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">        <span class="co"># hidden layer</span></a>
<a class="sourceLine" id="cb3-12" data-line-number="12">        hidden2 <span class="op">=</span> tf.layers.dense(img, d_units)</a>
<a class="sourceLine" id="cb3-13" data-line-number="13">        <span class="co"># leaky ReLU</span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14">        hidden2 <span class="op">=</span> tf.maximum(alpha <span class="op">*</span> hidden2, hidden2)</a>
<a class="sourceLine" id="cb3-15" data-line-number="15">        <span class="co"># dropout</span></a>
<a class="sourceLine" id="cb3-16" data-line-number="16">        hidden2 <span class="op">=</span> tf.layers.dropout(hidden2, rate<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb3-17" data-line-number="17">        <span class="co"># logits &amp; outputs</span></a>
<a class="sourceLine" id="cb3-18" data-line-number="18">        logits <span class="op">=</span> tf.layers.dense(hidden2,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb3-19" data-line-number="19">        outputs <span class="op">=</span> tf.tanh(logits)</a>
<a class="sourceLine" id="cb3-20" data-line-number="20"></a>
<a class="sourceLine" id="cb3-21" data-line-number="21">    <span class="cf">return</span> logits, outputs</a></code></pre></div>
<p>part3，mian() —初始化、训练、抽样评估</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">def</span> GAN_mnist():</a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3"><span class="co">    simple GAN generate handwritten digital picture</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4"><span class="co">    :return:</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5"><span class="co">    &quot;&quot;&quot;</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6">    mnist <span class="op">=</span> input_data.read_data_sets(<span class="st">&#39;./mnist_dataset/&#39;</span>,one_hot<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-7" data-line-number="7">    img_size <span class="op">=</span> mnist.train.images[<span class="dv">0</span>].shape[<span class="dv">0</span>]</a>
<a class="sourceLine" id="cb4-8" data-line-number="8">    noise_size <span class="op">=</span> <span class="dv">100</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9"></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">    <span class="co"># 1、parameters preparing</span></a>
<a class="sourceLine" id="cb4-11" data-line-number="11">    learning_rate <span class="op">=</span> <span class="fl">0.001</span></a>
<a class="sourceLine" id="cb4-12" data-line-number="12">    smooth <span class="op">=</span> <span class="fl">0.1</span>    <span class="co"># label smoothing</span></a>
<a class="sourceLine" id="cb4-13" data-line-number="13">    n_sample <span class="op">=</span> <span class="dv">25</span> <span class="co"># number of samples</span></a>
<a class="sourceLine" id="cb4-14" data-line-number="14">    samples<span class="op">=</span>[]</a>
<a class="sourceLine" id="cb4-15" data-line-number="15"></a>
<a class="sourceLine" id="cb4-16" data-line-number="16">    tf.reset_default_graph()</a>
<a class="sourceLine" id="cb4-17" data-line-number="17">    <span class="co"># 2、dataset preparing</span></a>
<a class="sourceLine" id="cb4-18" data-line-number="18">    real_img <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, img_size], name<span class="op">=</span><span class="st">&#39;real_img&#39;</span>) <span class="co"># image size784=28*28</span></a>
<a class="sourceLine" id="cb4-19" data-line-number="19">    <span class="co"># noise size input generator</span></a>
<a class="sourceLine" id="cb4-20" data-line-number="20">    noise_img <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, noise_size], name<span class="op">=</span><span class="st">&#39;noise_img&#39;</span>)</a>
<a class="sourceLine" id="cb4-21" data-line-number="21"></a>
<a class="sourceLine" id="cb4-22" data-line-number="22">    <span class="co"># 3、Generator generate image</span></a>
<a class="sourceLine" id="cb4-23" data-line-number="23">    g_logits,g_outputs <span class="op">=</span> generator(noise_img,reuse<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb4-24" data-line-number="24">    <span class="co"># generator的loss</span></a>
<a class="sourceLine" id="cb4-25" data-line-number="25"></a>
<a class="sourceLine" id="cb4-26" data-line-number="26">    <span class="co"># 4、discriminator discriminates between true and false</span></a>
<a class="sourceLine" id="cb4-27" data-line-number="27">    d_logits_real, d_outputs_real <span class="op">=</span> discriminator(real_img , reuse<span class="op">=</span><span class="va">False</span>)</a>
<a class="sourceLine" id="cb4-28" data-line-number="28">    d_logits_fake, d_outputs_fake <span class="op">=</span> discriminator(g_outputs , reuse<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-29" data-line-number="29"></a>
<a class="sourceLine" id="cb4-30" data-line-number="30">    <span class="co"># 5、calculate loss</span></a>
<a class="sourceLine" id="cb4-31" data-line-number="31">    <span class="co"># discriminator_loss：discriminate real image + discriminate fake image</span></a>
<a class="sourceLine" id="cb4-32" data-line-number="32"></a>
<a class="sourceLine" id="cb4-33" data-line-number="33">    <span class="co"># test the discriminant discriminant ability</span></a>
<a class="sourceLine" id="cb4-34" data-line-number="34">    d_loss_real <span class="op">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</a>
<a class="sourceLine" id="cb4-35" data-line-number="35">        logits<span class="op">=</span>d_logits_real,labels<span class="op">=</span>tf.ones_like(d_logits_real)) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> smooth))</a>
<a class="sourceLine" id="cb4-36" data-line-number="36"></a>
<a class="sourceLine" id="cb4-37" data-line-number="37">    <span class="co"># test the discriminant discriminant ability</span></a>
<a class="sourceLine" id="cb4-38" data-line-number="38">    d_loss_fake <span class="op">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</a>
<a class="sourceLine" id="cb4-39" data-line-number="39">        logits<span class="op">=</span>d_logits_fake,labels<span class="op">=</span>tf.zeros_like(d_logits_fake)))</a>
<a class="sourceLine" id="cb4-40" data-line-number="40"></a>
<a class="sourceLine" id="cb4-41" data-line-number="41">    <span class="co"># sum_loss</span></a>
<a class="sourceLine" id="cb4-42" data-line-number="42">    d_loss <span class="op">=</span> tf.add(d_loss_real, d_loss_fake)</a>
<a class="sourceLine" id="cb4-43" data-line-number="43"></a>
<a class="sourceLine" id="cb4-44" data-line-number="44">    <span class="co"># generator_loss</span></a>
<a class="sourceLine" id="cb4-45" data-line-number="45">    g_loss <span class="op">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</a>
<a class="sourceLine" id="cb4-46" data-line-number="46">        logits<span class="op">=</span>d_logits_fake,labels<span class="op">=</span>tf.ones_like(d_logits_fake)) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> smooth))</a>
<a class="sourceLine" id="cb4-47" data-line-number="47"></a>
<a class="sourceLine" id="cb4-48" data-line-number="48">    <span class="co"># 6、Gradient descent loss optimization</span></a>
<a class="sourceLine" id="cb4-49" data-line-number="49">    train_vars <span class="op">=</span> tf.trainable_variables()</a>
<a class="sourceLine" id="cb4-50" data-line-number="50">    <span class="co"># generator_tensor</span></a>
<a class="sourceLine" id="cb4-51" data-line-number="51">    g_vars <span class="op">=</span> [var <span class="cf">for</span> var <span class="kw">in</span> train_vars <span class="cf">if</span> var.name.startswith(<span class="st">&quot;generator&quot;</span>)]</a>
<a class="sourceLine" id="cb4-52" data-line-number="52">    <span class="co"># discriminator_tensor</span></a>
<a class="sourceLine" id="cb4-53" data-line-number="53">    d_vars <span class="op">=</span> [var <span class="cf">for</span> var <span class="kw">in</span> train_vars <span class="cf">if</span> var.name.startswith(<span class="st">&quot;discriminator&quot;</span>)]</a>
<a class="sourceLine" id="cb4-54" data-line-number="54">    <span class="co"># optimizer</span></a>
<a class="sourceLine" id="cb4-55" data-line-number="55">    d_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list<span class="op">=</span>d_vars)</a>
<a class="sourceLine" id="cb4-56" data-line-number="56">    g_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list<span class="op">=</span>g_vars)</a>
<a class="sourceLine" id="cb4-57" data-line-number="57"></a>
<a class="sourceLine" id="cb4-58" data-line-number="58">    <span class="co"># 7、trianing part</span></a>
<a class="sourceLine" id="cb4-59" data-line-number="59">    <span class="co"># train parameters</span></a>
<a class="sourceLine" id="cb4-60" data-line-number="60">    batch_size <span class="op">=</span> <span class="dv">64</span></a>
<a class="sourceLine" id="cb4-61" data-line-number="61">    epoch <span class="op">=</span> <span class="dv">5</span></a>
<a class="sourceLine" id="cb4-62" data-line-number="62">    n_batch<span class="op">=</span> mnist.train.num_examples <span class="op">//</span> batch_size   <span class="co"># Calculate how many lots there are</span></a>
<a class="sourceLine" id="cb4-63" data-line-number="63"></a>
<a class="sourceLine" id="cb4-64" data-line-number="64">    losses <span class="op">=</span> [] <span class="co"># store loss</span></a>
<a class="sourceLine" id="cb4-65" data-line-number="65"></a>
<a class="sourceLine" id="cb4-66" data-line-number="66">    <span class="co"># start training</span></a>
<a class="sourceLine" id="cb4-67" data-line-number="67">    <span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</a>
<a class="sourceLine" id="cb4-68" data-line-number="68">        sess.run(tf.global_variables_initializer())</a>
<a class="sourceLine" id="cb4-69" data-line-number="69"></a>
<a class="sourceLine" id="cb4-70" data-line-number="70">        <span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(epoch):</a>
<a class="sourceLine" id="cb4-71" data-line-number="71">            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_batch):</a>
<a class="sourceLine" id="cb4-72" data-line-number="72">                batch <span class="op">=</span> mnist.train.next_batch(batch_size)</a>
<a class="sourceLine" id="cb4-73" data-line-number="73">                batch_images <span class="op">=</span> batch[<span class="dv">0</span>].reshape((batch_size,<span class="dv">784</span>))  <span class="co"># batch[0]represents image,reshape-&gt; batch_size*784</span></a>
<a class="sourceLine" id="cb4-74" data-line-number="74">                batch_images <span class="op">=</span> batch_images <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>   <span class="co"># scale the pixel of images for outputs of tanh are (-1,1)</span></a>
<a class="sourceLine" id="cb4-75" data-line-number="75">                batch_noise <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>(batch_size, <span class="dv">100</span>))</a>
<a class="sourceLine" id="cb4-76" data-line-number="76"></a>
<a class="sourceLine" id="cb4-77" data-line-number="77">                _ <span class="op">=</span> sess.run(d_train_opt, feed_dict<span class="op">=</span>{real_img: batch_images, noise_img: batch_noise})</a>
<a class="sourceLine" id="cb4-78" data-line-number="78">                _ <span class="op">=</span> sess.run(g_train_opt, feed_dict<span class="op">=</span>{noise_img: batch_noise})</a>
<a class="sourceLine" id="cb4-79" data-line-number="79"></a>
<a class="sourceLine" id="cb4-80" data-line-number="80">            <span class="co"># calculate the loss after each epoch</span></a>
<a class="sourceLine" id="cb4-81" data-line-number="81">            train_loss_d_real <span class="op">=</span> sess.run(d_loss_real,feed_dict<span class="op">=</span>{real_img:batch_images,noise_img:batch_noise})</a>
<a class="sourceLine" id="cb4-82" data-line-number="82">            train_loss_d_fake <span class="op">=</span> sess.run(d_loss_fake,feed_dict<span class="op">=</span>{real_img:batch_images,noise_img:batch_noise})</a>
<a class="sourceLine" id="cb4-83" data-line-number="83"></a>
<a class="sourceLine" id="cb4-84" data-line-number="84">            train_loss_d <span class="op">=</span> sess.run(d_loss,feed_dict<span class="op">=</span>{real_img: batch_images, noise_img: batch_noise})</a>
<a class="sourceLine" id="cb4-85" data-line-number="85"></a>
<a class="sourceLine" id="cb4-86" data-line-number="86">            train_loss_g <span class="op">=</span> sess.run(g_loss,feed_dict<span class="op">=</span>{noise_img: batch_noise})</a>
<a class="sourceLine" id="cb4-87" data-line-number="87"></a>
<a class="sourceLine" id="cb4-88" data-line-number="88">            <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">...&quot;</span>.<span class="bu">format</span>(e <span class="op">+</span> <span class="dv">1</span>, epoch),</a>
<a class="sourceLine" id="cb4-89" data-line-number="89">                  <span class="st">&quot;Discriminator Loss: </span><span class="sc">{:.4f}</span><span class="st">(Real: </span><span class="sc">{:.4f}</span><span class="st"> + Fake: </span><span class="sc">{:.4f}</span><span class="st">)...&quot;</span>.<span class="bu">format</span>(train_loss_d, train_loss_d_real,</a>
<a class="sourceLine" id="cb4-90" data-line-number="90">                                                                                      train_loss_d_fake),</a>
<a class="sourceLine" id="cb4-91" data-line-number="91">                  <span class="st">&quot;Generator Loss: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(train_loss_g))</a>
<a class="sourceLine" id="cb4-92" data-line-number="92"></a>
<a class="sourceLine" id="cb4-93" data-line-number="93">            <span class="co"># record the values of loss</span></a>
<a class="sourceLine" id="cb4-94" data-line-number="94">            losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))</a>
<a class="sourceLine" id="cb4-95" data-line-number="95"></a>
<a class="sourceLine" id="cb4-96" data-line-number="96"></a>
<a class="sourceLine" id="cb4-97" data-line-number="97"></a>
<a class="sourceLine" id="cb4-98" data-line-number="98"></a>
<a class="sourceLine" id="cb4-99" data-line-number="99">            <span class="co"># sample to evaluate</span></a>
<a class="sourceLine" id="cb4-100" data-line-number="100">            sample_noise <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>(n_sample, noise_size))</a>
<a class="sourceLine" id="cb4-101" data-line-number="101">            gen_samples <span class="op">=</span> sess.run(generator(noise_img, reuse<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-102" data-line-number="102">                                   ,feed_dict<span class="op">=</span>{noise_img: sample_noise})</a>
<a class="sourceLine" id="cb4-103" data-line-number="103">            samples.append(gen_samples)</a>
<a class="sourceLine" id="cb4-104" data-line-number="104"></a>
<a class="sourceLine" id="cb4-105" data-line-number="105"></a>
<a class="sourceLine" id="cb4-106" data-line-number="106">    <span class="co"># record the data of samples</span></a>
<a class="sourceLine" id="cb4-107" data-line-number="107">    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;train_samples.pkl&#39;</span>, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> f:</a>
<a class="sourceLine" id="cb4-108" data-line-number="108">        pickle.dump(samples, f)</a>
<a class="sourceLine" id="cb4-109" data-line-number="109"></a>
<a class="sourceLine" id="cb4-110" data-line-number="110">    <span class="co"># plot loss curve</span></a>
<a class="sourceLine" id="cb4-111" data-line-number="111">    losses <span class="op">=</span> np.array(losses)</a>
<a class="sourceLine" id="cb4-112" data-line-number="112">    plt.figure(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb4-113" data-line-number="113">    plt.subplot(<span class="dv">221</span>)</a>
<a class="sourceLine" id="cb4-114" data-line-number="114">    plt.plot(losses.T[<span class="dv">0</span>], label<span class="op">=</span><span class="st">&#39;Discriminator Total Loss&#39;</span>)</a>
<a class="sourceLine" id="cb4-115" data-line-number="115">    plt.subplot(<span class="dv">222</span>)</a>
<a class="sourceLine" id="cb4-116" data-line-number="116">    plt.plot(losses.T[<span class="dv">1</span>], label<span class="op">=</span><span class="st">&#39;Discriminator Real Loss&#39;</span>)</a>
<a class="sourceLine" id="cb4-117" data-line-number="117">    plt.subplot(<span class="dv">223</span>)</a>
<a class="sourceLine" id="cb4-118" data-line-number="118">    plt.plot(losses.T[<span class="dv">2</span>], label<span class="op">=</span><span class="st">&#39;Discriminator Fake Loss&#39;</span>)</a>
<a class="sourceLine" id="cb4-119" data-line-number="119">    plt.subplot(<span class="dv">224</span>)</a>
<a class="sourceLine" id="cb4-120" data-line-number="120">    plt.plot(losses.T[<span class="dv">3</span>], label<span class="op">=</span><span class="st">&#39;Generator&#39;</span>)</a>
<a class="sourceLine" id="cb4-121" data-line-number="121">    plt.title(<span class="st">&quot;Training Losses&quot;</span>)</a>
<a class="sourceLine" id="cb4-122" data-line-number="122">    plt.show()</a>
<a class="sourceLine" id="cb4-123" data-line-number="123"></a>
<a class="sourceLine" id="cb4-124" data-line-number="124">    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;train_samples.pkl&#39;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> f:</a>
<a class="sourceLine" id="cb4-125" data-line-number="125">        samples <span class="op">=</span> pickle.load(f)</a>
<a class="sourceLine" id="cb4-126" data-line-number="126"></a>
<a class="sourceLine" id="cb4-127" data-line-number="127">    rows, cols <span class="op">=</span> <span class="dv">5</span>, <span class="dv">5</span></a>
<a class="sourceLine" id="cb4-128" data-line-number="128">    fig, axes <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>), nrows<span class="op">=</span>rows, ncols<span class="op">=</span>cols, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb4-129" data-line-number="129">    <span class="cf">for</span> ax, img <span class="kw">in</span> <span class="bu">zip</span>(axes.flatten(), samples[epoch <span class="op">-</span> <span class="dv">1</span>][<span class="dv">1</span>]):</a>
<a class="sourceLine" id="cb4-130" data-line-number="130">        ax.xaxis.set_visible(<span class="va">False</span>)</a>
<a class="sourceLine" id="cb4-131" data-line-number="131">        ax.yaxis.set_visible(<span class="va">False</span>)</a>
<a class="sourceLine" id="cb4-132" data-line-number="132">        ax.imshow(img.reshape((<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">&#39;Greys_r&#39;</span>)</a>
<a class="sourceLine" id="cb4-133" data-line-number="133">    plt.show()</a></code></pre></div>
<p>其中samples的部分就是抽样25个size为100的noise_samples送入生成器经过200epoch之后生成器生成图像，结果在第三部分。最后那几段是画图部分。</p>
<p>part4，you know</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</a>
<a class="sourceLine" id="cb5-2" data-line-number="2">    GAN_mnist()</a></code></pre></div>
<p><strong>三、看结果</strong></p>
<p>训练时候的loss，这里epoch取的200： <br> 左上是Discriminator Total Loss，右上是Discriminator Real Loss，左下Discriminator Fake Loss，最后是Generator loss。 <img src="https://img-blog.csdnimg.cn/20200316161700318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="loss"></p>
<p>生成的数字示例： <img src="https://img-blog.csdnimg.cn/20200316223639385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="运行结果"> 只是200epoch的结果，感觉隐隐约约能看出来是啥数字，可以试试修改epoch，运行一次有点久了，下次跑cGAN网络。如果有什么错误欢迎指出，谢谢~~</p>
]]></content>
      <categories>
        <category>work</category>
        <category>GAN</category>
      </categories>
      <tags>
        <tag>GAN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》</title>
    <url>/2020/04/07/%E6%96%87%E3%80%8AGround-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss%E3%80%8B/</url>
    <content><![CDATA[<h3 id="ground-to-aeria-limage-geo-localization-with-a-hard-exemplar-reweighting-triplet-loss2019-iccv">《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》（2019-ICCV）</h3>
<p>Author：Sudong Cai， Yulan Guo，Salman Khan Jiwei Hu， Gongjian Wen</p>
<pre><code>中心思想：基于Siamese network提出了FCANet，在basic ResNet中加入空间注意力和通道注意力机制(FCAM)，且给出一种新的在线困难样本重新分配权值三元组损失，并在其他文献的成就基础上，加入方向回归损失作为辅助损失。</code></pre>
<p><font color="red"><strong>补充</strong></font><strong>:</strong>　<br> <strong>1、什么是Hard example (困难样本)?</strong> <br> 　　困难负样本是指哪些容易被网络预测为正样本的proposal，即假阳性(false positive,FP)，训练hard negative对提升网络的分类性能具有极大帮助。Hard example mining的核心思想就是用分类器对样本进行分类，把其中错误分类的样本(hard negative)放入负样本集合再继续训练分类器。</p>
<p><strong>2、Triplet loss原理</strong> <br> 　　Triplet Loss是深度学习中的一种损失函数，用于训练差异性较小的样本，通过优化锚示例(a)与正示例(p)的距离小于锚示例(a)与负示例(n)的距离，实现样本的相似性计算。 - 输入一个三元组 &lt;a, p, n&gt; a： anchor p： positive, 与 a 是同一类别的样本 n： negative, 与 a 是不同类别的样本</p>
<ul>
<li><p>公式 <span class="math display">\[L= max(d(a,p)-d(a,n)+margin,0)\]</span></p>
<p>优化目标就是拉近 a, p 的距离， 拉远 a, n 的距离。其中hard triplets是: <span class="math inline">\(d(a,n)&lt;d(a,p)\)</span>, 即a, p的距离远；easy triplets: <span class="math inline">\(L=0\)</span> 即 <span class="math inline">\(d(a,p)+margin&lt;d(a,n)\)</span>，这种情况不需要优化，a, p的距离本来就很近， a, n的距离远。</p></li>
</ul>
<h4 id="一backgroundhighlight">一、Background＆Highlight</h4>
<ul>
<li><p><strong>Background</strong> <br> 1、ground-to-ground image geo-localization没有可用的参考图像数据集； <br> 2、cross-view图像匹配的弊端：视点的剧烈变化、方向信息的不定性、光照变化等； <br> 3、 handcrafted features用于计算相似度，导致结果准确率低； <br> 4、已有的hard example mining仍然很难找出困难样本。</p></li>
<li><p><strong>Highlight</strong> <br> 1、提出了一个新的triplet loss(<font color="red">online exemplar reweighting triplet loss </font>)提高训练，可以实现online hard example mining； <br> 2、提出了lightweight attention module (FCAM)。</p></li>
</ul>
<h4 id="二framework">二、Framework</h4>
<p><strong>1、 Overview</strong> <br> 文章提出的构架概览如图，网络是基于Siamese network设计的: <img src="https://img-blog.csdnimg.cn/20200318172040954.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 　　　　</p>
<p>主要组成部分：① baseline是两个CNN，用于提取图像的特征，网络分支之间没有共享权重(因为在其他文献中已经说明共享权重的效果并没它好，有一个说法就是这两个分支本来就是各自训练的)。② Dual Attention，就是文中提出的FCAM模型，是在两个CNN网络中都加入了包括通道注意力和空间维度注意力的机制，用于加大特征的区别。③ Main Loss，即文章所提的困难样本重加权triplet loss，把大的权重分配给有用的hard triplets，而把小的权重分配给信息较少的easy triplets。④ Auxiliary Loss，即Orientation regression (OR) 学习分支的损失。</p>
<p><strong>2、Feature Context-Based Attention Module</strong><br>
- <strong>Channel attention submodule</strong> <br> 　　通道注意力机制是用来强调信息相对丰富的通道。网络框架如下图所示，采用了卷积块注意模块(<a href="https://arxiv.org/pdf/1807.06521.pdf" target="_blank" rel="noopener">CBAM</a>)，输入特征图U(W×H×C)，应用max-pooling <span class="math inline">\(f_{max}\)</span>和 average-pooling <span class="math inline">\(f_{avg}\)</span> 作用于U产生一维的全局通道描述符<span class="math inline">\(v^1\)</span>和<span class="math inline">\(v^2\)</span>。然后，两个描述符经由 Multi-Layer Perceptron (MLP) <span class="math inline">\(f_{ext}(^.,^.)\)</span> 激活，分析通道之间的依赖关系。最后，用Sigmoid函数激活两个描述符的总和得到<span class="math inline">\(Z^C(U)\)</span>。输出channel attention map <span class="math inline">\(U&#39;\)</span>是通过<span class="math inline">\(Z^C(U)\)</span>和U之间的元素级乘法生成的。 <img src="https://img-blog.csdnimg.cn/20200318172206762.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<ul>
<li><p><strong>Spatial attention submodule</strong> <br> 空间注意力用于突出有意义的特征单元，网络构架如图所示。受 <a href="https://www.researchgate.net/publication/319535111_Learned_Contextual_Feature_Reweighting_for_Image_Geo-Localization" target="_blank" rel="noopener">Contextual Reweighting Network (CRN)</a> 启发，文章将特征上下文感知学习(特征reweighting strategy)集成到CBAM的基本控件注意子模块中。来自通道注意力的特征图<span class="math inline">\(U&#39;\)</span>作为输入，然后连接两个特征掩码<span class="math inline">\(f_{max}^c\)</span>和<span class="math inline">\(f_{avg}^c\)</span>(沿着通道轴线的最大池化和平均池化)生成S(W×H×2)。然后，为了利用特征元的上下文信息，使用了不同感受野的卷积（3×3，5×5和7×7）来生成中间特征掩码，再将masks连接起来生成特征掩码P(W×H×3)，再用1×1的卷积学习和积累权重生成<span class="math inline">\(Z^S(U&#39;)\)</span>。最后，spatial attention map <span class="math inline">\(U&#39;&#39;\)</span>是通过<span class="math inline">\(Z^S(U)\)</span>和<span class="math inline">\(U&#39;\)</span>之间的元素级乘法生成的。 <img src="https://img-blog.csdnimg.cn/20200318172347324.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li>
<li><p><strong>Building block</strong> <br> 集合FCAM到基本ResNet的每一个building block中就形成了CNN特征提取网络，如下图： <img src="https://img-blog.csdnimg.cn/2020031817270364.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li>
<li><strong>Hard Exemplar Reweighting Triplet Loss</strong> <br> 将基于triplet reweighting的在线困难样本挖掘策略集合到soft-margin triplet loss中，Loss定义如下： <span class="math display">\[L_{hard}(A_i,P_i,N_{i,k})= w_{hard}(A_i,P_i,N_{i,k})*log(1+exp(d_p(i)-d_n(i,k)))\]</span> 其中<span class="math inline">\(A_i\)</span>，<span class="math inline">\(P_i\)</span>和<span class="math inline">\(N_{i,k}\)</span>分别表示anchor，positive exemplar和 k-th negative exemplar。运用距离修正逻辑回归估计当前三元组的难度，根据<span class="math inline">\(gap(i,k)=d_n(i,k)-d_p(i)\)</span>的大小计算<span class="math inline">\(w_{hard}(A_i,P_i,N_{i,k})\)</span>，最终定义式如下： <img src="https://img-blog.csdnimg.cn/20200318160521251.png" srcset="/img/loading.gif" alt="困难样本权重"></li>
<li><p><strong>Orientation regression</strong> <br> 在现有的跨视图地理定位基准数据集中，锚点的方向及其对应的正样本在训练集中固定，而在测试集中打乱。所以文章随机旋转航空图像得到的不同角度可以作为训练的标签。为了解决方向未知的问题，在框架中增加一个方向回归，本文将重新分配权值的方向回归损失定义为： <span class="math display">\[L_{OR}(A_i,P_i,N_{i,k})= w_{hard}(A_i,P_i,N_{i,k})*(d^1_R(i)+d^2_R(i))\]</span> 其中<span class="math inline">\(d^1_R(i)\)</span>和<span class="math inline">\(d^2_R(i)\)</span> 分别表示sin和cosine值的回归误差。</p>
<p><font color="red"><strong>总的误差定义式如下：</strong></font> <span class="math display">\[L_{HER}(A_i,P_i,N_{i,k})= \lambda_1*L_{hard}(A_i,P_i,N_{i,k})+\lambda_2*L_{OR}(A_i,P_i,N_{i,k})\]</span></p></li>
</ul>
<h4 id="三dataset">三、Dataset</h4>
<ul>
<li><strong>CVUSA dataset</strong> <br> 包含35532个用于训练的地空图像对和8884个用于测试的图像对。地面图像为全景图像，并且地空图均为高分辨率。</li>
<li><strong>Vo and Hays’ (VH) dataset</strong> <br> 包含超过100万张由美国11个不同城市的谷歌地图收集的交叉视图图像对。使用8个子集作为训练集，并使用从Denver, Detroit, 和 Seattle捕获的其余3个子集进行评估，所有街道视图查询图像裁剪为固定大小230×230。另外，训练集中的方位角是固定的，测试集中是未知的。 <img src="https://img-blog.csdnimg.cn/20200318165049147.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="datasets"></li>
</ul>
<h4 id="四总结">四、总结</h4>
<ul>
<li><p><strong>Results</strong> <br> 1、匹配结果中top1%的recall达到了98.3%，但是top10左右的只有70%~80%；</p>
<p>2、性能排序：this approach&gt;add soft-margin ranking loss&gt; add additional OR&gt;pasitive pairs Euclidean loss;</p>
<p>3、CVUSA dataset&gt;VH dataset，因为前者是全景图像且分辨率高，包含的信息多；</p>
<p>4、直接学习方向的不变性作为辅助，比把所有图像特征聚集起来要好。</p></li>
<li><p><strong>Problems</strong> <br> 1、数据集的问题，放在正视图像上效果不太好；</p>
<p>2、网络有点复杂；</p>
<p>3、总的损失中权重<span class="math inline">\(\lambda\)</span>如何取值，对结果的影响。</p></li>
</ul>
]]></content>
      <categories>
        <category>work</category>
        <category>即使定位论文</category>
        <category>添加注意力机制</category>
      </categories>
      <tags>
        <tag>即使定位</tag>
      </tags>
  </entry>
  <entry>
    <title>pytorch学习记录</title>
    <url>/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="课程名pytorch-动态神经网络">课程名：《Pytorch 动态神经网络》</h2>
<p>课程来源：<a href="https://www.bilibili.com/video/av15997678?p=35" target="_blank" rel="noopener">here</a><br>
作者：莫烦</p>
<h2 id="day01-安装pytorch">day01 安装Pytorch</h2>
<p>前提：安装Anaconda参考别人的安装教程<a href="https://www.jianshu.com/p/742dc4d8f4c5" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/742dc4d8f4c5</a> - 在开始菜单找到Anaconda的命令提示行(Anaconda Prompt)，并输入conda create -n pytorch python=3.7(我自己的是3.7版本)，建立一个Pytorch的环境： <img src="https://img-blog.csdnimg.cn/20200318180532330.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 然后，出现以下情况，问是否安装等等工具包，选择[y]开始安装： <img src="https://img-blog.csdnimg.cn/20200318180734340.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 安装成功以后，会出现如下，就是激活环境的语句： <img src="https://img-blog.csdnimg.cn/20200318180919400.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入conda activate pytorch进入pytorch环境： <img src="https://img-blog.csdnimg.cn/20200318181124374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入pip list可以查看这个环境下的工具包，可以看到没有需要的pytorch，所以需要安装： <img src="https://img-blog.csdnimg.cn/2020031818131941.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_12,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 去官网查看自己适合哪个版本，比如我的是CPU，习惯用pip，如下图：<img src="https://img-blog.csdnimg.cn/20200322091319185.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">pip install torch<span class="op">==</span><span class="dv">1</span>.<span class="fl">4.0</span><span class="op">+</span>cpu torchvision<span class="op">==</span><span class="dv">0</span>.<span class="fl">5.0</span><span class="op">+</span>cpu <span class="op">-</span>f https:<span class="op">//</span>download.pytorch.org<span class="op">/</span>whl<span class="op">/</span>torch_stable.html</a></code></pre></div>
<ul>
<li>完了以后，在pytorch环境中进入&gt;&gt;python，测试一下是否安装成功，输入import torch即可。</li>
</ul>
<h2 id="day02">day02</h2>
<h3 id="一神经网络简介">一、神经网络简介</h3>
<p>1、 机器学习—梯度下降机制(optimization) <br> 2、神经网络黑盒：输入端-黑盒-输出端； <br> 　　　　　　黑盒：特征代表输入数据。</p>
<h3 id="二why-pytorch">二、why Pytorch？</h3>
<p>1、与tensorflow的区别</p>
<ul>
<li><p>tensorflow是静态的框架，构建好tensorflow的计算图之后，这个计算图是不能改变的，计算流程是固定的，类似C++，写代码时要用他自己的一些API。缺点之一例如训练的时候loss一直将不下来，模型很难得到优化，debug就很困难。</p></li>
<li><p>pytorch是动态的框架，和python一样，直接计算，不用开启会话。 <br></p></li>
</ul>
<h3 id="三variable变量">三、Variable变量</h3>
<ul>
<li>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置，里面的值会不停的变化，就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那里面的鸡蛋就是 Torch 的 Tensor 。</li>
<li>PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</li>
<li><p>from torch.autograd import Variable</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">ten<span class="op">=</span>torch.FloatTensor([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>]]) <span class="co"># tensor的类型</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">variable<span class="op">=</span>Variable(tensor,requires_grad<span class="op">=</span><span class="va">True</span>) </a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co"># 将tensor传给variable，需要Variable来建立一个计算图纸，把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度，如果要就会计算Variable节点的梯度</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">t_out <span class="op">=</span> torch.mean(ten<span class="op">*</span>ten) <span class="co"># 计算x^2</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">v_out <span class="op">=</span> torch.mean(variable<span class="op">*</span>variable)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">v_out.backward() <span class="co"># v_outbackward时，variable也会变化，因为是一体的</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="bu">print</span>(variable)</a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">#直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图),所以我们要转换一下, 将它变成 tensor 形式</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="bu">print</span>(variable.data)</a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="bu">print</span>(variable.data.numpy())<span class="co"># variable.data为tensor的形式，tensor才能转换为numpy形式</span></a></code></pre></div></li>
<li>autograd根据用户对Variable的操作构建其计算图，这个图将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候， 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力。</li>
<li>variable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li>
<li>多次反向传播时，<font color="red">梯度是累加的</font>。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li>
<li><p>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</p></li>
</ul>
<h2 id="day-03">day 03</h2>
<h3 id="一激励函数activation">一、激励函数（Activation）</h3>
<ul>
<li><p>什么是Activation 非线性的函数激活网络的输出：ReLU、Sigmoid、Tanh、Softplus</p></li>
<li><p>Torch中的激励函数</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="im">from</span> torch.autograd <span class="im">import</span> Variable</a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb3-5" data-line-number="5"></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># x data (tensor), shape=(100, 1)</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">x <span class="op">=</span> Variable(x)</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">x_np <span class="op">=</span> x.data.numpy()   <span class="co"># numpy 数据才能用来画图</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9"></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">y_relu <span class="op">=</span> torch.relu(x).data.numpy()</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">y_sigmoid <span class="op">=</span> torch.sigmoid(x).data.numpy()</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">y_tanh <span class="op">=</span> torch.tanh(x).data.numpy() <span class="co"># 计算出非线性函数输出后也要转化为numpy数据</span></a>
<a class="sourceLine" id="cb3-13" data-line-number="13"><span class="co"># y_softplus = F.softplus(x).data.numpy()   </span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14"></a>
<a class="sourceLine" id="cb3-15" data-line-number="15"><span class="co">#画图</span></a>
<a class="sourceLine" id="cb3-16" data-line-number="16">plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</a>
<a class="sourceLine" id="cb3-17" data-line-number="17">plt.subplot(<span class="dv">221</span>)</a>
<a class="sourceLine" id="cb3-18" data-line-number="18">plt.plot(x_np, y_relu, c<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb3-19" data-line-number="19">plt.ylim((<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>))</a>
<a class="sourceLine" id="cb3-20" data-line-number="20">plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>)</a></code></pre></div></li>
<li><p>结果 <img src="https://img-blog.csdnimg.cn/2020032211361126.png?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li>
</ul>
<h3 id="二regression回归">二、Regression回归</h3>
<p>直接放莫老师教的代码过来：</p>
<ul>
<li><p>Layer图搭建以及计算流程</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">class</span> Net(torch.nn.Module):   <span class="co"># torch.nn.Module是Net的主模块</span></a>
<a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output): <span class="co"># 搭建层所需要的信息</span></a>
<a class="sourceLine" id="cb4-3" data-line-number="3">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()  <span class="co"># 继承Net的模块功能</span></a>
<a class="sourceLine" id="cb4-4" data-line-number="4">        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></a>
<a class="sourceLine" id="cb4-5" data-line-number="5">        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb4-6" data-line-number="6"></a>
<a class="sourceLine" id="cb4-7" data-line-number="7">    <span class="kw">def</span> forward(<span class="va">self</span>, x): <span class="co"># 前向传递的过程，搭流程图</span></a>
<a class="sourceLine" id="cb4-8" data-line-number="8">           x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))      <span class="co"># activation function for hidden layer</span></a>
<a class="sourceLine" id="cb4-9" data-line-number="9">           x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></a>
<a class="sourceLine" id="cb4-10" data-line-number="10">     <span class="cf">return</span> x</a></code></pre></div></li>
<li><p>定义Net</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">net <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">1</span>)     <span class="co"># define</span></a></code></pre></div></li>
<li><p>优化神经网络(torch.optim.),以及loss function定义</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">optimizer <span class="op">=</span> torch.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">loss_func <span class="op">=</span> torch.nn.MSELoss() <span class="co"># 均方差作为loss</span></a></code></pre></div></li>
<li><p>开始训练</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">    prediction <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"></a>
<a class="sourceLine" id="cb7-4" data-line-number="4">    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"></a>
<a class="sourceLine" id="cb7-6" data-line-number="6">    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></a>
<a class="sourceLine" id="cb7-7" data-line-number="7">    loss.backward()         <span class="co"># backpropagation, compute gradients</span></a>
<a class="sourceLine" id="cb7-8" data-line-number="8">    optimizer.step()        <span class="co"># apply gradients</span></a>
<a class="sourceLine" id="cb7-9" data-line-number="9">    <span class="co"># 以上三步为优化步骤</span></a></code></pre></div></li>
</ul>
<h3 id="三-classification-分类">三、 Classification 分类</h3>
<p>　与上面不同的是：</p>
<ul>
<li>构造的伪数据不一样，是包含有对应标签的数据；(数据不能是一维)</li>
<li>网络输入输出不同，有两个输入两个输出；</li>
<li>loss用到的是交叉熵cross entropy loss，out与标签y</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">loss_func <span class="op">=</span> torch.nn.CrossEntropyLoss() </a>
<a class="sourceLine" id="cb8-2" data-line-number="2">loss <span class="op">=</span> loss_func(out, y)</a></code></pre></div>
<ul>
<li><p>output是取值，转换成概率值需要加softmax(out)</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">out <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2">prediction <span class="op">=</span> F.softmax(out)  <span class="co">#将输出对应值转化成概率</span></a></code></pre></div></li>
</ul>
<h3 id="四快速搭建网络">四、快速搭建网络</h3>
<ul>
<li><p>method1—搭建网络、流程图，定义网络</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">class</span> Net(torch.nn.Module):</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></a>
<a class="sourceLine" id="cb10-5" data-line-number="5">        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb10-6" data-line-number="6"></a>
<a class="sourceLine" id="cb10-7" data-line-number="7">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))      <span class="co"># activation function for hidden layer</span></a>
<a class="sourceLine" id="cb10-9" data-line-number="9">        x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></a>
<a class="sourceLine" id="cb10-10" data-line-number="10">        <span class="cf">return</span> x</a>
<a class="sourceLine" id="cb10-11" data-line-number="11"></a>
<a class="sourceLine" id="cb10-12" data-line-number="12">net1 <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">2</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">2</span>)  </a></code></pre></div></li>
<li><p>method2—利用torch.nn.Sequential直接定义网络</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1">net2<span class="op">=</span>torch.nn.Sequential(</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">    torch.nn.Linear(<span class="dv">2</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb11-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb11-5" data-line-number="5">)</a></code></pre></div></li>
</ul>
<h3 id="五网络的保存和提取">五、网络的保存和提取</h3>
<ul>
<li><p>方法1：保存—提取</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">torch.save(net1, <span class="st">&#39;net.pkl&#39;</span>) <span class="co"># 保存整个网络，以pkl形式保存</span></a></code></pre></div>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1">net2 <span class="op">=</span> torch.load(<span class="st">&#39;net.pkl&#39;</span>)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2">prediction <span class="op">=</span> net2(x)</a></code></pre></div></li>
<li><p>方法2：保存—提取</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">torch.save(net1.state_dict(), <span class="st">&#39;net_params.pkl&#39;</span>) <span class="co"># 只保存网络中节点的参数 (速度快, 占内存少)</span></a></code></pre></div>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1">net3 <span class="op">=</span> torch.nn.Sequential(</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">    torch.nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb15-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb15-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb15-6" data-line-number="6">net3.load_state_dict(torch.load(<span class="st">&#39;net_params.pkl&#39;</span>))</a>
<a class="sourceLine" id="cb15-7" data-line-number="7">prediction <span class="op">=</span> net3(x)</a></code></pre></div></li>
</ul>
<h3 id="六批数据训练mini_batch-training">六、批数据训练(mini_batch training)</h3>
<ul>
<li><p>将数据分批训练，一个epoch训练所有批次的数据：</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">BATCH_SIZE <span class="op">=</span> <span class="dv">5</span> <span class="co"># 抽取训练的数据</span></a>
<a class="sourceLine" id="cb16-3" data-line-number="3"><span class="co"># BATCH_SIZE = 8</span></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"></a>
<a class="sourceLine" id="cb16-5" data-line-number="5">x <span class="op">=</span> torch.linspace(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">10</span>)       <span class="co"># this is x data (torch tensor)</span></a>
<a class="sourceLine" id="cb16-6" data-line-number="6">y <span class="op">=</span> torch.linspace(<span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">10</span>)       <span class="co"># this is y data (torch tensor)</span></a>
<a class="sourceLine" id="cb16-7" data-line-number="7"></a>
<a class="sourceLine" id="cb16-8" data-line-number="8">torch_dataset <span class="op">=</span> Data.TensorDataset(data_tensor <span class="op">=</span> x, target_tensor <span class="op">=</span> y)</a>
<a class="sourceLine" id="cb16-9" data-line-number="9">loader <span class="op">=</span> Data.DataLoader(</a>
<a class="sourceLine" id="cb16-10" data-line-number="10">    dataset<span class="op">=</span>torch_dataset,      <span class="co"># torch TensorDataset format</span></a>
<a class="sourceLine" id="cb16-11" data-line-number="11">    batch_size<span class="op">=</span>BATCH_SIZE,      <span class="co"># mini batch size</span></a>
<a class="sourceLine" id="cb16-12" data-line-number="12">    shuffle<span class="op">=</span><span class="va">True</span>,               <span class="co"># random shuffle for training</span></a>
<a class="sourceLine" id="cb16-13" data-line-number="13">    num_workers<span class="op">=</span><span class="dv">2</span>,              <span class="co"># 多线程来读数据</span></a>
<a class="sourceLine" id="cb16-14" data-line-number="14">)</a>
<a class="sourceLine" id="cb16-15" data-line-number="15"></a>
<a class="sourceLine" id="cb16-16" data-line-number="16"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):   <span class="co"># 训练所有!整套!数据 3 次</span></a>
<a class="sourceLine" id="cb16-17" data-line-number="17">    <span class="cf">for</span> step, (batch_x, batch_y) <span class="kw">in</span> <span class="bu">enumerate</span>(loader):  <span class="co"># 每一步 loader 释放一小批数据用来学习</span></a>
<a class="sourceLine" id="cb16-18" data-line-number="18">        <span class="co"># 假设这里就是你训练的地方...</span></a>
<a class="sourceLine" id="cb16-19" data-line-number="19">        <span class="co"># 打出来一些数据</span></a>
<a class="sourceLine" id="cb16-20" data-line-number="20">        <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| Step: &#39;</span>, step, <span class="st">&#39;| batch x: &#39;</span>,</a>
<a class="sourceLine" id="cb16-21" data-line-number="21">              batch_x.numpy(), <span class="st">&#39;| batch y: &#39;</span>, batch_y.numpy())</a></code></pre></div></li>
<li><p>DataLoader 是PyTorch中数据读取的接口，PyTorch训练模型基本都会用到该接口，其目的：将dataset根据batch_size大小、shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。</p></li>
<li><p>enumerate()函数 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。在这里就是把是个数据分成size为5的两份数据后，将每一份数据对应的下标给step，数据给(batch_x, batch_y)。</p></li>
</ul>
<h2 id="day-04">day 04</h2>
<h3 id="一优化器optimizer加速神经网络训练深度学习">一、优化器Optimizer加速神经网络训练（深度学习）</h3>
<ul>
<li>数据分批送入网络，进行SGD优化；</li>
<li>Momentum更新参数方法：<span class="math inline">\(m=b1*m-Learningrate*dx,W+=m\)</span></li>
<li>AdaGrad：<span class="math inline">\(v+=dx^2，W+=-Learning rate*dx/\sqrt v\)</span></li>
<li>RMSProp方法(上述两种的合并)：<span class="math inline">\(v=b1*v+(1-b1)*dx^2,W+=-Learning_rate*dx/\sqrt v\)</span></li>
<li>Adam:<span class="math inline">\(m = b1*m+(1-b1)*dx\)</span>——&gt;Momentum 　　　 <span class="math inline">\(v = b2*v+(1-b2)*dx^2\)</span>——&gt;AdaGrad 　　　 <span class="math inline">\(W+=-Learning_rate*m/\sqrt v\)</span></li>
</ul>
<h3 id="二opttimizer优化器">二、Opttimizer优化器 　</h3>
<ul>
<li><p>几种常见优化器： 　</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># different optimizers</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">opt_SGD <span class="op">=</span> torch.optim.SGD(net_SGD.parameters(), lr<span class="op">=</span>LR)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">opt_Momentum <span class="op">=</span> torch.optim.SGD(net_Momentum.parameters(), lr<span class="op">=</span>LR, momentum<span class="op">=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">opt_RMSprop <span class="op">=</span> torch.optim.RMSprop(net_RMSprop.parameters(), lr<span class="op">=</span>LR, alpha<span class="op">=</span><span class="fl">0.9</span>)</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">opt_Adam <span class="op">=</span> torch.optim.Adam(net_Adam.parameters(), lr<span class="op">=</span>LR, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.99</span>))</a>
<a class="sourceLine" id="cb17-6" data-line-number="6">optimizers <span class="op">=</span> [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</a></code></pre></div></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200323135943525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 　　从图中可以看出，目前性能最优的应该是Adam。</p>
<h3 id="三-卷积神经网络cnn">三、 卷积神经网络(CNN)</h3>
<p>　　图像处理中，不是对每个像素点卷积处理，而是对一小块区域进行计算，这样加强了图像信息的连续性，使得神经网络能看到图片信息而非一个点，同时加深了神经网络对图片的理解。批量过滤器每次对图像收集一小块信息，最后将这些整理出来得到边缘信息，再对这些信息进行类似的处理，得到更高层的信息结构(例如眼睛、鼻子等)，最后把总结出来的信息套入几层full connection进行分类等操作。卷积操作时，神经层会丢失一些信息，池化层可以将Layer中有用的信息筛选出来给下一层，因此图片的长宽不断压缩，压缩的工作是池化层进行的。</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1">1、import需要的工具包和库：torch、torchvision、torch.nn、torch.utils.data</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">2、超参数：EPOCH、BATCH_SIZE、LR</a>
<a class="sourceLine" id="cb18-3" data-line-number="3">3、下载mnist数据集：torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;./mnist/&#39;</span>,train<span class="op">=</span><span class="va">True</span>,transform<span class="op">=</span>torchvision.transform.ToTensor(),download<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb18-4" data-line-number="4"> <span class="co">#root是保存或提取的位置，transform是将数据集PIL.Image or numpy.ndarray转换成torch.FloatTensor(C×H×W)，训练的时候normalize成[0,1]间的值</span></a>
<a class="sourceLine" id="cb18-5" data-line-number="5"> test数据集处理：test—_x,test_y</a>
<a class="sourceLine" id="cb18-6" data-line-number="6"> 4、批训练train_loader定义：Data.DataLoader(dataset<span class="op">=</span>train_data,batch_size<span class="op">=</span>BATCH_SIZE,shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb18-7" data-line-number="7"> 5、定义网络构架CNN(nn.Module):conv1—conv2—RELU—pooling—conv2—ReLU—pooling—output</a>
<a class="sourceLine" id="cb18-8" data-line-number="8"> 网络计算流程：conv1(x)——conv2(x)——展平多维卷积图——计算输出</a>
<a class="sourceLine" id="cb18-9" data-line-number="9"> 6、定义optimizer和loss function</a>
<a class="sourceLine" id="cb18-10" data-line-number="10"> 7、训练和测试</a></code></pre></div>
<h3 id="四什么是lstm循环卷积网络rnn">四、什么是LSTM循环卷积网络(RNN)</h3>
<ul>
<li>LSTM(Long Short-Term Memory)——长短期记忆</li>
<li>RNN是在有序的数据上进行学习 <img src="https://img-blog.csdnimg.cn/20200323202037102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li>
<li>分类问题(mnist数据集) 　我们将图片数据看成一个时间上的连续数据, 每一行的像素点都是这个时刻的输入, 读完整张图片就是从上而下的读完了每行的像素点。然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了。</li>
<li>回归问题 这部分内容参考<a href="https://blog.csdn.net/qq_41639077/article/details/105218095" target="_blank" rel="noopener">KiKi的另一篇blog</a>，包括分类问题和回归问题的pytorch实现。</li>
</ul>
<h3 id="五自编码非监督学习autoencoder">五、自编码/非监督学习（Autoencoder）　</h3>
<p>　　原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作。 所以, 何不<strong>压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了。</strong> 训练好的自编码中间这一部分就是能总结原数据的精髓，我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。(摘自莫烦python) * 代码</p>
<pre><code>```python
self.encoder = nn.Sequential(
    nn.Linear(28*28, 128),
    nn.Tanh(),
    nn.Linear(128, 64),
    nn.Tanh(),
    nn.Linear(64, 12),
    nn.Tanh(),
    nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt
)

self.decoder = nn.Sequential(
    nn.Linear(3, 12),
    nn.Tanh(),
    nn.Linear(12, 64),
    nn.Tanh(),
    nn.Linear(64, 128),
    nn.Tanh(),
    nn.Linear(128, 28*28),
    nn.Sigmoid(),       # compress to a range (0, 1)
)

def forward(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return encoded, decoded

autoencoder = AutoEncoder()

```</code></pre>
<h3 id="六gan生成对抗网络">六、GAN—生成对抗网络</h3>
<p>　（原理已经学习过了，直接上代码）</p>
<ul>
<li><p>pytorch中实现：(代码中的对象不是图像，用到的是二次曲线) 　<br></p></li>
<li><p>超参数</p>
<pre><code>  ```python
  BATCH_SIZE = 64
  LR_G = 0.0001 # 生成器的学习率
  LR_D = 0.0001 # 判别器的学习率
  N_IDEAS = 5 # random_noise的个数
  ART_COMPONENTS = 15  # 定义规格，一条曲线上有多少个点
  PAINT_POINTS = np.vstack([np.linspace(-1,1,ART_COMPONENTS)for _ in range(BATCH_SIZE)]) # 规定整批画的点，从-1到1共15个点
  ``` </code></pre></li>
<li><p>没有train data，自己伪造一些real data</p>
<pre><code>  ```python
  def artist_works():     # painting from the famous artist (real target)
      a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis] # 二次曲线的系数
      paintings = a * np.power(PAINT_POINTS, 2) + (a-1)  # 二次曲线的参数，区间表示upper和
      paintings = torch.from_numpy(paintings).float()
      return paintings
  ```     </code></pre></li>
<li><p>定义生成器和判别器</p>
<pre><code>  ```python
  G = nn.Sequential(                      # Generator
      nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)
      nn.ReLU(),
      nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas
  )

  D = nn.Sequential(                      # Discriminator
      nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G
      nn.ReLU(),
      nn.Linear(128, 1),
      nn.Sigmoid(),                       # tell the probability that the art work is made by artist
  )
  ```</code></pre></li>
<li><p>优化器</p>
<pre><code>  ```python
      opt_D = torch.optim.Adam(D.parameters(), lr=LR_D)
  opt_G = torch.optim.Adam(G.parameters(), lr=LR_G)
  ```</code></pre></li>
<li><p>训练啦</p>
<pre><code>  ```python
  for step in range(10000):
      artist_paintings = artist_works()           # real painting from artist
      G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas
      G_paintings = G(G_ideas)                    # fake painting from G (random ideas)

      prob_artist0 = D(artist_paintings)          # D try to increase this prob
      prob_artist1 = D(G_paintings)               # D try to reduce this prob

      D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))
      G_loss = torch.mean(torch.log(1. - prob_artist1))

      opt_D.zero_grad()
      D_loss.backward(retain_graph=True)      # reusing computational graph
      opt_D.step()

      opt_G.zero_grad()
      G_loss.backward()
      opt_G.step()
  ```</code></pre></li>
</ul>
<p><strong>补充：</strong> cGAN与GAN的区别在于多了一个类别标签，这个label会跟随noise一起输入到生成器中，并且也要跟随fake和real一起输入到判别其中，最终计算各自的loss。</p>
<h3 id="七为什么torch是动态的待补充">七、为什么Torch是动态的<font color="red">(待补充)</font></h3>
<p>　例子：RNN网络 　Tensorflow就是预先定义好要做的task的框架、步骤，然后开启会话之后喂数据一步到位的计算出结果，开启会话后便不能修改网络构架了，只能是照着计算流图跟着计算，所以是静态的；Torch也可以先定义好框架然后套进去，但计算的时候无论网络怎么变化每一个叶子节点的梯度都能给出，tensorflow就做不到这一点，并且torch是边给出计算图纸一边进行训练。torch就像是散装的一样，可以一块一块的制作好并进行计算，比较灵活，所以是动态的。</p>
<h3 id="八gpu加速">八、GPU加速</h3>
<p>　以之前CNN为例，对其代码进行修改</p>
<ul>
<li><p>dataset部分</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1">test_x <span class="op">=</span> torch.unsqueeze(test_data.test_data, dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(torch.FloatTensor).cuda()<span class="op">/</span><span class="fl">255.</span>   <span class="co"># Tensor on GPU</span></a>
<a class="sourceLine" id="cb25-2" data-line-number="2">test_y <span class="op">=</span> test_data.test_labels.cuda()</a></code></pre></div></li>
<li><p>CNN网络的参数改为GPU兼容形式</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">class</span> CNN(nn.Module):</a>
<a class="sourceLine" id="cb26-2" data-line-number="2">    ...</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">cnn <span class="op">=</span> CNN()</a>
<a class="sourceLine" id="cb26-4" data-line-number="4"><span class="co">##########转换cnn到CUDA#########</span></a>
<a class="sourceLine" id="cb26-5" data-line-number="5">cnn.cuda()   <span class="co"># Moves all model parameters and buffers to the GPU.</span></a></code></pre></div></li>
<li><p>training data变成GPU形式</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="cf">for</span> epoch ..:</a>
<a class="sourceLine" id="cb27-2" data-line-number="2">    <span class="cf">for</span> step, ...:</a>
<a class="sourceLine" id="cb27-3" data-line-number="3">        <span class="co">##########修改1###########</span></a>
<a class="sourceLine" id="cb27-4" data-line-number="4">        b_x <span class="op">=</span> x.cuda()    <span class="co"># Tensor on GPU</span></a>
<a class="sourceLine" id="cb27-5" data-line-number="5">        b_y <span class="op">=</span> y.cuda()    <span class="co"># Tensor on GPU</span></a>
<a class="sourceLine" id="cb27-6" data-line-number="6">        ...</a>
<a class="sourceLine" id="cb27-7" data-line-number="7"></a>
<a class="sourceLine" id="cb27-8" data-line-number="8">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb27-9" data-line-number="9">            test_output <span class="op">=</span> cnn(test_x)</a>
<a class="sourceLine" id="cb27-10" data-line-number="10"></a>
<a class="sourceLine" id="cb27-11" data-line-number="11">            <span class="co"># !!!!!!!! 修改2  !!!!!!!!! #</span></a>
<a class="sourceLine" id="cb27-12" data-line-number="12">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a>
<a class="sourceLine" id="cb27-13" data-line-number="13"></a>
<a class="sourceLine" id="cb27-14" data-line-number="14">            accuracy <span class="op">=</span> torch.<span class="bu">sum</span>(pred_y <span class="op">==</span> test_y) <span class="op">/</span> test_y.size(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb27-15" data-line-number="15">            ...</a>
<a class="sourceLine" id="cb27-16" data-line-number="16"></a>
<a class="sourceLine" id="cb27-17" data-line-number="17">test_output <span class="op">=</span> cnn(test_x[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb27-18" data-line-number="18"></a>
<a class="sourceLine" id="cb27-19" data-line-number="19"><span class="co"># !!!!!!!! 修改3 !!!!!!!!! #</span></a>
<a class="sourceLine" id="cb27-20" data-line-number="20">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a>
<a class="sourceLine" id="cb27-21" data-line-number="21">...</a>
<a class="sourceLine" id="cb27-22" data-line-number="22"><span class="bu">print</span>(test_y[:<span class="dv">10</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div></li>
</ul>
<h2 id="day-05">day 05</h2>
<h3 id="一过拟合overfitting">一、过拟合(Overfitting)</h3>
<ul>
<li><p>过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。模型在训练集上效果好，然而在测试集上效果差，模型泛化能力差。</p></li>
<li><p>原因 <br> 　1）在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候； <br> 　2）权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。 　</p></li>
<li><p>解决方法</p>
<p>方法一： <strong>增加数据量</strong>。 <br> 方法二：<strong>运用正规化</strong>，L1、 L2 regularization等等。（神经网络的正规化方法<strong>dropout</strong>——就是在训练的时候, 随机忽略掉一些神经元和神经联结 , 使这个神经网络变得”不完整”，用这个不完整的神经网络训练一次。第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象每次训练的时候, 让每一次预测结果不会依赖于其中某部分特定的神经元。像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数，Dropout 的做法是从根本上让神经网络没机会过度依赖。）</p></li>
<li><p>Dropout</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># 不加dropout的网络</span></a>
<a class="sourceLine" id="cb28-2" data-line-number="2">net_overfitting <span class="op">=</span> torch.nn.Sequential(</a>
<a class="sourceLine" id="cb28-3" data-line-number="3">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a>
<a class="sourceLine" id="cb28-4" data-line-number="4">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb28-5" data-line-number="5">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a>
<a class="sourceLine" id="cb28-6" data-line-number="6">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb28-7" data-line-number="7">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb28-8" data-line-number="8">    )</a>
<a class="sourceLine" id="cb28-9" data-line-number="9"></a>
<a class="sourceLine" id="cb28-10" data-line-number="10">    <span class="co"># 加上dropout</span></a>
<a class="sourceLine" id="cb28-11" data-line-number="11">net_dropped <span class="op">=</span> torch.nn.Sequential(</a>
<a class="sourceLine" id="cb28-12" data-line-number="12">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a>
<a class="sourceLine" id="cb28-13" data-line-number="13">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a>
<a class="sourceLine" id="cb28-14" data-line-number="14">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb28-15" data-line-number="15">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a>
<a class="sourceLine" id="cb28-16" data-line-number="16">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a>
<a class="sourceLine" id="cb28-17" data-line-number="17">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb28-18" data-line-number="18">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb28-19" data-line-number="19">    )＃除了网络构架不同外，其他大同小异。</a></code></pre></div>
<p>　 　</p></li>
</ul>
<h3 id="二批标准化batch-normalization">二、批标准化(Batch Normalization)</h3>
<ul>
<li><p>什么是批标准化 <br> 　Batch Normalization(BN), 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法。 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律。数据随着神经网络的传递计算，激活函数的存在会造成网络层对数据的不敏感，比如0.1和2经过Tanh函数后，前者仍0.1，而2变成1，那这样再大的数都会变成1，所以神经层对数据失去了感觉，这样的问题同样存在于隐藏层中，所以BN则是用在这些神经层中优化网络的方法。 <br> 　Batch就是数据分批处理，每一批数据前向传递的过程中，每一层都进行BN处理，添加在层和激励函数之间。反BN：<span class="math inline">\(BN_(\gamma,\beta)(x_i)\)</span>是将 normalize 后的数据再扩展和平移，是为了让神经网络自己去学着使用和修改这个扩展参数 <span class="math inline">\(\gamma\)</span>, 和 平移参数<span class="math inline">\(\beta\)</span>, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 <span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>来抵消一些 normalization 的操作。</p></li>
<li><p>代码 　莫烦<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py" target="_blank" rel="noopener">BN_code</a></p></li>
</ul>
<p>　　　　　　<font color="red">部分内容待学习</font></p>
]]></content>
      <categories>
        <category>work</category>
        <category>pytorch学习</category>
      </categories>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>RNN-循环神经网络入门</title>
    <url>/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/</url>
    <content><![CDATA[<h2 id="循环神经网络入门pytorch实现">循环神经网络入门（pytorch实现）</h2>
<p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著  参考视频：<a href="https://www.bilibili.com/video/BV134411w76S" target="_blank" rel="noopener">微软人工智能公开课—循环神经网络RNN</a></p>
<p><strong>一、回顾前馈神经网络</strong> <br> 　　RNN是在前馈式神经网络基础上的，所以先回顾一下前馈神经网络。 <br> 　　前馈式神经网络（FNN, Feedforward Neural Network）一般的结构如下图： 　　<img src="https://img-blog.csdnimg.cn/20200403141749146.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="前馈式神经网络"> 图中神经元的输出表示为：<span class="math inline">\(O_t=f(\sum_{i=1}^n a_iw_i+b)\)</span>假设某一层的输出为<span class="math inline">\(h_n\)</span>，则下一层的输出为<span class="math inline">\(f_{out}(h_n)\)</span>，这样重复迭代网络的输出为<span class="math inline">\(y=f_{out}(f_n(f_{n-1}(...)))\)</span>，如此一来神经网络就成了巨大的复合函数，但实际我们并不会用到这个函数来计算，而使用计算图来表示，这样方便得出每一个节点的输入输出数据的梯度。对于RNN，就从这样的简单的前馈神经网络的传递方式说起。</p>
<p><strong>二、RNN网络简介</strong> <br> 　　循环神经网络（ Recurrent Neural Network, RNN ）雏形见于美国物理学家 J.J.Hopfield 于 1982 年提出的可用作联想存储器的互联网络——Hopfield 神经网络模型，如下图，每个节点都有输入，两两节点之间有双相连接： 　　<img src="https://img-blog.csdnimg.cn/20200403144156566.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Hopfiled Network"> 　　基于传统的机器学习算法十分依赖人工特征的提取致使模型一直无法提高正确率，而之前的全连神经网络以及卷积神经网络中，信息只在层与层之间存在运算关系，而没有连接节点之间的信息流动，这样在每一个时间都会有一个单独的参数，<font color="red">因此不能在时间上共享不同序列长度或序列不同位置的统计强度，所以无法对训练时没有见过的序列长度进行泛化</font>。 所以发展RNN对具有时间序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，在模型的不同部分共享参数解决了这个问题，并使得模型能够扩展到对不同形式的样本（这里指不同长度的样本）进行泛化。 <strong>循环神经网络会在几个时间步内共享相同的权重以刻画一个序列当前的输出与之前信息的关系，这体现在结构上是循环神经网络的隐藏层之间存在连接，隐藏层的输入来自于输入层的数据以及上一时刻隐藏层的输出</strong>。这样的结构使得循环神经网络会对之前的信息有所记忆， 同时利用之前的信息影响后面节点的输出。 <br> 　　RNN网络主要用于处理离散序列数据：离散线性、长度可变的序列，例如时域语音信号，金融市场走势等。网络可用于序列数据的分析（市场趋势预测）、序列数据的生产（基于图片的文字描述）、序列数据的转换（语音识别以及机器翻译）。</p>
<p><strong>三、循环网络结构</strong> <br> 　 　循环神经网络典型结构及其按时间先后展开结构如图所示：　 　 　<img src="https://img-blog.csdnimg.cn/20200403160334980.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="RNN"> 对于主体结构<span class="math inline">\(A\)</span>，一般可认为是循环神经网络的一个隐藏单元。在<span class="math inline">\(t\)</span>时刻，主体结构<span class="math inline">\(A\)</span>会读取输入层的输入<span class="math inline">\(x_t\)</span>以及上一时刻的输出，并输出当前时刻的<span class="math inline">\(o_t\)</span>值（图中未给出）。此后<span class="math inline">\(A\)</span>结构在<span class="math inline">\(t\)</span>时刻的状态值表达式：<span class="math inline">\(h^t=f(h^{t-1},x_t;\theta)\)</span>，其中<span class="math inline">\(\theta\)</span>可以是网络中的其它参数比如权重或偏置等，循环的过程就是<span class="math inline">\(A\)</span>不断被执行的过程。但是循环神经网络目前无法做到无限循环，因为循环过多会出现梯度消失的问题。 <br> 　　假设隐藏单元的激活函数是tanh函数，则循环体结构<span class="math inline">\(A\)</span>如下：　　<img src="https://img-blog.csdnimg.cn/20200403155812724.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="循环体"> 则隐藏单元状态值表示为：<span class="math inline">\(h^t=tanh(Wh^{t-1}+Ux_t+b^h)\)</span>，<span class="math inline">\(b^h\)</span>是由<span class="math inline">\(x_t\)</span>得到<span class="math inline">\(h^t\)</span>的偏置，<span class="math inline">\(W\)</span>是相邻时刻隐藏单元间的权重矩阵，<span class="math inline">\(U\)</span>是从<span class="math inline">\(x_t\)</span>计算得到这个隐藏单元时用到的权重矩阵。从当前的状态值<span class="math inline">\(h^t\)</span>都得到输出还需要一个全连接神经网络来完成这个过程，表达式为：<span class="math inline">\(o^t=b_o+Vh^t\)</span>，<span class="math inline">\(V\)</span>是由<span class="math inline">\(h^t\)</span>得到<span class="math inline">\(o^t\)</span>的权重矩阵。如果输出是离散的，则可以用softmax处理<span class="math inline">\(o^t\)</span>得到标准化的概率向量<span class="math inline">\(y\)</span>：<span class="math inline">\(y^t=softmax(o^t)\)</span>，向量的值对应离散变量可能值的概率。</p>
<p><strong>四、网络的训练</strong> <br> 　　要对网络进行训练，需要有一个与<span class="math inline">\(x\)</span>序列配对的<span class="math inline">\(o\)</span>的所有时间步内的总loss。 <br> 　　RNN的反向传播算法称为时间反向传播 (Back-Propagation Through Time, BPTT)。基本原理和 BP 算法是一样的，三步走：前向计算每个神经元的输出值；反向计算误差项值；计算每个权重的梯度；最后再用随机梯度下降算法更新权重。详细的计算公式参考<a href="https://zhuanlan.zhihu.com/p/85776566" target="_blank" rel="noopener">知乎</a>。在反向传播过程中，当输入序列过长的时候，在求取一个比较远的时刻的梯度时，需要回溯到前面的所有时刻的信息，由于连乘项的存在，导致前面时刻的信息会缺失，这就是RNN中的梯度消失问题，还有就是当连乘出现大于1时的梯度爆炸，后者采用clip的方式即可，前者将会用到LSTM来缓解。</p>
<p><strong>五、LSTM</strong> <br> 　　LSTM 结构由 Sepp Hochreiter教授和 Jurgen Schrnidhuber教授于 1997 年 提出，它本身就是一种特殊的循环体结构 。在一个整体的循环神经网络中，除了外部的RNN大循环（<strong>循环体是 LSTM</strong>）外，还需要考虑 LSTM 本身单元“细胞”之间的自循环。LSTM单元结构如下图： 　　<img src="https://img-blog.csdnimg.cn/20200403173928627.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM单元"> LSTM由自身的三个门结构进行控制，门结构使用Sigmoid函数对输入的信息进行控制，让信息有选择性的影响RNN中每个时刻的状态。遗忘门（Forget Gate）是让网络“忘记”之前没用的信息，上一时刻的状态值通过与自循环权重 (该权重就是遗忘门的输出) 进行位乘可得到当前时刻状态值的一个加数。遗忘门输出表达式：</p>
<p><span class="math display">\[f_i^t=sigmoid(b_i^f+\sum U_{i,j}^fx_j^t+\sum W_{i,j}^fh_j^{t-1})\]</span></p>
<p>其中，<span class="math inline">\(h^{t-1}\)</span>是包含一个LSTM细胞上一时刻的所有输出，可以被看作是当前隐藏层向量，数量为<span class="math inline">\(j\)</span>，<span class="math inline">\(W\)</span>是循环权重。要保存长期的记忆，还需要输入门，同样是根据<span class="math inline">\(x^t\)</span>、<span class="math inline">\(b^g\)</span>和<span class="math inline">\(h^{t-1}\)</span>来决定哪些部分将进入当前时刻的状态，输入门的值表示为：</p>
<p><span class="math display">\[g_i^t=sigmoid(b_i^g+\sum U_{i,j}^gx_j^t+\sum W_{i,j}^gh_j^{t-1})\]</span></p>
<p>进一步计算LSTM结构当前时刻的状态值：</p>
<p><span class="math display">\[C_i^t=f_i^tC_i^{t-1}+g_i^ttanh(b_i+\sum U_{i,j}x_j^t+\sum W_{i,j}h_j^{t-1})\]</span></p>
<p>其中，<span class="math inline">\(W\)</span>值为遗忘门的循环权重。然后得出结构当前时刻的输出值：</p>
<p><span class="math display">\[h^t=tanh(C_i^t)q_i^t\]</span></p>
<p><span class="math display">\[q_i^t=sigmoid(b_i^q+\sum U_{i,j}^qx_j^t+\sum W_{i,j}^qh_j^{t-1})\]</span></p>
<p>其中，<span class="math inline">\(W^q\)</span>为遗忘门的循环权重，<span class="math inline">\(q_i^t\)</span>为输出们的输出值。用LSTM作为循环体的RNN网络如下图所示： <img src="https://img-blog.csdnimg.cn/20200403192938761.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM-RNN"></p>
<p>　　循环神经网络的变种还有：双向循环神经网络和深层循环神经网络。</p>
<ul>
<li>至此，关于RNN和LSTM的学习就结束啦~ 用pytorch实现LSTM的RNN对MNIST数据集分类，以及RNN实现一元二次曲线的回归的代码地址：<a href="https://gitee.com/sparklekk/RNN" target="_blank" rel="noopener">sparklekk</a></li>
</ul>
]]></content>
      <categories>
        <category>work</category>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>循环神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>总结SLAM相关知识点资源库以及企业</title>
    <url>/2020/03/31/%E8%A7%86%E8%A7%89SLAM%E7%9F%A5%E8%AF%86%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E4%BC%81%E4%B8%9A%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h3 id="视觉slam的知识资源以及相关企业总结">视觉SLAM的知识资源以及相关企业总结</h3>
<pre><code>先在这儿记录一下，SLAM涉及的太多了，现在接触的只是冰山一角。</code></pre>
<p>声名：我只是微信公众号<font color="red">计算机视觉life</font>的搬运工。 <bar> <bar> <bar></bar></bar></bar></p>
<h4 id="一slam知识库">一、SLAM知识库</h4>
<p><strong>1、SLAM框架/算法流程</strong></p>
<p><a href="img-Nb7dyKIQ-1586231125936"></a><img src="https://img-blog.csdnimg.cn/20200407114420210.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" width="50%"></p>
<p><strong>2、应掌握的数学知识</strong></p>
<ul>
<li><strong>矩阵</strong>：四则运算、求逆、反对称矩阵；矩阵分解（SVD、QR、Cholesky）。</li>
<li><strong>李群与李代数</strong>：指数对数映射；李代数求导；扰动模型。</li>
<li><strong>非线性优化</strong>：梯度下降；牛顿法；高斯牛顿法；LM算法；Bundle Adjustment。</li>
<li><strong>微积分</strong>：求(偏)导、泰勒展开等。</li>
</ul>
<p><strong>3、专业知识</strong> <br> - <strong>计算机视觉</strong> <br> 　<strong>①</strong> 传感器类型： <br> 　　　激光雷达 <br> 　　　视觉传感器：单目相机、双目相机、RGB-D相机、全景相机、Event相机。 <br> 　　　IMU (Inertial measurement unit，测量物体三轴姿态角及加速度的装置) <br> 　<strong>②</strong> 相机： <br> 　　　针孔相机模型、双目相机模型、RGB-D相机模型、相机标定、 去畸变。 <br> 　<strong>③</strong> 特征点 <br> 　　　特征点检测、特征点描述子、特征点匹配、特征点筛选。 <br> 　<strong>④</strong> 多视角几何 <br> 　　　对极约束、本质矩阵、单应矩阵、三角化。 <br>　　　 - <strong>书籍文献</strong> <br> 　《视觉SLAM十四讲》、《Multiple View Geometry》、《机器人状态估计》</p>
<p><strong>4、编程环境</strong></p>
<ul>
<li>Linux系统操作：推荐Ubuntu16.04 <br> 　　　　　　　　书《鸟哥的Linux私房菜》 　　　　　　　　</li>
<li>开发环境：Clion（JetBrains推出的C/C++跨平台集成开发环境） <br> 　　　　　　Kdevelop（免费的） 　　　　　　</li>
<li><p>编译工具：学习跨平台编译器cmake、电子书《Cmake Practice》</p></li>
<li>第三方库：Opencv(计算机视觉)、Eigen(几何变换)、Sophus(李代数)、Ceres(非线性优化)、G2o(图优化)、OpenGL(计算机图形学) 　　　　　　</li>
<li><p>文档编辑：Gedit、Vim、Nano</p></li>
</ul>
<p><strong>5、应用场景</strong></p>
<ul>
<li><p>自动驾驶：作为激光传感器的辅助（百度、腾讯、驭势、图森）</p></li>
<li><p>增强现实：手机、智能眼镜上结合IMU用于定位（微软、三星、华为、虹软、悉见）</p></li>
<li><p>机器人：无人机定位、建立地图（大疆）；服务机器人室内定位及导航（思岚）；工业机器人定位导航（阿里、京东AGV仓库运输）</p></li>
<li><p>三维重建：物体重建（3D打印、3D虚拟试衣）；大场景重建（虚拟全景漫游）</p></li>
</ul>
<p><strong>6、公开数据集</strong> <br> 　　TUM RGB-D SLAM Dataset、KITTI Vision Benchmark Suite、EuRoC MAV Dataset.</p>
<p><strong>7、典型开源方案</strong></p>
<ul>
<li>稀疏法：ORB-SLAM2（单目、双目、RGB-D）</li>
<li>半稠密法：LSD-SLAM（单目、双目、RGB-D）、DSO（单目）</li>
<li>稠密法：Elastic Fusion（RGB-D）、Bundle Fusion（RGB-D）、 　　　　　RGB-D SLAM V2（RGB-D）</li>
<li>多传感器融合：VINS（单目+IMU）、OKVIS（单目、双目、四目+IMU）</li>
</ul>
<h4 id="二slam学习资源总结">二、SLAM学习资源总结</h4>
<p><strong>1、公众号</strong> <br> 　　泡泡机器人SLAM、计算机视觉life、3D视觉工坊、小白学视觉、计算机视觉之路、AI算法修炼营、PCL点云。 　</p>
<p><strong>2、视频公开课</strong> <br> 　　① <a href="https://space.bilibili.com/38737757" target="_blank" rel="noopener">泡泡机器人公开课</a> <br> 　　② <a href="https://space.bilibili.com/45189691" target="_blank" rel="noopener">计算机视觉life SLAM研习社直播公开课</a> <br> 　　③ <a href="https://www.dis.uniroma1.it/~labrococo/tutorial_icra_2016/" target="_blank" rel="noopener">SLAM Totorial@ICRA 2016</a> <br> 　　④ <a href="https://www.coursera.org/specializations/robotics" target="_blank" rel="noopener">Robotics-UPenn on Coursera by Vijay Kumar(2016)</a> <br> 　　⑤ <a href="https://vision.in.tum.de/teaching/ss2016/mvg2016" target="_blank" rel="noopener">Computer Vision Ⅱ：Multiple View Geometry-TUM by Daniel Cremers (Spring 2016)</a> <br> 　　⑥ <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa15/" target="_blank" rel="noopener">Advanced Robotics-UCBerkeley by Pieter Abbeel (Fall 2015)</a> <br> 　　⑦ <a href="https://ylatif.github.io/movingsensors/" target="_blank" rel="noopener">The Problem of Mobile Sensors</a> <br></p>
<p><strong>3、主要研究方向</strong> <br> 　　视觉里程计：线特征；点线特征融合；点面特征融合；点线面多特征融合；多鱼眼VO；特征点发直接法结合；抗光照变化。 <br></p>
<p>　　语义SLAM：3D语义地图重定位；语义建图互相促进。 <br></p>
<p>　　多传感器融合：VIO（松耦合、紧耦合）；多传感器在线标定；camera，lidar融合定位；lidar，天花板摄像头融合；lidar，imu融合；TOF辅助双目密集匹配；事件相机SLAM；多机协同SLAM。 <br></p>
<p>优化方法：非线性增量优化；BA改进；离散连续图模型优化；边缘的约束图优化。 <br></p>
<p>　　三维重建：大尺度场景下的实时重建；RGBD室内稠密重建；地图数据关联；MVS；3D物体分类，提升重建质量；多子图融合。 <br></p>
<p>　　结合深度学习：基于深度学习的特征点；结合深度学习的深度估计；深度学习做闭环检测；深度学习估计关键帧深度。</p>
<p><strong>4、会议期刊</strong></p>
<ul>
<li><p>英文期刊 <br> ICRA（IEEE International Conference on Robotics and Automation） <br> IROS（IEEE International Conference on Intelligent Robots and Systems） <br> CVPR（IEEE International Conference on Computer Vision and Pattern Recognition） <br> RSS（Robotics：Science and Systems） <br> ECCV（European Conference on Computer Vision） <br> ISMAR（IEEE and ACM International Symposium on Mixed and Augmented Reality.IEEE） <br> JFR（Journal of Field Robotics） <br> IEEE Transactions on Robotics <br> ACRA（Australian Conference on Robotics and Automation） <br> ICARCV（International Conference on Cotrol，Automation，Robotics and Vision） <br> ISR（International Symposium on Robotics） <br> IEEE Transactions On Pattern Analysis And achine Intelligence <br></p></li>
<li><p>中文期刊 <br> 计算机辅助设计与图形学学报 <br> 机器人 <br> 计算机应用研究 <br> 中国科学：信息科学 <br></p></li>
</ul>
<p><strong>5、知名研究实验室</strong></p>
<ul>
<li><p>欧洲 <br> 苏黎世联邦理工学院的Autonomous System Lab <br> 苏黎世大学Robotics and Perception Group <br> 慕尼黑工业大学The Computer Vision Group <br> 英国伦敦大学帝国理工学院 Dyson 机器人实验室 <br> 英国牛津大学Active Vision Laboratory <br> 德国弗莱堡大学Autonomous Intelligent Systems <br> 西班牙萨拉戈萨大学SLAM实验室 <br></p></li>
<li><p>北美 <br> 麻省理工计算机科学与人工智能实验室（CSAIL）海洋机器人组 <br> 明尼苏达大学Multiple Autonomous Robotics Systems Laboratory <br> 宾夕法尼亚大学GRASP实验室 <br> 华盛顿大学UW Robotics and Estimation Lab <br> 哥伦比亚大学计算机视觉与机器人组 <br> 加拿大谢布鲁克大学IntRoLab <br> 斯坦福大学人工智能实验室自动驾驶团队 <br> 卡内基梅隆大学Robot Perception Lab <br> 特拉华大学Robot Perception and Navigation Group</p></li>
<li><p>亚洲 <br> 香港科技大学Aerial Robotics Group <br> 浙江大学CAD&amp;CG国家重点实验室Computer Vision Group <br> 清华大学自动化系宽带网络与数字媒体实验室BBNC <br> 中科院自动化研究所国家模式识别实验室Robot Vision Group <br> 上海交通大学感知与导航研究所 <br> 武汉大学Computer Vision &amp; Remote Sensing Lab <br> 日本先进工业科技研究所 <br> 筑波大学智能机器人研究室 <br> 新加坡南洋理工大学HESL实验室 <br> 韩国科学技术研究院 <br></p></li>
<li><p>澳洲 <br> 澳大利亚悉尼科技大数学CAS实验室 <br> 澳大利亚机器学习研究所机器人视觉中心 <br></p></li>
</ul>
<h4 id="三slam相关企业">三、SLAM相关企业</h4>
<p><strong>1、移动机器人</strong></p>
<ul>
<li><p>扫地机器人 <br> 科沃斯（苏州）、石头（北京）、追觅（上海）、银星智能（深圳）</p></li>
<li><p>服务机器人 <br> 优必选（深圳）、达闼（北京）、思岚（上海）、高仙（上海）、猎户星空（北京）、速感（北京）、普渡（深圳）、美团（北京）</p></li>
<li><p>仓储机器人 <br> 旷视（北京、上海）、京东（北京）、顺丰（深圳）、海康威视（杭州）、极智嘉（北京）</p></li>
</ul>
<p><strong>2、相机传感器</strong></p>
<ul>
<li><p>双目相机 <br> 小觅（无锡）、Indemind（北京）、爱观（上海）、中科慧眼（北京）</p></li>
<li><p>RGBD相机 <br> Intel（北京、上海）、奥比中光（深圳）、Pico（青岛、北京）、图漾（上海）、云从（重庆）</p></li>
<li><p>激光雷达 <br> 禾赛（上海）、镭神智能（深圳）、速腾聚创（深圳）</p></li>
<li><p>事件相机 <br> 芯仑（上海）</p></li>
</ul>
<p><strong>3、无人机</strong> <br> 　　大疆（深圳、上海）、亿航（广州）、臻迪（北京）</p>
<p><font color="red"><strong>4、智能驾驶</strong></font></p>
<ul>
<li><p>无人驾驶 <br> 百度（北京）、阿里菜鸟/达摩院（杭州）、腾讯（北京）、驭势（北京、上海）、momenta（北京）、滴滴（北京）、图森（北京）、飞步科技（杭州）、纽励科技（上海）、小马智行（广州）、Aptiv（上海）、文远知行（广州）</p></li>
<li><p>辅助驾驶 <br> 纵目科技（上海）、魔视智能（上海）、极目智能（武汉）、虹软（杭州）、商汤（北京）</p></li>
<li><p>芯片 <br> NVIDIA（上海）、地平线（北京、南京）</p></li>
<li><p>高精地图 <br> 高德（北京）、四维图新（北京）</p></li>
<li><p>汽车厂商 <br> 上汽研究院（上海）、蔚来汽车（上海）、小鹏汽车（广州）、宇通客车（郑州）</p></li>
</ul>
<p><strong>5、增强现实</strong></p>
<ul>
<li><p>智能眼镜 <br> 联想（上海）、视辰（上海）、肇观电子（上海）、悉见（北京）</p></li>
<li><p>手机 <br> 华为（上海）、虹软（杭州、上海）、商汤（杭州）、亮风台（上海）、今日头条（北京）</p></li>
</ul>
]]></content>
      <categories>
        <category>work</category>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>总结经典深度学习网络（pytorch、tensorflow实现）</title>
    <url>/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/</url>
    <content><![CDATA[<h2 id="总结经典的深度学习模型">总结经典的深度学习模型</h2>
<p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著</p>
<h3 id="一lenet-5">一、LeNet-5</h3>
<p><strong>1、LeNet-5网络简介</strong></p>
<p>　　LeNet-5是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。LeNet-5 模型由Yann LeCun教授于1998年提出， 在MNIST数据集上，LeNet-5模型可以达到大约99.4%的准确率。与近几年的卷积神经网络比较，LeNet-5的网络规模比较小，但却包含了构成现代CNN网络的基本组件——卷积层、 Pooling层、全连接层。 <br> <strong>2、模型结构</strong> <br> 　　网络一共有8层(包含输入和输出在内)，基本网络架构如下： <img src="https://img-blog.csdnimg.cn/20200401163708326.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="ＬｅＮｅｔ－５"> 备注：图中C表示卷积层 ，S表示池化层。　<br></p>
<ul>
<li><p><font color="red"><strong>Layer1</strong></font>：Input层，输入图片大小32×32×1，MNIST数据集大小是28×28，所以输入reshape为32×32是希望高层特征监测感受野的中心能够收集更多潜在的明显特征。</p></li>
<li><p><font color="red"><strong>Layer2</strong></font>：Conv1层——卷积层，有6个Feature Map，即Convolutions操作时的卷积核的个数为6，卷积核大小为5×5，该层每个单元和输入层的25个单元连接，padding没有使用，stride为1。</p></li>
<li><p><font color="red"><strong>Layer3</strong></font>：S2为下采样层(Subsampling)，6个14×14的特征图，是上一层的每个特征图经过2×2的最大池化操作得到的，且长宽步长都为2，S2层每个特征图的每一个单元与C3对应特征图中2×2大小的区域连接。</p></li>
<li><p><font color="red"><strong>Layer4</strong></font>：Conv3层，由上一层的特征图经过卷积操作得到，卷积核大小为5×5，卷积核个数为16，即该层有16个特征图，但并不是与上一层的6个特征图一一对应，而是有固定的连接关系，例如第一个特征图只与Layer3的第1、2、3个特征图的有卷积关系。</p></li>
<li><p><font color="red"><strong>Layer5</strong></font>：S4层，16个5×5的特征图，每个特征图都是由第四层经过一个2×2的最大池化操作得到的，意义同Layer3。</p></li>
<li><p><font color="red"><strong>Layer6</strong></font>：Conv5，120个特征图，是由上一层输出经过120个大小为5×5的卷积核得到的，没有padding，stride为1，上一层的16个特征图都连接到该层的每一个单元，所以这里相当于一个全连接层。</p></li>
<li><p><font color="red"><strong>Layer7</strong></font>：F6是全连接层，有84个神经元，与上一层构成全连接的关系，再经由Sigmoid激活函数传到输出层。</p></li>
<li><p><font color="red"><strong>Layer8</strong></font>：Output层也是一个全连接层，共有10个单元，对应0~9十个数字。本层单元计算的是径向基函数：<span class="math inline">\(y_i =\sum_{j}(x-w_{i,j})^2\)</span>,RBF的计算与第i个数字的比特图编码有关，对于第i个单元，yi的值越接近0，则表示越接近第i个数字的比特编码，即识别当前输入的结果为第i个数字。</p></li>
</ul>
<p><strong>3、pytorch和tensorflow实现LeNet-5网络的MNIST手写数字识别</strong> <br> 　　代码地址：<a href="https://github.com/KK-xi/LeNet-5" target="_blank" rel="noopener">GitHub</a> <br> 　　LeNet-5网络规模比较小，所以它无法很好的处理类似ImageNet的比较大的图像数据集。</p>
<h3 id="二alexnet">二、AlexNet</h3>
<p>1、<strong>AlexNet网络简介</strong> <br> 　　2012年， Hinton的学生Alex Krizhevsky借助深度学习的相关理论提出了深度卷积神经网络模型AlexNet。卷积层的数量有5个，池化层的数量有3个，也就是说，并不是所有的卷积层后面都连接有池化层。在这些卷积与池化层之后是3个全连层，最后一个全连层的单元数量为1000个，用于完成对ImageNet数据集中的图片完成1000分类（具体分类通过Softmax层实现)。</p>
<p>2、<strong>模型结构</strong> <br>　　 　　网络结构如下图所示，有两个子网络，可以用<font color="red">GPU分别进行训练</font>(特点1)，两个GPU之间存在通信： 　　<img src="https://img-blog.csdnimg.cn/20200401164504200.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="AlexNet"></p>
<ul>
<li><p><font color="red"><strong>第一段卷积</strong></font>: 使用了96个11×11卷积核对输入的224×224×3的RGB图像进行卷积操作，长宽移动步长均为4，得到的结果是96个55x55的特征图；得到基本的卷积数据后，第二个操作是<font color="red">ReLU去线性化</font>(这是AlexNet的特点2)；第三个操作是<font color="red">LRN局部归一化</font>（AlexNet首次提出,特点3)；第四个操作是3×3的max pooling，步长为2。</p></li>
<li><p><font color="red"><strong>第二段卷积</strong></font>：流程类似第一阶段，用到的核大小不同，256个深度为3(因为是三通道图像)的5×5的卷积核，stride为1×1，然后ReLU去线性化，再是LRN局部归一化，最后是3×3的最大池化操作，步长为2。</p></li>
<li><p><font color="red"><strong>第三段卷积</strong></font>：输入上一阶段的特征图，首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，这一阶段没有LRN和池化。</p></li>
<li><p><font color="red"><strong>第四段卷积</strong></font>：首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化。</p></li>
<li><p><font color="red"><strong>第五段卷积</strong></font>：首先经过256个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，最后是3×3的最大池化操作，步长为2。</p></li>
<li><p><font color="red"><strong>第六段全连接(FC)</strong></font>：以上过程完了以后经过三层全连接层，前两层有4096个单元，与前一层构成全连接关系，然后都经过一个ReLU函数，左后一层是1000个单元的全连接层(Softmax层)，训练时这几个<font color="red">全连接层使用了Dropout</font>(特点4)。</p></li>
</ul>
<p>备注：<font color="red">数据增强的运用</font>(特点5)，在训练的时候模型随机从大小为256×256的原始图像中截取224×224大小的区域，同时还得到图像水平翻转的镜像图，用以增加样本的数量。在测试时，模型会首先截取一张图片的四个角加中间的位置，并进行左右翻转，这样会获得10张图片，将这10张图片作为预测的输入并对得到的10个预测结果求均值，就是这张图片最终的预测结果。</p>
<p><strong>3、pytorch和tensorflow实现</strong> <br> 　　代码地址：<a href="https://github.com/KK-xi/AlexNet" target="_blank" rel="noopener">Github</a></p>
<h3 id="三vggnet">三、VGGNet</h3>
<p><strong>1、VGGNet网络简介</strong> <br> 　　2014年ILSVRC图像分类竞赛的第二名是VGGNet网络模型，其 top-5 错误率为 7.3%，它对卷积神经网络的深度与其性能之间的关系进行了探索。在将网络迁移到其他图片数据上进行应用时， VGGNet比GoogleNet有着更好的泛化性。此外，VGGNet模型是从图像中提取特征的CNN首选算法。<br></p>
<p><strong>2、模型结构</strong> <br> 　　网络的结构非常简洁，在整个网络中全部使用了大小相同的卷积核 (3×3 )和 最大油化核 (2x2 )。<font color="red">VGGNet模型通过不断地加深网络结构来提升性能</font>，通过重复堆叠的方式，使用这些卷积层和最大池化层成功地搭建了11～19层深的卷积神经网络。下表是这些网络的结构组成层： <img src="https://img-blog.csdnimg.cn/20200401200252309.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGGNet"> 　　表中的conv3表示大小为3×3的卷积核，conv1则是1×1的卷积核，参数量主要集中在全连接层，其他卷积层的参数共享和局部连接降低了参数的数量。表中五个阶段的卷积用于提取特征，每段卷积后面都有最大池化操作，目的是缩小图像尺寸。多个卷积的堆叠可以降低参数量，也有助于学习特征，C级的VGG用到conv1是为了在输入通道数和输出通道数不变(不发生数据降维)的情况下实现线性变换，对非线性提升效果有较好的作用，但是换成conv3效果更好。VGG19的效果只比VGG16好一点点，所以牛津的研究团队就停止在VGG19层了，不再增加更多层数了。下图是VGG-16的网络构架图： 　　<img src="https://img-blog.csdnimg.cn/20200401203149754.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGG-16"></p>
<p><strong>3、pytorch和tensorflow实现</strong> <br> 　　这里实现的是VGG-16网络(pytorch中只是搭建了网络)，代码地址：<a href="https://github.com/KK-xi/VGGNet-16" target="_blank" rel="noopener">GitHub</a> 　　在通常的网络训练时，<font color="red">VGGNet通过Multi-Scale方法对图像进行数据增强处理</font>，我自己在训练MNIST的时候没有用数据增强。因为卷积神经网络对于图像的缩放有一定的不变性，所以将这种经过 Multi-Scale多尺度缩放裁剪后的图片合在一起输入到卷积神经网络中训练，可以增加网络的这种<font color="red">不变性</font>(不变性的理解：pooling操作时，对局部感受野取其极大值，如果图像在尺度上发生了变化，有一定概率在尺度变化后对应的感受野取到的极大值不变，这样就可以使特征图不变，同样也增加了一定的平移不变性)。在预测时也采用了Multi-Scale的方法，将图像Scale到一个尺寸再裁剪并输入到卷积网络计算。输入到网络中的图片是某一张图片经过缩放裁剪后的多个样本，这样会得到一张图片的多个分类结果，所以紧接着要做的事就是对这些分类结果进行平均以得到最后这张图片的分类结果，这种平均的方式会提高图片数据的利用率并使分类的效果变好。</p>
<h3 id="四inceptionnet-v3">四、InceptionNet-V3</h3>
<p><strong>1、InceptionNet-V3网络简介</strong> <br> 　Google的InceptionNet首次亮相是在2014年的ILSVRC比赛中，称为 Inception-V1，后来又开发了三个版本，其中InceptionNet-V3最具代表性。相比VGGNet, Inception-V1增加了深度，达到了22层，但是其参数却只有500万个左右(SM)，这是远低于AlexNet(60M 左右)和VGGNet (140M 左右)的，是因为该网络将全连层和一般的卷积中采用了<font color="red">稀疏连接的方法(Hebbian原理)</font>。根据相关性高的单元应该被聚集在一起的结论，这些在同一空间位置但在不同通道的卷积核的输出结果也是稀疏的，也可以通过类似将稀疏矩阵聚类为较为密集的子矩阵的方式来提高计算性能。沿着这样的一个思路，Google团队提出了Inception Module结构来实现这样的目标。</p>
<p><strong>2、模型结构</strong> <br> 　　网络中主要用到了稀疏连接的思想提出了<font color="red">Inception Module</font>，其借鉴了论文《Network in Network》的做法，即提出的<font color="red">MLPConv</font>——使用MLP对卷积操作得到的特征图进行进一步的操作，从而得到本层的最终输出特征图，这样可以允许在输出通道之间组合信息，以此提升卷积层的表达能力。在InceptionNet-V3中，Inception Module的基本结构如下图：　　<img src="https://img-blog.csdnimg.cn/20200401233024519.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Inception Module"> 结构中的<font color="red">Filter Concat</font>是将特征图在深度方向(channel维度)进行串联拼接，这样可以构建出符合Hebbian原理的稀疏结构。conv1的作用是把相关性高、同一空间位置不同通道的特征连接在一起，并且计算量小，<strong>可以增加一层特征变换和非线性化</strong>，最大池化是为了增加网络对不同尺度的适应性。在Inception V2中首次用到了<font color="red">.批标准化(Batch Normalization)</font>，V3在V2基础上的Module<font color="red">改进之处就是在分支中使用分支，将二维卷积拆分为两个非对称一维卷积</font>，这种卷积可以在处理更丰富的空间特征以及增加特征多样性等方面做得比普通卷积更好。多个这种Inception Module堆叠起来就形成了InceptionNet-V3，其网络构架如下图，整个网络的主要思想就是找到一个最优的Inception Module结构，更好的实现局部稀疏的稠密化。 <img src="https://img-blog.csdnimg.cn/20200401235114326.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="InceptionNet-V3"></p>
<p><strong>3、使用InceptionNet-V3完成模型迁移学习</strong></p>
<ul>
<li><font color="red">迁移学习</font>：随着卷积神经网络模型层数的加深以及复杂度的逐渐增加，训练这些模型所需要的带有标注的数据也越来越多。比如对于ResNet，其深度有 152 层，使用ImageNet数据集中带有标注的 120 万张图片才能将其训练得到 96.5% 的准确率。尽管这是个比较不错的准确率，但是在真实应用中，几乎很难收集到这么多带有标注的图片数据，而且这些数据训练一个复杂的卷积神经网络也要花费很长的时间 。<strong>迁移学习的出现就是为了解决上述标注数据以及训练时间的问题</strong>。所谓迁移学习，就是将一个问题上训练好的模型通过简单的调整使其适用于 一个新的问题，例如在 Google 提供的基于 ImageNet 数据集训练好的 Inception V3 模型的基础上进行简单的修改，使其能够解决基于其他数据集的图片分类任务。被修改的全连接层之前的一个网络层叫做<font color="red">瓶颈层</font>(Bottleneck，这里是V3中最后一个Dropout层)。</li>
<li>模型迁移学习tensorflow代码地址：<a href="https://github.com/KK-xi/Inception_V3_transfer_learning" target="_blank" rel="noopener">Github</a></li>
</ul>
<h3 id="五resnet">五、ResNet</h3>
<p><strong>1、ResNet网络简介</strong> <br> 　　ResNet (Residual Neural Network）由微软研究院的何情明等 4 名华人提出，网络深度达到了152 层，top-5 错误率 只有3.57% 。虽然ResNet的深度远远高于 VGGNet，但是参数量却比 VGGNet 低，效果也更好。ResNet 中最创新就是<font color="red">残差学习单元（Residual Unit）</font>的引入，它是参考了瑞士教授 Schmidhuber 的论文《Training Very Deep Networks》中提出的 Highway Network。 <font color="red"> Highway Network</font> 的出现是为了解决较深的神经 网络难以训练的问题，主要思想是启发于LSTM的门(Gate)结构，使得有一定比例的前一层的信息没有经过矩阵乘法和非线性变换而是直接传输到下一层，网络要学习的就是原始信息应该以何种比例保留下来。后来残差网络的学习单元就受益于Highway Network加深网络层数的做法。</p>
<p><strong>2、模型结构</strong> <br> 　　随着网络加深，由于反向传播过程的叠乘可能出现<font color="red">梯度消失</font>，结果就是准确率下降，为此ReNet中引入残差学习单元(如图所示，2层和3层)，思想就是对于一个达到了准确率饱和的较浅网络，在后面加几个全等映射层允许初始信息直接传到下一层(y=x)时，误差不会因此而增加，并且网络要学习的就是原来输出<span class="math inline">\(H(x)\)</span>与原始输入<span class="math inline">\(x\)</span>的残差<span class="math inline">\(F(x)=H(x)-x\)</span>。 <img src="https://img-blog.csdnimg.cn/20200402093002712.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="残差单元"> 　　将多个残差单元堆叠起来就组成了ResNet网络，如下图所示是一个34层的残差网络： <img src="https://img-blog.csdnimg.cn/20200402095125460.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="ResNet"> 旁边的支线就是将上一层残差单元的输出直接参与到该层的输出，这样的连接方式被称为<font color="red">Shortcut和Skip Connection</font>，这样可以一定程度上保护信息的完整性。最后在改进的ResNet V2中通过该连接使用的激活函数(ReLU)被更换为Identity Mappings (<span class="math inline">\(y=x\)</span>)，残差单元都使用了BN归一化处理，使得训练更加容易并且泛化能力更强。</p>
<p><strong>3、pytorch和tensorflow实现ResNet</strong> <br> 　代码地址：<a href="https://github.com/KK-xi/ResNet" target="_blank" rel="noopener">GitHub</a> 　这里只是搭建的网络，没有进行任何任务的训练，可以在自己的数据集上训练试一下。</p>
]]></content>
      <categories>
        <category>work</category>
        <category>深度学习模型</category>
      </categories>
      <tags>
        <tag>深度学习网络</tag>
      </tags>
  </entry>
  <entry>
    <title>图像处理的注意力机制</title>
    <url>/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    <content><![CDATA[<h3 id="图像处理中的attention-mechanism">图像处理中的Attention Mechanism</h3>
<pre><code>摘要：关于在图像处理任务(图像分类、超分辨率、图像描述、图像分割等)中添加注意力机制的问题，计算机视觉（computer vision）中的注意力机制（attention）就是想让系统学会注意重点信息。</code></pre>
<p>参考博客：<a href="https://blog.csdn.net/xys430381_1/article/details/89323444" target="_blank" rel="noopener">blog1</a>总结的很好，<a href="https://blog.csdn.net/bvl10101111/article/details/78470716" target="_blank" rel="noopener">blog2</a>给了添加注意力的基本套路，<a href="https://blog.csdn.net/Wayne2019/article/details/78488142" target="_blank" rel="noopener">blog3</a>也可以看看。</p>
<p>参考文献：《Spatial transformer networks》、《Squeeze and Excitation Networks》、《CBAM: Convolutional Block Attention Module》</p>
<h4 id="一图像处理的注意力机制基本概念">一、图像处理的注意力机制基本概念</h4>
<p>　　通常举的例子来讲，有一幅图：一只鸟儿翱翔在天空。如果要对这幅图进行鸟类的分类识别任务，人眼去看可能就关注这只鸟是什么鸟，神经网络去做这个任务就要提取特征然后再分类，他对于这幅图中像素点的处理都是一样的，不会像人眼一样专门关注鸟儿，所以神经网络需要我们告知它重点关注哪部分，这就是注意力机制。 <br> 　　现在的深度学习与视觉注意力机制结合的研究工作，大多数是集中于使用<font color="red">掩码(mask)</font>来形成注意力机制。(<strong>掩码的原理</strong>就是：根据掩码矩阵（也称作核）重新计算图像中每个像素的值。掩码矩阵中的值表示一层新的权重，将图片数据中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。)</p>
<h4 id="二注意力机制分类">二、注意力机制分类</h4>
<p><font color="navy">▲ <strong>按注意力的可微性来分</strong></font> <font color="navy"><strong>1、软注意力 (soft attention)</strong></font> <br> 　　每个区域被关注的程度高低，用0~1的score表示。软注意力的关键点在于，这种注意力更关注区域或者通道，而且软注意力是确定性的注意力，学习完成后直接可以通过网络生成，最关键的地方是软注意力是可微的。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。软注意力的注意力域可以分为：空间域 (spatial domain)、通道域(channel domain)、层域(layer domain)、混合域(mixed domain)。</p>
<p><strong>① 空间域</strong>： <br> 　　从文章<font color="maroon"><strong>《Spatial transformer networks》</strong></font>来了解空间注意力机制，其中的spatial transformer就起到了空间注意力机制的作用，不仅可以选择空间中最关注（注意力集中）的区域，还可以将这些区域转换为规范的、预期的姿态（Spatial transformer具有旋转、缩放变换的功能，这样图片局部的重要信息能够通过变换而被框盒提取出来），从而简化以下层的识别。下面说说这篇文章：</p>
<ul>
<li><p><font color="teal"><strong>文章的思路及其好处</strong></font>：在CNNs中只有有限的（小空间的）、预先定义好的池化机制（max pooling 或者average pooling 的方法）来处理数据空间排列的变化，将图片信息压缩以减少运算量提升准确率，但这样导致了空间不变性只在较深的网络层中存在，而在输入数据变换较大时实际上不存在这种不变性。这篇文章中的空间转换模块优点：一个spatial transformer可以将适当的区域裁剪和缩放标准化，这样可以简化后续的分类任务，并获得更好的分类性能；给定一组包含相同（但未知）类的不同实例的图像，可以使用spatial transformer在每个图像中框选它们；利用spatial transformer（注意力）的好处在于，低分辨率的输入可以转换为高分辨率的原始输入，从而提高计算效率。</p></li>
<li><p><font color="teal"><strong>模型构架</strong></font>：spatial transformer可以放在任意网络中，并且计算速度快，其构架如下图： <img src="https://img-blog.csdnimg.cn/20200411231633386.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="100%"> 空间转换模块(注意力机制模块)有三个组成部分：</p>
<p><strong>Localisation Network</strong> —— 为了便于计算，首先用一个定位网络(Localisation Network) 获取输入的特征图<span class="math inline">\(U∈R^{H×W×C}\)</span>，并通过若干隐藏层输出应用于特征图的空间变换参数<span class="math inline">\(θ=floc(U)\)</span>。本地化网络函数<span class="math inline">\(floc()\)</span>可以采用任何形式，例如完全连接的网络或卷积网络，但应包括最终回归层，以生成转换参数<span class="math inline">\(θ\)</span>。</p>
<p><strong>Grid generator</strong>—— 使用变换参数<span class="math inline">\(θ\)</span>生成一个采样网格 (即变换矩阵<span class="math inline">\(T_θ(G)\)</span>)，该网格是一组对输入映射进行采样从而产生变换后的输出点，这是由Grid generator完成的。下图是文章的一个例子： 　　　　 <img src="https://img-blog.csdnimg.cn/20200412091624329.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="80%"> 其中，(a)中的采样矩阵是单位矩阵，不做任何变换，其中I是恒等变换参数。(b)中的矩阵是可以产生缩放旋转变换<span class="math inline">\(T_θ(G)\)</span>的采样矩阵。</p>
<p><strong>Sampler</strong>—— 最后，将原始特征图 U 和变换器作为采样器的输入，生成最后的特征图V，其中 V 的像素点是输入 U 经由上一阶段的采样网格作用的结果。现假设这个变换是2D的情况 (假设<span class="math inline">\(T_θ=A_θ\)</span>)，变换公式如下： <span class="math display">\[ \left(\begin{array}{c}x_i^s \\y_i^s\end{array}\right) =T_θ(G_i)=A_θ\left(\begin{array}{c}x_i^t \\y_i^t\\1\end{array}\right) =\left[\begin{array}{ccc}θ_{11}&amp;θ_{12}&amp;θ_{13}\\θ_{21}&amp;θ_{22}&amp;θ_{23}\end{array}\right] \left(\begin{array}{c}x_i^t \\y_i^t\\1\end{array}\right)\]</span> 公式中<span class="math inline">\((x_i^t ，y_i^t)\)</span>是输出特征图的常规网格的目标坐标，<span class="math inline">\((x_i^s，y_i^s)\)</span>是输入特征图中定义采样点的源坐标，<span class="math inline">\(A_θ\)</span>是仿射变换矩阵。宽度和高度坐标均为归一化的。当然，也有定义3D情况下的变换，允许裁剪、平移、旋转和缩放。这个模块加进去最大的好处就是能够对上一层信号的关键信息进行识别(即添加attention)。</p></li>
<li><p><font color="teal"><strong>结果讨论</strong></font>：由于文章的方法对通道信息没有特别的处理变换，所以这种变换更适合用在添加卷积层之前的变换，因为每一个卷积核(filter)在卷积操作以后都会产生的通道信息，而且它们含有的信息以及信息的关注程度是不一样的。所以有了第二种注意域的机制——通道域。</p></li>
</ul>
<p><strong>② 通道域</strong>： <br> 　　从文章<font color="maroon"><strong>《Squeeze and Excitation Networks》</strong></font>来了解通道注意力机制。计算机视觉任务中，捕获特征的空间相关性可以提升CNNs的性能，以上有了空间注意力，这篇文章就研究了网络设计的不同方面，即添加通道注意力。</p>
<ul>
<li><p><font color="teal"><strong>文章的思路及其好处</strong></font>：文章引入了一种新的架构单元 Squeeze-and-Excitation (SE) block，其目的是通过利用卷积特征之间的相互依赖关系来提高网络的质量。文中提出了一种机制，允许网络执行特征重新校准，通过这种机制，网络可以学习使用全局信息来选择性地强调信息特征和抑制不太有用的特征。</p></li>
<li><p><font color="teal"><strong>模型构架</strong></font>：多个这种SE block聚合在一起就构成了SENet。文章所提到的SE blok构架如下图所示： <img src="https://img-blog.csdnimg.cn/20200412131746928.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="-"> 对于任何给定的变换<span class="math inline">\(F_{tr}\)</span> (例后边将其当作如卷积)，将输入X变换到<span class="math inline">\(U∈R^{H×W×C}\)</span>，可以构造相应的SE block来执行特征重新校准。重先对一些符号做提前声明：<span class="math inline">\(V=[v_1,v_2,...,v_c]\)</span>表示变换<span class="math inline">\(F_{tr}\)</span>的一系列卷积核，<span class="math inline">\(v_c\)</span>表示第 c 个卷积的参数；卷积之后的输出<span class="math inline">\(U=[u_1,u_2,...,u_c]\)</span>，其中<span class="math inline">\(u_c=v_c*X\)</span>，需注意具体过程是vc的一个单通道 (二维空间核<span class="math inline">\(v_c^s\)</span>) 作用于x的对应通道 (<span class="math inline">\(x^s\)</span>) 上，为了简化表示省略了偏置项。校准的三个过程过程就是通道注意力机制的过程：</p>
<p><strong>挤压(squeeze， Global Information Embedding)</strong> —— 由于卷积操作只是局部感受野，最终输出的特征图的每个小单元不能用来探索除开这个小区域以外的联系特征，所以该方法首先将特征 U 通过挤压操作传递，挤压操作通过压缩其空间维度（H×W）上的特征来生成通道描述符。基于通道的特征量<span class="math inline">\(z∈R^c\)</span>，其第 c 个量计算方式如下： 　　　　　　　　　<img src="https://img-blog.csdnimg.cn/20200412151403621.png" srcset="/img/loading.gif" width="50%"> 其实这里就是全局平均池化操作。</p>
<p><strong>激励(excitation )</strong> —— 聚合之后是一个激励操作，其目的是捕获通道依赖关系，为满足条件，文章用到的机制函数如下：<span class="math display">\[s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))\]</span>其中的<span class="math inline">\(\sigma\)</span>表示ReLU函数，<span class="math inline">\(\delta\)</span>是sigmoid函数，用于生成channel 间0~1的 attention weights（注意力权重用于激励每一层通道，权重通过学习生成）。为简化模型，在非线性前接了一个两层全连接FC的botteneck (一个降维层压缩 channel 数)，然后再是ReLU和升维层重构回 channel 数。</p>
<strong>缩放(scale)</strong> —— block的最终输出是通过激活s重新缩放 U 来获得的，其公式如下：<span class="math display">\[\tilde{x}=F_{scale}(u_c,s_c)=s_cu_c\]</span>其中<span class="math inline">\(\tilde{X}=[\tilde{x_1},\tilde{x_2},...,\tilde{x_c}]\)</span>，<span class="math inline">\(F_{scale}(u_c,s_c)\)</span>表<span class="math inline">\(s_c\)</span>与H×W维度上的特征图<span class="math inline">\(u_c\)</span>的乘积，不同通道的值乘上不同的权重，从而可以增强对关键通道域的注意力。 　　</li>
<li><p><font color="teal"><strong>结果讨论</strong></font>：通道域的注意力是对一个通道内的信息直接全局平均池化，而忽略每一个通道内的局部信息，这种做法其实也是比较简单粗暴。所以结合两种思路，就可以设计出混合域的注意力机制模型。</p></li>
</ul>
<p><strong>③混合域</strong>： <br> 　　从2018年的文章<font color="maroon"><strong>《CBAM: Convolutional Block Attention Module》</strong></font>来了解混合域注意力机制。这篇文章是基于 SE-Net 的 Squeeze-and-Excitation block进行进一步拓展，可以理解为文中把 channel attention看成是教网络关注什么；而spatial attention 是教网络关注何处。</p>
<ul>
<li><p><font color="teal"><strong>文章的思路及好处</strong></font>：提出了一个新模块“卷积块注意模块”。由于卷积运算通过将跨通道和空间信息融合在一起来提取信息特征，因此文中用到的模块是用来注意沿这两个维度（通道和空间轴）的重要特征。文章的贡献之处有：注意力模块（CBAM）简单而有效，该模块可应用于提高CNN提取描述符的能力。</p></li>
<li><p><font color="teal"><strong>模型构架</strong></font>：在SENet的基础上进行改进，得到CBAM模型基本构架如下图所示：<img src="https://img-blog.csdnimg.cn/2020041223143241.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="CBAM"> 给定一个中间特征图<span class="math inline">\(F∈R^{C×H×W}\)</span>作为输入，CBAM依次得到1D的通道注意图<span class="math inline">\(M_c∈R^{C×1×1}\)</span>和2D空间注意图$M_s∈R^{1×H×W}。 整个注意力过程可以总结为： <span class="math display">\[F&#39; = M_c(F)⊗F\]</span><span class="math display">\[F&#39;&#39;= M_s(F_0)⊗F_0\]</span>其中⊗表示逐元素乘法。 在乘法过程中，注意值会相应地传递：通道注意值会沿空间维度传递，反之亦然。 <span class="math inline">\(F&#39;&#39;\)</span>是最终输出。下面介绍每个注意模块：</p>
<p><strong>Channel attention module</strong> —— 该子模块构架如下图： <img src="https://img-blog.csdnimg.cn/20200412235136512.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif"> 为计算通道注意力，我们压缩输入特征图的空间尺寸，过使用平均池化操作和最大池化操作来收集特征图的部分信息，生成两个不同的空间上下文描述符：<span class="math inline">\(F^c_{avg}\)</span>和<span class="math inline">\(F^c_{max}\)</span>，分别表示平均池化特征和最大池化特征。然后将两个描述符传送到共享网络(多层感知器MLP组成)，以生成我们的频道注意图<span class="math inline">\(M_c\)</span>。为了减少参数，隐藏层的激活输出大小设置为<span class="math inline">\(R^{C / r×1×1}\)</span>，其中 r 是缩小率。通道注意力的计算公式为：<span class="math display">\[
M_c(F)=σ(MLP(AvgPool(F))+ MLP(MaxPool(F)))=σ(W_1(W_0(F^c_{avg}))+ W_1(W_0(F^c_{max})))\]</span> 其中 σ 表示Sigmoid函数，<span class="math inline">\(W_0∈R^{C/ r×C}\)</span>，<span class="math inline">\(W_1∈R^{C×C / r}\)</span>。两个输入均共享MLP权重<span class="math inline">\(W_0\)</span>和<span class="math inline">\(W_1\)</span>，并且ReLU激活后才乘<span class="math inline">\(W_0\)</span>。</p>
<p><strong>Spatial attention module</strong> —— 空间注意力子模块的构架如下图： <img src="https://img-blog.csdnimg.cn/20200412235258831.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif"> 利用特征之间的空间关系来生成空间注​​意图，空间注意力集中在“哪里”是一个信息部分，与通道注意力是互补的。为了计算空间注意力，首先沿通道轴应用平均池化和最大池化操作，来聚合特征图的通道信息，生成两个二维图：<span class="math inline">\(F^s_{avg}∈R^{1×H×W}\)</span>和<span class="math inline">\(F^s_{max}∈R^{1×H×W}\)</span>。在级联的特征描述符上，应用卷积层以生成空间注​​意图<span class="math inline">\(M_s(F)∈R^{H×W}\)</span>，该图对强调或抑制的位置进行编码。，空间注意力的计算公式为： 　　　　　　　　<img src="https://img-blog.csdnimg.cn/20200413000142165.png" srcset="/img/loading.gif" width="60%"></p>
其中σ表示S型函数，<span class="math inline">\(f^{7×7}\)</span>表示大小为7×7的卷积核。下图是一个ResNet中嵌入这个CBAM块的Resblock构架图： <img src="https://img-blog.csdnimg.cn/20200413000728407.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif"></li>
<li><p><font color="teal"><strong>结果讨论</strong></font>：这里的CBAM模块是轻量级的。所以放在网络中也比较方便，而且包含了通道信息和空间信息的处理，总体上比SENet要好。</p></li>
</ul>
<p><font color="blue"><strong>2、硬注意力</strong> (hard attention)</font> <br> 　　首先强注意力是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更强调动态变化。当然，最关键是强注意力是一个不可微的注意力，训练过程往往是通过增强学习(reinforcement learning)来完成的。Hard-attention，就是0/1问题，哪些区域是被 attentioned，哪些区域不关注。在这里不做介绍。</p>
<p><font color="blue"><strong>3、自意力</strong> (self-attention)</font> <br> 　　自注意力机制是注意力机制的改进，其减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性。自注意力机制 (self-attention)在序列模型中取得了很大的进步；另外一方面，上下文信息（context information）对于很多视觉任务都很关键，如语义分割，目标检测。自注意力机制通过（key, query, value）的三元组提供了一种有效的捕捉全局上下文信息的建模方式。</p>
<p>　　<strong>自注意力的缺点和相应的改进策略</strong>：由于每一个点都要捕捉全局的上下文信息，这就导致了自注意力机制模块会有很大的计算复杂度和显存容量。如果我们能知道一些先验信息，比如上述的特征对其通常是一定的邻域内，我们可以通过限制在一定的邻域内来做，另外还有如何进行高效的稀疏化，以及自注意力机制和图卷积的联系。同时，这种建模方式的缺点还没有考虑channel上信息，相应的改进策略是如何进行spatial和channel上信息的有效结合。 　　不对自注意力机制做过多的介绍。</p>
<h4 id="三注意力机制的实现">三、注意力机制的实现</h4>
<p>主要参考别人复现的CBAM，即通道注意力和空间注意力结合的模型：pytorch实现，地址：<a href="https://github.com/KK-xi/CBAM-attention_mechanism-_pytorch" target="_blank" rel="noopener">github</a>。</p>
]]></content>
      <categories>
        <category>work</category>
        <category>attention mechanism</category>
      </categories>
      <tags>
        <tag>attention mechanism</tag>
      </tags>
  </entry>
  <entry>
    <title> 视觉SLAM十四讲 第一章</title>
    <url>/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/</url>
    <content><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2>
<p>作者：高翔 | 张涛 <br> 最后更新于：2017.03.31</p>
<h3 id="第一-章-前言">第一 章 前言</h3>
<h4 id="本书讲什么">1.1本书讲什么</h4>
<p>　　本书介绍视觉SLAM。</p>
<p>　　SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为 “视觉 SLAM”，此时我们要做的，就是根据一张张连续运动的图像（它们形成一段视频），从中推断相机的运动，以及周围环境的情况。</p>
<p>　　如今视觉SLAM的应用点有许多，在许多地方我们都想知道自己的位置：室内的扫地机和移动机器人需要定位，野外的自动驾驶汽车需要定位，空中的无人机需要 定位，虚拟现实和增强现实的设备也需要定位。</p>
<p>　　本书提及了 SLAM 的历史、理论、算法、现状，并且把完整的 SLAM 系统分成几个模块：视觉里程计、后端优化、建图以及回环检测。一些必要的数学理论和许多编程知识，会用到 <strong>Eigen、 OpenCV、PCL、g2o、Ceres </strong>等库。</p>
<h4 id="如何使用本书">1.2如何使用本书</h4>
<p><font color="red"><strong>理论+习题+编程</strong></font>： <br></p>
<p>１. 第一部分为<font color="maroon">数学基础篇</font>，铺垫与视觉 SLAM 相关的数学知识，包括： <br></p>
<p>　• 第二讲为 SLAM 系统概述，介绍一个 SLAM 系统由哪些模块组成，各模块的具体工作是什么。实践部分介绍编程环境的搭建过程以及 IDE 的使用。 <br></p>
<p>　• 第三讲介绍三维空间运动，将接触旋转矩阵、四元数、欧拉角的相关知识，并且在 Eigen 当中使用它们。 <br></p>
<p>　• 第四讲为李群和李代数。将学习李代数的定义和使用方式，然后通过 Sophus 操作它们。 <br></p>
<p>　 • 第五讲介绍针孔相机模型以及图像在计算机中的表达。将用 OpenCV 来调取相机的内外参数。 <br></p>
<p>　• 第六讲介绍非线性优化，包括状态估计理论基础、最小二乘问题、梯度下降方 法。完成一个使用 Ceres 和 g2o 进行曲线拟合的实验。 <br></p>
<p>２. 第二部分为 <font color="maroon">SLAM 技术篇</font>。使用第一部分所介绍的理论，讲述视觉 SLAM 中各个模块的工作原理： <br></p>
<p>　• 第七讲为特征点法的视觉里程计。该讲内容比较多，包括特征点的提取与匹配、 对极几何约束的计算、PnP 和 ICP 等。实践中用这些方法去估计两个图像之间的运动。 <br></p>
<p>　• 第八讲为直接法的视觉里程计。将学习光流和直接法的原理，然后利用 g2o 实现一个简单的 RGB-D 直接法。 <br></p>
<p>　• 第九讲为视觉里程计的实践章，将搭建一个视觉里程计框架，综合应用先学过的知识，实现它的基本功能。 <br></p>
<p>　• 第十讲为后端优化，主要为 Bundle Adjustment 的深入讨论，包括基本的 BA 以及如何利用稀疏性加速求解过程。 <br></p>
<p>　• 第十一讲主要讲后端优化中的位姿图。位姿图是表达关键帧之间约束的一种更紧凑的形式。将用 g2o 和 gtsam 对一个位姿球进行优化。 <br></p>
<p>　• 第十二讲为回环检测，我们主要介绍以词袋方法为主的回环检测。你将使用 dbow3 书写字典训练程序和回环检测程序。 <br></p>
<p>　• 第十三讲为地图构建。会讨论如何使用单目进行稠密深度图的估计（以及这是多么不可靠），然后讨论 RGB-D 的稠密地图构建过程。写极线搜索与块匹配的程序，然后在 RGB-D 中遇到点云地图和八叉树地图的构建问题。 <br></p>
<p>　• 第十四讲主要介绍当前的开源 SLAM 项目以及未来的发展方向。 <br></p>
<p><font color="red"><strong>需要具备的：Linux &amp; C++ &amp; 基础数学知识</strong></font>， 装一个Ubuntu子系统去，参考<a href="https://blog.csdn.net/daybreak222/article/details/87968078" target="_blank" rel="noopener">博客</a>。</p>
]]></content>
      <categories>
        <category>work</category>
        <category>视觉SLAM十四讲</category>
        <category>第一章</category>
      </categories>
      <tags>
        <tag>视觉SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>seu</title>
    <url>/2020/03/30/eu/</url>
    <content><![CDATA[<ul>
<li>2019.10.06 东大一角</li>
</ul>
<figure>
<img src="https://img-blog.csdnimg.cn/20200330115513856.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="seu"><figcaption>seu</figcaption>
</figure>
]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
  <entry>
    <title>单目深度估计总结</title>
    <url>/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/</url>
    <content><![CDATA[<p><bar> <bar></bar></bar></p>
<p><bar> <bar></bar></bar></p>
<p>《High Quality Monocular Depth Estimation via Transfer Learning》 作者：Ibraheem Alhashim and Peter Wonka</p>
<p>备注：只是一篇总结，不是解读哒~</p>
<h2 id="一为什么要看这篇文章">一、为什么要看这篇文章？</h2>
<p>　1、因为最近萌生了一个想法，觉得可以用用深度图；</p>
<p>　2、这篇文章相比其他的文章，模型结构更简单，深度图分辨率更高。</p>
<h2 id="二文章提出的出发点">二、文章提出的出发点</h2>
<p>　 1、首先，一张图片2D到3D的深度估计是很多场景理解或者是重建工作中的基础；</p>
<p>　2、其次，这篇文章希望提出的深度估计方法能获得高分辨率的深度估计结果。</p>
<p>　3、一些任务（例如图像增强的或用到3D重建等的）要求更快的计算速度。</p>
<p>　所以设计出一个更简单的模块化的网络构架，使得获得的深度图分辨率更高、质量更高，训练更容易，并且更方便的将以后在其他任务中表现较好的模型迁移到深度估计问题。</p>
<h2 id="三framework">三、Framework</h2>
<p>文章中给出的简化架构：<img src="https://img-blog.csdnimg.cn/20200314154230721.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="看似很简单的网络架构"> <strong>1、Encoder</strong></p>
<p>　　这里用到的编码器是另一篇文章《Densely Connected Convolutional Networks》里的DenseNet-169。下图是生长率k=4的5层的Dense-block结构图，对这里的k的一种解释是，每个层都可以访问这个块中的所有前面的特征映射，因此也可以访问网络的集体特征。可以将特征图视为网络的全局状态，每层将其自身的k个特征映射添加到该状态，增长率决定了每一层对全局状态贡献多少新信息。一旦写入全局状态，就可以从网络中的任何地方访问全局状态，并且与传统的网络架构不同，不需要从一层复制到另一层。 <img src="https://img-blog.csdnimg.cn/20200314160111493.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="5-layer dense block with growth rate of k=4"> 然后，为了在网络中进行下采样（改变图像大小），将多个Dense-block连接起来，就形成了下图的样子： <img src="https://img-blog.csdnimg.cn/20200314161447623.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="Dense Net with three dense blocks"> 两个相邻块之间的层称为过渡层，并通过卷积和池化来改变特征图的大小，但是本文中是删除了最后的top-layer，因为文章是做depth estimation而不是Classification task。但是呢，深度估计这篇文章用的DensNet-169有4个blocks，网络结构参数如下： <img src="https://img-blog.csdnimg.cn/20200314162029675.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="DenseNet architectures for ImageNet"></p>
<p><strong>2、Decoder</strong></p>
<p>　　对于decoder，从一个1×1的卷积层开始，其输出通道的数量与去掉top-layer的编码器的输出相同。然后依次添加2×2的双线性upsampling块和串接到encoder的池化层POOL，其后跟两个并列的3×3卷积层，这样的构造重复3次（不同之处在于串接到的池化层分别是Pool3，Pool2，Pool1），然后就是2×2的双线性upsampling块和串接到encoder的卷积层CONV1，然后是两个并列的3×3的卷积层（为了使该块的输出为输入通道数的一半），最后经过一个3×3卷积层最终输出通道数为1的图像。</p>
<p>　　这个encoder-decoder带有跳过连接，encoder构架不太紧凑，大概的结构就这样啦~~</p>
<h2 id="四文章的亮点">四、文章的亮点</h2>
<p>1、第一点大概就是用了这么一个简单的网络来做深度估计，网络复杂不等于结果好；　<br> 2、定义了一个损失函数，通过最小化深度值的差异来平衡重建深度图像之间的关系，同时惩罚深度图的图像域中高频细节的失真： <img src="https://img-blog.csdnimg.cn/20200314165259760.png" srcset="/img/loading.gif" alt="损失函数L"><br>
3、运用数据增强策略，文章只用了镜像翻转，以及改变颜色通道排列，后者还可以做一个further work； <br> 4、提出了一个新的test dataset。</p>
<h2 id="五结果以及改进空间">五、结果以及改进空间</h2>
<p>　　1、在室外场景中没别人的方法表现好，猜想是因为提供的深度图的性质（由于损失函数不仅要考虑逐点差异，而且还要通过查看每个点周围的区域来优化边缘和外观保留，因此对于非常稀疏的深度图像，学习过程不能很好地收敛）；　<br> 　　2、文章用到的方法仍然需要label； <br> 　　3、文章所提的改进空间挺多的，比如用在嵌入式设备上，这个网络存在局限性，以及更清楚地确定不同编码器、数据增强和学习策略对性能和贡献的影响，都是未来工作中值得关注的内容。</p>
]]></content>
      <categories>
        <category>work</category>
        <category>深度估计</category>
      </categories>
      <tags>
        <tag>paper conclusion</tag>
      </tags>
  </entry>
</search>
