<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.jpg">
  <link rel="icon" type="image/png" href="/img/avatar.jpg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="记录生活">
  <meta name="author" content="kiki">
  <meta name="keywords" content="CV">
  <title>paper reading 1| Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion - KK&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Welcome to here.</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/work/">工作</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/life/">生活</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期六, 五月 30日 2020, 8:59 早上
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    3.7k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      13 分钟
                  </span>
                

                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
            <div class="markdown-body">
              <h1 id="unsupervised-event-based-learning-of-optical-flow-depth-and-egomotion2019">Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion(2019)</h1>
<p>Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis University of Pennsylvania {alexzhu, lzyuan, chaneyk, kostas}<span class="citation" data-cites="seas.upenn.edu">@seas.upenn.edu</span></p>
<p><strong>Abstract：</strong> 在这项工作中，提出了一种用于事件相机无监督学习的新颖框架，该框架仅从事件流中学习运动信息。特别地，以离散量的形式提出事件的输入表示，该离散量保持事件的时间分布，通过神经网络来预测事件的运动。此运动用于尝试消除事件图像中的任何运动模糊。然后，我们提出一种应用于运动补偿事件图像的损耗函数，该函数可测量该图像中的运动模糊。我们在此框架下训练了两个网络，一个用于预测光学流，一个用于预测自我运动和深度，并在“多车立体视觉事件摄像机”数据集上评估这些网络，以及来自各种不同场景的定性结果。</p>
<h2 id="一文章思路">一、文章思路</h2>
<p>首先event-camera需要开发新的方法去解决传统SLAM/机器人系统中的一些问题。</p>
<p>现在基于事件相机的SLAM 的方法还比较少，作者针对两篇最近的paper[24,20]存在的缺陷进行了改进。<strong>缺点一：</strong> 这两篇文章都用了颜色一致性原理，分别用于灰度图像和事件图像，前者依赖于灰度图像，后者的颜色一致性假设可能在很模糊的场景中不成立；<strong>缺点二：</strong> 两项工作都将时间数据进行了汇总，这样一来就丢失了时间信息。</p>
<p><font color="red">文章提出的方法思路：</font><strong>其一</strong>提出了一个无监督网络构架；<strong>其二</strong>针对上述缺陷（一个用了灰度图，一个用事件图，且忽略时间信息）提出了一种新的输入表示，包含所有的时空信息（事件和时间信息都是离散化的），然后以类似于插值的线性加权方式来将事件积累到一起；<strong>其三</strong>提出了新的无监督Loss函数；<strong>其四</strong>同时训练两个网络，分别预测光流，深度和自运动，将预测结果用以消除在事件投影到二维图像时产生的运动模糊，如下图所示。关于无监督方式，无监督损失会测量去模糊后的事件图像中的运动模糊量，从而为网络提供训练信号，另外还将去模糊事件图像可与edge maps比较，对这它们的人口普查变换应用了立体损失，以使我们的网络能够了解度量姿态和深度。 <img src="https://img-blog.csdnimg.cn/20200530082857175.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图 1：网络学习通过从一组事件的输入，模糊，事件（左）中预测光流（顶部）或自运动和深度（底部），并通过对图像进行去模糊后使运动模糊量最小化，来从运动模糊中预测运动。 产生去模糊图像的预测运动（右）。 彩色效果最佳。</p>
<h2 id="二文章贡献">二、文章贡献</h2>
<ol type="1">
<li><p>事件表示上：提出新的离散的事件表示形式，将事件输入到神经网络中；</p></li>
<li><p>无监督损失：一个基于运动模糊损失函数的新应用，只允许从事件中无监督地学习运动信息（改编自[13]）；</p></li>
<li><p>损失：一种新的立体视觉相似度损失应用于一对去模糊事件图像的人口普查变换；</p></li>
<li><p>评估方法上：对Multi Vehicle Stereo Event Camera dataset[26]进行定量评估，并对各种夜间和其他具有挑战性的场景进行定性和定量评估。</p></li>
</ol>
<h2 id="三相关的工作">三、相关的工作</h2>
<p><strong>1. 光流估计</strong></p>
<p><strong>基于模型:</strong></p>
<p>[2]：通过将平面与x-y-t空间中的事件拟合，可以估计出正常的流。</p>
<p>[1]：流量估计可以写成一个联合求解图像强度和流量的凸优化问题。</p>
<p><strong>无模型的，深度网络：</strong></p>
<p>[8]：建立了一个可以从亮度稳定性和平滑度中学习光流的网络。</p>
<p>[12]：扩展[8]的工作，通过双向的普查损失提高估计流的质量。</p>
<p><strong>2. 相机姿态(运动)和深度估计：</strong></p>
<p>[9]：在SFM和VIO中，[9]证明卡尔曼滤波器可以重建相机位姿和局部地图。</p>
<p>[23]：网络使用摄像机重投影和颜色一致性损失来学习相机的自运动和深度。</p>
<p>[22][18]：增加了立体约束，使网络能够学习绝对尺度。</p>
<p>[19]：将上述概念与递归神经网络一起应用。</p>
<p><strong>3. 视觉里程计：</strong></p>
<p>[25]：使用基于EM（期望最大化）的特征跟踪方法进行视惯性测程。</p>
<p>[16]：使用运动补偿对事件图像进行去模糊，并运行标准的基于图像的特征跟踪来进行视惯性测程。</p>
<p><strong>4. 最近的工作:</strong></p>
<p>[4、5、13、25 、?]：可以从事件的时空量，通过沿着光流的方向传播，试图将事件图像的运动模糊最小化，以此来估计光流和其他类型的运动信息。(运动模糊作为一种损失的概念可以被看作是一种类似于帧的光度误差，应用于事件。)</p>
<h2 id="四网络构架">四、网络构架</h2>
<p>网络构架的Overview如下： <img src="https://img-blog.csdnimg.cn/20200530083302800.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<p><strong>1、Input：离散事件量</strong></p>
<p>（以往的表示：通过对每个像素的事件数求和来生成事件图像；总结每个像素上的事件数量，以及每个像素上的最后时间戳和平均时间戳。这两种表示输入到网络中，都能准确预测光流。存在的问题：虽然这保留了一些时间信息，但是通过总和事件中的高分辨率时间信息仍然丢失了很多信息。）</p>
<p><font color="red">本文设计的输入表示的目的：</font>为了提高沿时域的分辨率超过bin的数量，使用类似于双线性插值的线性加权累加将事件插入到该量中。</p>
<p><font color="red">好处：</font>在没有事件重叠的情况下，能准确得出事件集；有重叠时，求和确实会损失一些信息，但保留了窗口跨时空维度的事件分布。</p>
<p>对于N个输入事件，以及一个集合B 组距来离散时间维度，将时间戳扩展到[0,B−1]范围内。表示的公式如下： <img src="https://img-blog.csdnimg.cn/20200530083435487.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> kb(a)是等价于[7]中定义的双线性采样核。(相机不失真或者校正的时候，必须在x,y维上插值，会导致非整数像素) 将时域作为传统2D图像中的通道，并在x,y空间维度上执行二维卷积。</p>
<p><strong>2、运动补偿监督</strong></p>
<p>解决的问题：当事件相机记录对数强度的变化时，颜色一致性的标准模型并不直接适用于事件。</p>
<p>运动补偿概念：[16]</p>
<p>运动补偿目的：利用每个事件的运动模型对事件图像进行去模糊处理，如图： <img src="https://img-blog.csdnimg.cn/20200530083521550.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图 2 网络从运动模糊中通过从一组输入、模糊、事件(2)，预测光流、自运动或者深度(1)来学习预测运动，减少预测运动的去模糊后的运动模糊来产生去模糊图像(3)。流的颜色标识方向，在颜色图(4)中。</p>
<p>对于最一般的逐像素光流量u(x,y)，v(x,y)，可以传播事件<span class="math inline">\({(x_i,y_i,t_i,p_i)}_{i=1,2,...,N}\)</span>到单个时间<span class="math inline">\(t&#39;\)</span>: <img src="https://img-blog.csdnimg.cn/20200530083604496.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<p>如果输入的光流是正确的，将反转事件中的运动，并去除运动模糊；如果不正确，则可能导致进一步的运动模糊。</p>
<p><font color="red">评估去模糊效果作为监督信号：</font>[4]里用传播事件生成的图像上的图像方差，缺点就是通过预测将图像每个区域内的所有事件推到一条直线上的流值，网络很容易过拟合这种损失；文章采用[13]中的损失，就是最小化每个像素的平均时间戳的平方和，但这种损失不可微（时间戳是四舍五入来产生2D图像的），本文用双线性插值代替四舍五入。具体做法：</p>
<ol type="1">
<li>首先通过将事件按极性分开，并生成每个像素的平均时间戳<span class="math inline">\((T+，T-)\)</span>的图像来应用损失:</li>
</ol>
<p><img src="https://img-blog.csdnimg.cn/20200530083934259.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 上式中<span class="math inline">\(t&#39;\)</span>改变，式中使用的时间戳不会变。损失就是两幅图像的和的平方 <img src="https://img-blog.csdnimg.cn/20200530084005374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<p>然而，使用单个<span class="math inline">\(t&#39;\)</span>来表示这种损失会造成规模问题。在公式(4)里面，输出的流u、v用<span class="math inline">\((t&#39;-t_i)\)</span>缩放，在反向传播的时候，这将权衡时间戳距t'更高的事件的梯度，而时间戳非常接近t'的事件将被忽略。为减轻这个缩放，文章计算了前向和向后的损失，<span class="math inline">\(t’=t_1,t’=t_N\)</span>: <img src="https://img-blog.csdnimg.cn/20200530084143569.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<p><strong>3、光流预测网络</strong></p>
<p>使用类似[24]中的编码-解码器。网络以像素/bin为单位输出流值，应用到(4)并最终计算(9)。预测光流的网络使用了(7)里面的时间损失，结合局部光滑正则化： <img src="https://img-blog.csdnimg.cn/20200530084212560.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中<span class="math inline">\(\rho(x)=\sqrt{x^2+\epsilon^2}\)</span>是[3]里面的Charbonnier损失，N(x,y)是(x,y)附近的4连通邻域。总的光流预测的损失为： <img src="https://img-blog.csdnimg.cn/20200530084520841.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<p><strong>4、自运动和深度估计</strong></p>
<p>类似[22,18]中的预测相机自运动和场景结构：</p>
<p>给定一对时间同步的离散事件量，分别将每个量传递到网络中，但是在训练时使用这两个量来应用立体视差损失，使网络能够学习度量尺度。还用了运动补偿中的时间戳损失以及一个鲁棒的去模糊事件图像的人口普查变换间的相似性损失[21,17]。</p>
<p>网络预测欧拉角（ψ，β，φ），平移T和每个像素的视差di。视差是使用与流网络中相同的编码器-解码器体系结构生成的，不同之处在于最终激活函数是S型，并通过图像宽度进行缩放。姿态与视差共享编码器网络，并且由跨步卷积生成，这些卷积通过6个通道将空间尺寸从16×16减小到1×1。</p>
<ul>
<li><strong>时间投影损失</strong></li>
</ul>
<p>给定网络输出，相机的内部构造, K，以及两个相机之间的基线，b，每个事件在像素位置(xi,yi)的光流(ui,vi)为： <img src="https://img-blog.csdnimg.cn/20200530084612420.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中，f是相机的焦距，R是对应于（ψ，β，φ）的旋转矩阵，并且π是投影函数<span class="math inline">\(π((X Y Z)^T)=(\frac{X}{Z} \frac{Y}{Z})^T\)</span>。请注意，由于网络仅在输入处为离散量，因此它不知道时间窗口的大小。最后我们计算的光流以像素/bin为单位，其中B是用于生成输入量的像素数量，然后将光流代入（4）中以计算损失。</p>
<ul>
<li><strong>立体视差损失</strong></li>
</ul>
<p>1、从光流中，可以使用（4）对左右摄像机的事件进行模糊处理，并生成一对事件图像，该图像对应于去模糊后每个像素处的事件数。给定正确的流，这些图像代表相应灰度图像的边缘图，在该图像上可以应用光度损失。</p>
<p>2、但是，两个摄像机之间的事件数量也可能有所不同，因此对图像的普查变换[21]应用了相似性损失。</p>
<p>3、另外，如[6]所定义的，在两个预测的视差之间应用了左右一致性损失。</p>
<p>4、最后，如（8）所示，我们对视差应用了局部平滑度正则化器。</p>
<p>SFM模型总损失：</p>
<figure>
<img src="https://img-blog.csdnimg.cn/20200530085045874.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<h2 id="五实验">五、实验</h2>
<p><strong>1、Train</strong></p>
<p>MVSEC[26]的整个户外day2序列中训练：由11分钟的立体视觉事件数据在公共道路上行驶组成。</p>
<p>（在训练中，每个输入包含N = 30000个事件，这些事件将转换为具有256x256分辨率（集中裁剪）和B = 9 bins的离散事件量。每个损失的权重为：<span class="math inline">\((λ1，λ2，λ3，λ4) =( 1.0,1.0,0.1,0.2)\)</span>。）</p>
<p><strong>2、Test</strong></p>
<p>光流：在MVSEC的室内流动和室外白天序列上测试了光流网络，ground truth由[24]提供。</p>
<p>自运动：MVSEC评估我们的户外day1序列的自我运动估计网络。</p>
<p><strong>3、 Evaluate metric</strong></p>
<p><font color="red">光流：</font>平均端点误差（AEE）、AEE大于3个像素点的百分比、超过具有有效ground truth且至少有一个事件的像素点的百分比。</p>
<p><font color="red">自运动：</font>相对位姿误差(RPE)和相对旋转误差(RRE) <img src="https://img-blog.csdnimg.cn/2020053008523364.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中，Rpred是与输出的欧拉角对应的旋转矩阵，logm是矩阵对数。</p>
<p><font color="red">深度：</font>度不变深度度量。</p>
<h2 id="六结果">六、结果</h2>
<p><strong>1、光流估计</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200530085345635.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 表1光流估计结果</p>
<p>对于每个序列，平均端点误差(AEE)以像素计算，%离群值以AEE &gt; 3 pix的点的百分比计算。dt=1是在两个连续的灰度帧之间的一个时间窗口计算得到的，dt=4是在四个灰度帧之间。 <img src="https://img-blog.csdnimg.cn/20200530085407198.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> <img src="https://img-blog.csdnimg.cn/20200530085421678.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 从左到右：灰度图像，事件图像，带有航向的深度预测，带有航向的ground truth。 前两个是流量结果，后一个是深度结果。 对于深度，越近越亮。航向绘制为圆形。 在户外夜晚的结果中，由于闪光灯发出的事件导致航向偏向。</p>
<p><strong>2、自运动估计结果</strong></p>
<p><img src="https://img-blog.csdnimg.cn/20200530085536727.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 本文的相对姿态和旋转误差明显优于SFM-Learner，但比ECN差。但是，ECN只能预测5dof姿态，但要达到比例因子，而我们的网络必须学习具有比例的完整6dof姿态。</p>
<p><strong>3、深度估计</strong> <img src="https://img-blog.csdnimg.cn/20200530085554800.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 序列中，都比Monodepth表现得更好，这可能是因为事件没有强度信息，因此网络被迫学习对象的几何属性。 <img src="https://img-blog.csdnimg.cn/20200530085610171.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 根据ECN[20]对深度网络的标准深度指标进行定量评估从左到右，度量标准是:绝对相对距离、RMSE对数、比例不变对数，以及预测深度大于或小于地面真实值1.25、1.252和1.253倍的点的百分比</p>
<h2 id="七优点及问题">七、优点及问题</h2>
<p>1、室外夜晚的自运动估计结果不好，因为夜晚有日光灯照射，会产生虚假事件——&gt;有必要进行今后的工作以滤除此类异常(例如，如果先验闪光速率已知，则可以通过检测以所需频率产生的事件来简单地过滤光)。</p>
<p>2、但是，深度估计中的δ百分比低于预期，或许将来用在其他数据集上的效果会好些。</p>

            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/categories/work/">work</a>
                      &nbsp;
                    
                      <a class="hover-with-bg" href="/categories/work/event-based-camera/">event-based camera</a>
                      &nbsp;
                    
                      <a class="hover-with-bg" href="/categories/work/event-based-camera/paper-reading/">paper reading</a>
                      &nbsp;
                    
                      <a class="hover-with-bg" href="/categories/work/event-based-camera/paper-reading/%E4%BB%BB%E5%8A%A1%E9%9B%86%E6%88%90/">任务集成</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/tags/event-camera/">event-camera</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/05/07/Computer-Vision-for-Autonomous-Vehicles-Chapter-6-Object-Tracking/">
                        <span class="hidden-mobile">Computer Vision for Autonomous Vehicles | Chapter 6 Object Tracking</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="fa fa-chevron-right"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

              
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-md">
    <div class="container custom post-content mx-auto">
                   我在你身边，等着你回答
    </div>
  </div>


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var toc = $('#toc');
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;
      var tocLimMax = 2 * boardTop + boardCtn.height();

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = boardCtn.css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>





  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>








<!-- Plugins -->



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "paper reading 1| Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion&nbsp;",
      ],
      cursorChar: "|",
      typeSpeed: 80,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });
      MathJax.Hub.Register.StartupHook("End Jax",function () {
        var BROWSER = MathJax.Hub.Browser;
        var jax = "HTML-CSS";
        if (BROWSER.isMSIE && BROWSER.hasMathPlayer) jax = "NativeMML";
        return MathJax.Hub.setRenderer(jax);
      });
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  










</body>
</html>
