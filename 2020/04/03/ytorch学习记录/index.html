<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/avatar.jpg">
  <link rel="icon" type="image/png" href="/img/avatar.jpg">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="记录生活">
  <meta name="author" content="kiki">
  <meta name="keywords" content="CV">
  <title>pytorch学习记录 - KK&#39;s Blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/5.12.1/css/all.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/mdbootstrap/4.13.0/css/mdb.min.css" />
<link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/3.0.1/github-markdown.min.css" />

<link rel="stylesheet" href="//at.alicdn.com/t/font_1067060_qzomjdt8bmp.css">



  <link  rel="stylesheet" href="/lib/prettify/tomorrow-night-eighties.min.css" />

<link  rel="stylesheet" href="/css/main.css" />


  <link defer rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />


<!-- 自定义样式保持在最底部 -->


<meta name="generator" content="Hexo 4.2.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>Welcome to here.</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/">首页</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/archives/">归档</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/">分类</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/tags/">标签</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/work/">工作</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/categories/life/">生活</a>
          </li>
        
          
          
          
          
          <li class="nav-item">
            <a class="nav-link" href="/about/">关于</a>
          </li>
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('/img/default.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask rgba-black-light flex-center">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
                <p class="mt-3 post-meta">
                  <i class="fas fa-calendar-alt" aria-hidden="true"></i>
                  星期五, 四月 3日 2020, 10:50 晚上
                </p>
              

              <p class="mt-1">
                
                  
                  <span class="post-meta">
                    <i class="far fa-chart-bar"></i>
                    4.6k 字
                  </span>
                

                
                  
                  <span class="post-meta">
                      <i class="far fa-clock"></i>
                      19 分钟
                  </span>
                

                
              </p>
            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5 z-depth-3" id="board">
          <div class="post-content mx-auto" id="post">
            
            <div class="markdown-body">
              <h2 id="课程名pytorch-动态神经网络">课程名：《Pytorch 动态神经网络》</h2>
<p>课程来源：<a href="https://www.bilibili.com/video/av15997678?p=35" target="_blank" rel="noopener">here</a><br>
作者：莫烦</p>
<p>@<a href="目录">TOC</a></p>
<h2 id="day01-安装pytorch">day01 安装Pytorch</h2>
<p>前提：安装Anaconda参考别人的安装教程<a href="https://www.jianshu.com/p/742dc4d8f4c5" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/742dc4d8f4c5</a> - 在开始菜单找到Anaconda的命令提示行(Anaconda Prompt)，并输入conda create -n pytorch python=3.7(我自己的是3.7版本)，建立一个Pytorch的环境： <img src="https://img-blog.csdnimg.cn/20200318180532330.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 然后，出现以下情况，问是否安装等等工具包，选择[y]开始安装： <img src="https://img-blog.csdnimg.cn/20200318180734340.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 安装成功以后，会出现如下，就是激活环境的语句： <img src="https://img-blog.csdnimg.cn/20200318180919400.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入conda activate pytorch进入pytorch环境： <img src="https://img-blog.csdnimg.cn/20200318181124374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入pip list可以查看这个环境下的工具包，可以看到没有需要的pytorch，所以需要安装： <img src="https://img-blog.csdnimg.cn/2020031818131941.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_12,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 去官网查看自己适合哪个版本，比如我的是CPU，习惯用pip，如下图：<img src="https://img-blog.csdnimg.cn/20200322091319185.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="在这里插入图片描述"></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">pip install torch<span class="op">==</span><span class="dv">1</span>.<span class="fl">4.0</span><span class="op">+</span>cpu torchvision<span class="op">==</span><span class="dv">0</span>.<span class="fl">5.0</span><span class="op">+</span>cpu <span class="op">-</span>f https:<span class="op">//</span>download.pytorch.org<span class="op">/</span>whl<span class="op">/</span>torch_stable.html</a></code></pre></div>
<ul>
<li>完了以后，在pytorch环境中进入&gt;&gt;python，测试一下是否安装成功，输入import torch即可。</li>
</ul>
<h2 id="day02">day02</h2>
<h3 id="一神经网络简介">一、神经网络简介</h3>
<p>1、 机器学习—梯度下降机制(optimization) <br> 2、神经网络黑盒：输入端-黑盒-输出端； <br> 　　　　　　黑盒：特征代表输入数据。</p>
<h3 id="二why-pytorch">二、why Pytorch？</h3>
<p>1、与tensorflow的区别</p>
<ul>
<li><p>tensorflow是静态的框架，构建好tensorflow的计算图之后，这个计算图是不能改变的，计算流程是固定的，类似C++，写代码时要用他自己的一些API。缺点之一例如训练的时候loss一直将不下来，模型很难得到优化，debug就很困难。</p></li>
<li><p>pytorch是动态的框架，和python一样，直接计算，不用开启会话。 <br></p></li>
</ul>
<h3 id="三variable变量">三、Variable变量</h3>
<ul>
<li>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置，里面的值会不停的变化，就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那里面的鸡蛋就是 Torch 的 Tensor 。</li>
<li>PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</li>
<li><p>from torch.autograd import Variable</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">ten<span class="op">=</span>torch.FloatTensor([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>]]) <span class="co"># tensor的类型</span></a>
<a class="sourceLine" id="cb2-2" data-line-number="2">variable<span class="op">=</span>Variable(tensor,requires_grad<span class="op">=</span><span class="va">True</span>) </a>
<a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co"># 将tensor传给variable，需要Variable来建立一个计算图纸，把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度，如果要就会计算Variable节点的梯度</span></a>
<a class="sourceLine" id="cb2-4" data-line-number="4">t_out <span class="op">=</span> torch.mean(ten<span class="op">*</span>ten) <span class="co"># 计算x^2</span></a>
<a class="sourceLine" id="cb2-5" data-line-number="5">v_out <span class="op">=</span> torch.mean(variable<span class="op">*</span>variable)</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">v_out.backward() <span class="co"># v_outbackward时，variable也会变化，因为是一体的</span></a>
<a class="sourceLine" id="cb2-8" data-line-number="8"><span class="bu">print</span>(variable)</a>
<a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">#直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图),所以我们要转换一下, 将它变成 tensor 形式</span></a>
<a class="sourceLine" id="cb2-10" data-line-number="10"><span class="bu">print</span>(variable.data)</a>
<a class="sourceLine" id="cb2-11" data-line-number="11"><span class="bu">print</span>(variable.data.numpy())<span class="co"># variable.data为tensor的形式，tensor才能转换为numpy形式</span></a></code></pre></div></li>
<li>autograd根据用户对Variable的操作构建其计算图，这个图将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候， 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力。</li>
<li>variable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li>
<li>多次反向传播时，<font color="red">梯度是累加的</font>。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li>
<li><p>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</p></li>
</ul>
<h2 id="day-03">day 03</h2>
<h3 id="一激励函数activation">一、激励函数（Activation）</h3>
<ul>
<li><p>什么是Activation 非线性的函数激活网络的输出：ReLU、Sigmoid、Tanh、Softplus</p></li>
<li><p>Torch中的激励函数</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"></a>
<a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a>
<a class="sourceLine" id="cb3-3" data-line-number="3"><span class="im">from</span> torch.autograd <span class="im">import</span> Variable</a>
<a class="sourceLine" id="cb3-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a>
<a class="sourceLine" id="cb3-5" data-line-number="5"></a>
<a class="sourceLine" id="cb3-6" data-line-number="6">x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># x data (tensor), shape=(100, 1)</span></a>
<a class="sourceLine" id="cb3-7" data-line-number="7">x <span class="op">=</span> Variable(x)</a>
<a class="sourceLine" id="cb3-8" data-line-number="8">x_np <span class="op">=</span> x.data.numpy()   <span class="co"># numpy 数据才能用来画图</span></a>
<a class="sourceLine" id="cb3-9" data-line-number="9"></a>
<a class="sourceLine" id="cb3-10" data-line-number="10">y_relu <span class="op">=</span> torch.relu(x).data.numpy()</a>
<a class="sourceLine" id="cb3-11" data-line-number="11">y_sigmoid <span class="op">=</span> torch.sigmoid(x).data.numpy()</a>
<a class="sourceLine" id="cb3-12" data-line-number="12">y_tanh <span class="op">=</span> torch.tanh(x).data.numpy() <span class="co"># 计算出非线性函数输出后也要转化为numpy数据</span></a>
<a class="sourceLine" id="cb3-13" data-line-number="13"><span class="co"># y_softplus = F.softplus(x).data.numpy()   </span></a>
<a class="sourceLine" id="cb3-14" data-line-number="14"></a>
<a class="sourceLine" id="cb3-15" data-line-number="15"><span class="co">#画图</span></a>
<a class="sourceLine" id="cb3-16" data-line-number="16">plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</a>
<a class="sourceLine" id="cb3-17" data-line-number="17">plt.subplot(<span class="dv">221</span>)</a>
<a class="sourceLine" id="cb3-18" data-line-number="18">plt.plot(x_np, y_relu, c<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</a>
<a class="sourceLine" id="cb3-19" data-line-number="19">plt.ylim((<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>))</a>
<a class="sourceLine" id="cb3-20" data-line-number="20">plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>)</a></code></pre></div></li>
<li><p>结果 <img src="https://img-blog.csdnimg.cn/2020032211361126.png?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li>
</ul>
<h3 id="二regression回归">二、Regression回归</h3>
<p>直接放莫老师教的代码过来： * Layer图搭建以及计算流程</p>
<pre><code>  ```python
  class Net(torch.nn.Module):   # torch.nn.Module是Net的主模块
      def __init__(self, n_feature, n_hidden, n_output): # 搭建层所需要的信息
          super(Net, self).__init__()  # 继承Net的模块功能
          self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer
          self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer

      def forward(self, x): # 前向传递的过程，搭流程图
             x = F.relu(self.hidden(x))      # activation function for hidden layer
             x = self.predict(x)             # linear output
       return x
      ```</code></pre>
<ul>
<li><p>定义Net</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">net <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">1</span>)     <span class="co"># define</span></a></code></pre></div></li>
<li><p>优化神经网络(torch.optim.),以及loss function定义</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">optimizer <span class="op">=</span> torch.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)</a>
<a class="sourceLine" id="cb6-2" data-line-number="2">loss_func <span class="op">=</span> torch.nn.MSELoss() <span class="co"># 均方差作为loss</span></a></code></pre></div></li>
<li><p>开始训练</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</a>
<a class="sourceLine" id="cb7-2" data-line-number="2">    prediction <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a>
<a class="sourceLine" id="cb7-3" data-line-number="3"></a>
<a class="sourceLine" id="cb7-4" data-line-number="4">    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></a>
<a class="sourceLine" id="cb7-5" data-line-number="5"></a>
<a class="sourceLine" id="cb7-6" data-line-number="6">    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></a>
<a class="sourceLine" id="cb7-7" data-line-number="7">    loss.backward()         <span class="co"># backpropagation, compute gradients</span></a>
<a class="sourceLine" id="cb7-8" data-line-number="8">    optimizer.step()        <span class="co"># apply gradients</span></a>
<a class="sourceLine" id="cb7-9" data-line-number="9">    <span class="co"># 以上三步为优化步骤</span></a></code></pre></div></li>
</ul>
<h3 id="三-classification-分类">三、 Classification 分类</h3>
<p>　与上面不同的是：</p>
<ul>
<li>构造的伪数据不一样，是包含有对应标签的数据；(数据不能是一维)</li>
<li>网络输入输出不同，有两个输入两个输出；</li>
<li>loss用到的是交叉熵cross entropy loss，out与标签y</li>
</ul>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">loss_func <span class="op">=</span> torch.nn.CrossEntropyLoss() </a>
<a class="sourceLine" id="cb8-2" data-line-number="2">loss <span class="op">=</span> loss_func(out, y)</a></code></pre></div>
<ul>
<li><p>output是取值，转换成概率值需要加softmax(out)</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">out <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a>
<a class="sourceLine" id="cb9-2" data-line-number="2">prediction <span class="op">=</span> F.softmax(out)  <span class="co">#将输出对应值转化成概率</span></a></code></pre></div></li>
</ul>
<h3 id="四快速搭建网络">四、快速搭建网络</h3>
<ul>
<li><p>method1—搭建网络、流程图，定义网络</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">class</span> Net(torch.nn.Module):</a>
<a class="sourceLine" id="cb10-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</a>
<a class="sourceLine" id="cb10-3" data-line-number="3">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a>
<a class="sourceLine" id="cb10-4" data-line-number="4">        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></a>
<a class="sourceLine" id="cb10-5" data-line-number="5">        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></a>
<a class="sourceLine" id="cb10-6" data-line-number="6"></a>
<a class="sourceLine" id="cb10-7" data-line-number="7">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a>
<a class="sourceLine" id="cb10-8" data-line-number="8">        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))      <span class="co"># activation function for hidden layer</span></a>
<a class="sourceLine" id="cb10-9" data-line-number="9">        x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></a>
<a class="sourceLine" id="cb10-10" data-line-number="10">        <span class="cf">return</span> x</a>
<a class="sourceLine" id="cb10-11" data-line-number="11"></a>
<a class="sourceLine" id="cb10-12" data-line-number="12">net1 <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">2</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">2</span>)  </a></code></pre></div></li>
<li><p>method2—利用torch.nn.Sequential直接定义网络 <code>python  net2=torch.nn.Sequential(      torch.nn.Linear(2,10),      torch.nn.ReLU(),      torch.nn.Linear(10,2)  )</code></p></li>
</ul>
<h3 id="五网络的保存和提取">五、网络的保存和提取</h3>
<ul>
<li><p>方法1：保存—提取</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1">torch.save(net1, <span class="st">&#39;net.pkl&#39;</span>) <span class="co"># 保存整个网络，以pkl形式保存</span></a></code></pre></div>
<div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">net2 <span class="op">=</span> torch.load(<span class="st">&#39;net.pkl&#39;</span>)</a>
<a class="sourceLine" id="cb12-2" data-line-number="2">prediction <span class="op">=</span> net2(x)</a></code></pre></div></li>
<li><p>方法2：保存—提取</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1">torch.save(net1.state_dict(), <span class="st">&#39;net_params.pkl&#39;</span>) <span class="co"># 只保存网络中节点的参数 (速度快, 占内存少)</span></a></code></pre></div>
<div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">net3 <span class="op">=</span> torch.nn.Sequential(</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">    torch.nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb14-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb14-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">net3.load_state_dict(torch.load(<span class="st">&#39;net_params.pkl&#39;</span>))</a>
<a class="sourceLine" id="cb14-7" data-line-number="7">prediction <span class="op">=</span> net3(x)</a></code></pre></div></li>
</ul>
<h3 id="六批数据训练mini_batch-training">六、批数据训练(mini_batch training)</h3>
<ul>
<li><p>将数据分批训练，一个epoch训练所有批次的数据：</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">BATCH_SIZE <span class="op">=</span> <span class="dv">5</span> <span class="co"># 抽取训练的数据</span></a>
<a class="sourceLine" id="cb15-3" data-line-number="3"><span class="co"># BATCH_SIZE = 8</span></a>
<a class="sourceLine" id="cb15-4" data-line-number="4"></a>
<a class="sourceLine" id="cb15-5" data-line-number="5">x <span class="op">=</span> torch.linspace(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">10</span>)       <span class="co"># this is x data (torch tensor)</span></a>
<a class="sourceLine" id="cb15-6" data-line-number="6">y <span class="op">=</span> torch.linspace(<span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">10</span>)       <span class="co"># this is y data (torch tensor)</span></a>
<a class="sourceLine" id="cb15-7" data-line-number="7"></a>
<a class="sourceLine" id="cb15-8" data-line-number="8">torch_dataset <span class="op">=</span> Data.TensorDataset(data_tensor <span class="op">=</span> x, target_tensor <span class="op">=</span> y)</a>
<a class="sourceLine" id="cb15-9" data-line-number="9">loader <span class="op">=</span> Data.DataLoader(</a>
<a class="sourceLine" id="cb15-10" data-line-number="10">    dataset<span class="op">=</span>torch_dataset,      <span class="co"># torch TensorDataset format</span></a>
<a class="sourceLine" id="cb15-11" data-line-number="11">    batch_size<span class="op">=</span>BATCH_SIZE,      <span class="co"># mini batch size</span></a>
<a class="sourceLine" id="cb15-12" data-line-number="12">    shuffle<span class="op">=</span><span class="va">True</span>,               <span class="co"># random shuffle for training</span></a>
<a class="sourceLine" id="cb15-13" data-line-number="13">    num_workers<span class="op">=</span><span class="dv">2</span>,              <span class="co"># 多线程来读数据</span></a>
<a class="sourceLine" id="cb15-14" data-line-number="14">)</a>
<a class="sourceLine" id="cb15-15" data-line-number="15"></a>
<a class="sourceLine" id="cb15-16" data-line-number="16"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):   <span class="co"># 训练所有!整套!数据 3 次</span></a>
<a class="sourceLine" id="cb15-17" data-line-number="17">    <span class="cf">for</span> step, (batch_x, batch_y) <span class="kw">in</span> <span class="bu">enumerate</span>(loader):  <span class="co"># 每一步 loader 释放一小批数据用来学习</span></a>
<a class="sourceLine" id="cb15-18" data-line-number="18">        <span class="co"># 假设这里就是你训练的地方...</span></a>
<a class="sourceLine" id="cb15-19" data-line-number="19">        <span class="co"># 打出来一些数据</span></a>
<a class="sourceLine" id="cb15-20" data-line-number="20">        <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| Step: &#39;</span>, step, <span class="st">&#39;| batch x: &#39;</span>,</a>
<a class="sourceLine" id="cb15-21" data-line-number="21">              batch_x.numpy(), <span class="st">&#39;| batch y: &#39;</span>, batch_y.numpy())</a></code></pre></div></li>
<li><p>DataLoader 是PyTorch中数据读取的接口，PyTorch训练模型基本都会用到该接口，其目的：将dataset根据batch_size大小、shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。</p></li>
<li><p>enumerate()函数 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。在这里就是把是个数据分成size为5的两份数据后，将每一份数据对应的下标给step，数据给(batch_x, batch_y)。</p></li>
</ul>
<h2 id="day-04">day 04</h2>
<h3 id="一优化器optimizer加速神经网络训练深度学习">一、优化器Optimizer加速神经网络训练（深度学习）</h3>
<ul>
<li>数据分批送入网络，进行SGD优化；</li>
<li>Momentum更新参数方法：<span class="math inline">\(m=b1*m-Learningrate*dx,W+=m\)</span></li>
<li>AdaGrad：<span class="math inline">\(v+=dx^2，W+=-Learning rate*dx/\sqrt v\)</span></li>
<li>RMSProp方法(上述两种的合并)：<span class="math inline">\(v=b1*v+(1-b1)*dx^2,W+=-Learning_rate*dx/\sqrt v\)</span></li>
<li>Adam:<span class="math inline">\(m = b1*m+(1-b1)*dx\)</span>——&gt;Momentum 　　　 <span class="math inline">\(v = b2*v+(1-b2)*dx^2\)</span>——&gt;AdaGrad 　　　 <span class="math inline">\(W+=-Learning_rate*m/\sqrt v\)</span></li>
</ul>
<h3 id="二opttimizer优化器">二、Opttimizer优化器 　</h3>
<ul>
<li>几种常见优化器： 　 <code>python  # different optimizers  opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR)  opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)  opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)  opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))  optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</code></li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20200323135943525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 　　从图中可以看出，目前性能最优的应该是Adam。</p>
<h3 id="三-卷积神经网络cnn">三、 卷积神经网络(CNN)</h3>
<p>　　图像处理中，不是对每个像素点卷积处理，而是对一小块区域进行计算，这样加强了图像信息的连续性，使得神经网络能看到图片信息而非一个点，同时加深了神经网络对图片的理解。批量过滤器每次对图像收集一小块信息，最后将这些整理出来得到边缘信息，再对这些信息进行类似的处理，得到更高层的信息结构(例如眼睛、鼻子等)，最后把总结出来的信息套入几层full connection进行分类等操作。卷积操作时，神经层会丢失一些信息，池化层可以将Layer中有用的信息筛选出来给下一层，因此图片的长宽不断压缩，压缩的工作是池化层进行的。</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1">1、import需要的工具包和库：torch、torchvision、torch.nn、torch.utils.data</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">2、超参数：EPOCH、BATCH_SIZE、LR</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">3、下载mnist数据集：torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;./mnist/&#39;</span>,train<span class="op">=</span><span class="va">True</span>,transform<span class="op">=</span>torchvision.transform.ToTensor(),download<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb16-4" data-line-number="4"> <span class="co">#root是保存或提取的位置，transform是将数据集PIL.Image or numpy.ndarray转换成torch.FloatTensor(C×H×W)，训练的时候normalize成[0,1]间的值</span></a>
<a class="sourceLine" id="cb16-5" data-line-number="5"> test数据集处理：test—_x,test_y</a>
<a class="sourceLine" id="cb16-6" data-line-number="6"> 4、批训练train_loader定义：Data.DataLoader(dataset<span class="op">=</span>train_data,batch_size<span class="op">=</span>BATCH_SIZE,shuffle<span class="op">=</span><span class="va">True</span>)</a>
<a class="sourceLine" id="cb16-7" data-line-number="7"> 5、定义网络构架CNN(nn.Module):conv1—conv2—RELU—pooling—conv2—ReLU—pooling—output</a>
<a class="sourceLine" id="cb16-8" data-line-number="8"> 网络计算流程：conv1(x)——conv2(x)——展平多维卷积图——计算输出</a>
<a class="sourceLine" id="cb16-9" data-line-number="9"> 6、定义optimizer和loss function</a>
<a class="sourceLine" id="cb16-10" data-line-number="10"> 7、训练和测试</a></code></pre></div>
<h3 id="四什么是lstm循环卷积网络rnn">四、什么是LSTM循环卷积网络(RNN)</h3>
<ul>
<li>LSTM(Long Short-Term Memory)——长短期记忆</li>
<li>RNN是在有序的数据上进行学习 <img src="https://img-blog.csdnimg.cn/20200323202037102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li>
<li>分类问题(mnist数据集) 　我们将图片数据看成一个时间上的连续数据, 每一行的像素点都是这个时刻的输入, 读完整张图片就是从上而下的读完了每行的像素点。然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了。</li>
<li>回归问题 这部分内容参考<a href="https://blog.csdn.net/qq_41639077/article/details/105218095" target="_blank" rel="noopener">KiKi的另一篇blog</a>，包括分类问题和回归问题的pytorch实现。</li>
</ul>
<h3 id="五自编码非监督学习autoencoder">五、自编码/非监督学习（Autoencoder）　</h3>
<p>　　原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作。 所以, 何不<strong>压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了。</strong> 训练好的自编码中间这一部分就是能总结原数据的精髓，我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。(摘自莫烦python) * 代码</p>
<pre><code>```python
self.encoder = nn.Sequential(
    nn.Linear(28*28, 128),
    nn.Tanh(),
    nn.Linear(128, 64),
    nn.Tanh(),
    nn.Linear(64, 12),
    nn.Tanh(),
    nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt
)

self.decoder = nn.Sequential(
    nn.Linear(3, 12),
    nn.Tanh(),
    nn.Linear(12, 64),
    nn.Tanh(),
    nn.Linear(64, 128),
    nn.Tanh(),
    nn.Linear(128, 28*28),
    nn.Sigmoid(),       # compress to a range (0, 1)
)

def forward(self, x):
    encoded = self.encoder(x)
    decoded = self.decoder(encoded)
    return encoded, decoded

autoencoder = AutoEncoder()

```</code></pre>
<h3 id="六gan生成对抗网络">六、GAN—生成对抗网络</h3>
<p>　（原理已经学习过了，直接上代码）</p>
<ul>
<li><p>pytorch中实现：(代码中的对象不是图像，用到的是二次曲线) 　<br></p></li>
<li><p>超参数</p>
<pre><code>  ```python
  BATCH_SIZE = 64
  LR_G = 0.0001 # 生成器的学习率
  LR_D = 0.0001 # 判别器的学习率
  N_IDEAS = 5 # random_noise的个数
  ART_COMPONENTS = 15  # 定义规格，一条曲线上有多少个点
  PAINT_POINTS = np.vstack([np.linspace(-1,1,ART_COMPONENTS)for _ in range(BATCH_SIZE)]) # 规定整批画的点，从-1到1共15个点
  ``` </code></pre></li>
<li><p>没有train data，自己伪造一些real data</p>
<pre><code>  ```python
  def artist_works():     # painting from the famous artist (real target)
      a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis] # 二次曲线的系数
      paintings = a * np.power(PAINT_POINTS, 2) + (a-1)  # 二次曲线的参数，区间表示upper和
      paintings = torch.from_numpy(paintings).float()
      return paintings
  ```     </code></pre></li>
<li><p>定义生成器和判别器</p>
<pre><code>  ```python
  G = nn.Sequential(                      # Generator
      nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)
      nn.ReLU(),
      nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas
  )

  D = nn.Sequential(                      # Discriminator
      nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G
      nn.ReLU(),
      nn.Linear(128, 1),
      nn.Sigmoid(),                       # tell the probability that the art work is made by artist
  )
  ```</code></pre></li>
<li><p>优化器</p>
<pre><code>  ```python
      opt_D = torch.optim.Adam(D.parameters(), lr=LR_D)
  opt_G = torch.optim.Adam(G.parameters(), lr=LR_G)

  ```</code></pre></li>
<li><p>训练啦</p>
<pre><code>  ```python
  for step in range(10000):
      artist_paintings = artist_works()           # real painting from artist
      G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas
      G_paintings = G(G_ideas)                    # fake painting from G (random ideas)

      prob_artist0 = D(artist_paintings)          # D try to increase this prob
      prob_artist1 = D(G_paintings)               # D try to reduce this prob

      D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))
      G_loss = torch.mean(torch.log(1. - prob_artist1))

      opt_D.zero_grad()
      D_loss.backward(retain_graph=True)      # reusing computational graph
      opt_D.step()

      opt_G.zero_grad()
      G_loss.backward()
      opt_G.step()
  ```</code></pre></li>
</ul>
<p><strong>补充：</strong> cGAN与GAN的区别在于多了一个类别标签，这个label会跟随noise一起输入到生成器中，并且也要跟随fake和real一起输入到判别其中，最终计算各自的loss。</p>
<h3 id="七为什么torch是动态的待补充">七、为什么Torch是动态的<font color="red">(待补充)</font></h3>
<p>　例子：RNN网络 　Tensorflow就是预先定义好要做的task的框架、步骤，然后开启会话之后喂数据一步到位的计算出结果，开启会话后便不能修改网络构架了，只能是照着计算流图跟着计算，所以是静态的；Torch也可以先定义好框架然后套进去，但计算的时候无论网络怎么变化每一个叶子节点的梯度都能给出，tensorflow就做不到这一点，并且torch是边给出计算图纸一边进行训练。torch就像是散装的一样，可以一块一块的制作好并进行计算，比较灵活，所以是动态的。</p>
<h3 id="八gpu加速">八、GPU加速</h3>
<p>　以之前CNN为例，对其代码进行修改</p>
<ul>
<li><p>dataset部分</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb23-1" data-line-number="1">test_x <span class="op">=</span> torch.unsqueeze(test_data.test_data, dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(torch.FloatTensor).cuda()<span class="op">/</span><span class="fl">255.</span>   <span class="co"># Tensor on GPU</span></a>
<a class="sourceLine" id="cb23-2" data-line-number="2">test_y <span class="op">=</span> test_data.test_labels.cuda()</a></code></pre></div></li>
<li><p>CNN网络的参数改为GPU兼容形式</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb24-1" data-line-number="1"><span class="kw">class</span> CNN(nn.Module):</a>
<a class="sourceLine" id="cb24-2" data-line-number="2">    ...</a>
<a class="sourceLine" id="cb24-3" data-line-number="3">cnn <span class="op">=</span> CNN()</a>
<a class="sourceLine" id="cb24-4" data-line-number="4"><span class="co">##########转换cnn到CUDA#########</span></a>
<a class="sourceLine" id="cb24-5" data-line-number="5">cnn.cuda()   <span class="co"># Moves all model parameters and buffers to the GPU.</span></a></code></pre></div></li>
<li><p>training data变成GPU形式</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1"><span class="cf">for</span> epoch ..:</a>
<a class="sourceLine" id="cb25-2" data-line-number="2">    <span class="cf">for</span> step, ...:</a>
<a class="sourceLine" id="cb25-3" data-line-number="3">        <span class="co">##########修改1###########</span></a>
<a class="sourceLine" id="cb25-4" data-line-number="4">        b_x <span class="op">=</span> x.cuda()    <span class="co"># Tensor on GPU</span></a>
<a class="sourceLine" id="cb25-5" data-line-number="5">        b_y <span class="op">=</span> y.cuda()    <span class="co"># Tensor on GPU</span></a>
<a class="sourceLine" id="cb25-6" data-line-number="6">        ...</a>
<a class="sourceLine" id="cb25-7" data-line-number="7"></a>
<a class="sourceLine" id="cb25-8" data-line-number="8">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a>
<a class="sourceLine" id="cb25-9" data-line-number="9">            test_output <span class="op">=</span> cnn(test_x)</a>
<a class="sourceLine" id="cb25-10" data-line-number="10"></a>
<a class="sourceLine" id="cb25-11" data-line-number="11">            <span class="co"># !!!!!!!! 修改2  !!!!!!!!! #</span></a>
<a class="sourceLine" id="cb25-12" data-line-number="12">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a>
<a class="sourceLine" id="cb25-13" data-line-number="13"></a>
<a class="sourceLine" id="cb25-14" data-line-number="14">            accuracy <span class="op">=</span> torch.<span class="bu">sum</span>(pred_y <span class="op">==</span> test_y) <span class="op">/</span> test_y.size(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb25-15" data-line-number="15">            ...</a>
<a class="sourceLine" id="cb25-16" data-line-number="16"></a>
<a class="sourceLine" id="cb25-17" data-line-number="17">test_output <span class="op">=</span> cnn(test_x[:<span class="dv">10</span>])</a>
<a class="sourceLine" id="cb25-18" data-line-number="18"></a>
<a class="sourceLine" id="cb25-19" data-line-number="19"><span class="co"># !!!!!!!! 修改3 !!!!!!!!! #</span></a>
<a class="sourceLine" id="cb25-20" data-line-number="20">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a>
<a class="sourceLine" id="cb25-21" data-line-number="21">...</a>
<a class="sourceLine" id="cb25-22" data-line-number="22"><span class="bu">print</span>(test_y[:<span class="dv">10</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div></li>
</ul>
<h2 id="day-05">day 05</h2>
<h3 id="一过拟合overfitting">一、过拟合(Overfitting)</h3>
<ul>
<li><p>过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。模型在训练集上效果好，然而在测试集上效果差，模型泛化能力差。</p></li>
<li><p>原因 <br> 　1）在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候； <br> 　2）权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。 　</p></li>
<li><p>解决方法</p>
<p>方法一： <strong>增加数据量</strong>。 <br> 方法二：<strong>运用正规化</strong>，L1、 L2 regularization等等。（神经网络的正规化方法<strong>dropout</strong>——就是在训练的时候, 随机忽略掉一些神经元和神经联结 , 使这个神经网络变得”不完整”，用这个不完整的神经网络训练一次。第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象每次训练的时候, 让每一次预测结果不会依赖于其中某部分特定的神经元。像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数，Dropout 的做法是从根本上让神经网络没机会过度依赖。）</p></li>
<li><p>Dropout</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="co"># 不加dropout的网络</span></a>
<a class="sourceLine" id="cb26-2" data-line-number="2">net_overfitting <span class="op">=</span> torch.nn.Sequential(</a>
<a class="sourceLine" id="cb26-3" data-line-number="3">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a>
<a class="sourceLine" id="cb26-4" data-line-number="4">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb26-5" data-line-number="5">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a>
<a class="sourceLine" id="cb26-6" data-line-number="6">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb26-7" data-line-number="7">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb26-8" data-line-number="8">    )</a>
<a class="sourceLine" id="cb26-9" data-line-number="9"></a>
<a class="sourceLine" id="cb26-10" data-line-number="10">    <span class="co"># 加上dropout</span></a>
<a class="sourceLine" id="cb26-11" data-line-number="11">net_dropped <span class="op">=</span> torch.nn.Sequential(</a>
<a class="sourceLine" id="cb26-12" data-line-number="12">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a>
<a class="sourceLine" id="cb26-13" data-line-number="13">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a>
<a class="sourceLine" id="cb26-14" data-line-number="14">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb26-15" data-line-number="15">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a>
<a class="sourceLine" id="cb26-16" data-line-number="16">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a>
<a class="sourceLine" id="cb26-17" data-line-number="17">    torch.nn.ReLU(),</a>
<a class="sourceLine" id="cb26-18" data-line-number="18">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a>
<a class="sourceLine" id="cb26-19" data-line-number="19">    )＃除了网络构架不同外，其他大同小异。</a></code></pre></div>
<p>　 　</p></li>
</ul>
<h3 id="二批标准化batch-normalization">二、批标准化(Batch Normalization)</h3>
<ul>
<li><p>什么是批标准化 <br> 　Batch Normalization(BN), 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法。 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律。数据随着神经网络的传递计算，激活函数的存在会造成网络层对数据的不敏感，比如0.1和2经过Tanh函数后，前者仍0.1，而2变成1，那这样再大的数都会变成1，所以神经层对数据失去了感觉，这样的问题同样存在于隐藏层中，所以BN则是用在这些神经层中优化网络的方法。 <br> 　Batch就是数据分批处理，每一批数据前向传递的过程中，每一层都进行BN处理，添加在层和激励函数之间。反BN：<span class="math inline">\(BN_(\gamma,\beta)(x_i)\)</span>是将 normalize 后的数据再扩展和平移，是为了让神经网络自己去学着使用和修改这个扩展参数 <span class="math inline">\(\gamma\)</span>, 和 平移参数<span class="math inline">\(\beta\)</span>, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 <span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>来抵消一些 normalization 的操作。</p></li>
<li><p>代码 　莫烦<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py" target="_blank" rel="noopener">BN_code</a></p></li>
</ul>
<p>　　　　　　<font color="red">部分内容待学习</font></p>

            </div>
            <hr>
            <div>
              <p>
                
                  <span>
                <i class="iconfont icon-inbox"></i>
                    
                      <a class="hover-with-bg" href="/categories/work/">work</a>
                      &nbsp;
                    
                      <a class="hover-with-bg" href="/categories/work/pytorch%E5%AD%A6%E4%B9%A0/">pytorch学习</a>
                      &nbsp;
                    
                  </span>&nbsp;&nbsp;
                
                
                  <span>
                <i class="iconfont icon-tag"></i>
                    
                      <a class="hover-with-bg" href="/tags/pytorch/">pytorch</a>
                    
                  </span>
                
              </p>
              
                <p class="note note-warning">本博客所有文章除特别声明外，均采用 <a href="https://zh.wikipedia.org/wiki/Wikipedia:CC_BY-SA_3.0%E5%8D%8F%E8%AE%AE%E6%96%87%E6%9C%AC" target="_blank" rel="nofollow noopener noopener">CC BY-SA 3.0协议</a> 。转载请注明出处！</p>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/">
                        <span class="hidden-mobile">RNN-循环神经网络入门</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="fa fa-chevron-right"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

              
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc-start"></div>
<div id="toc">
  <p class="h5"><i class="far fa-list-alt"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->

  <div class="col-lg-7 mx-auto nopadding-md">
    <div class="container custom post-content mx-auto">
                   我在你身边，等着你回答
    </div>
  </div>


    
  </main>

  
    <a class="z-depth-1" id="scroll-top-button" href="#" role="button">
      <i class="fa fa-chevron-up scroll-top-arrow" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">
    <div>
      <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><b>Hexo</b></a>
      <i class="iconfont icon-love"></i>
      <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"> <b>Fluid</b></a>
    </div>
    

    

    
  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/popper.js/1.16.1/umd/popper.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="https://cdn.staticfile.org/mdbootstrap/4.13.0/js/mdb.min.js" ></script>
<script  src="/js/main.js" ></script>


  <script  src="/js/lazyload.js" ></script>



  
  <script  src="https://cdn.staticfile.org/tocbot/4.10.0/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var navHeight = $('#navbar').height();
      var toc = $('#toc');
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;
      var tocLimMax = 2 * boardTop + boardCtn.height();

      $(window).scroll(function () {
        var tocLimMin = $('#toc-start').offset().top - navHeight;
        var scroH = document.body.scrollTop + document.documentElement.scrollTop;

        if (tocLimMin <= scroH && scroH <= tocLimMax) {
          toc.css({
            'display': 'block',
            'position': 'fixed',
            'top': navHeight,
          });
        } else if (scroH <= tocLimMin) {
          toc.css({
            'position': '',
            'top': '',
          });
        } else if (scroH > tocLimMax) {
          toc.css('display', 'none');
        }
      });
      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: '.post-content',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc > p').css('visibility', 'visible');
      }
      var offset = boardCtn.css('margin-right')
      $('#toc-ctn').css({
        'right': offset
      })
    });
  </script>





  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>








<!-- Plugins -->



  <script  src="https://cdn.staticfile.org/prettify/188.0.0/prettify.min.js" ></script>
  <script>
    $(document).ready(function () {
      $('pre').addClass('prettyprint  linenums');
      prettyPrint();
    })
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "pytorch学习记录&nbsp;",
      ],
      cursorChar: "|",
      typeSpeed: 80,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      getSearchFile(path);
      this.onclick = null
    }
  </script>



  <script defer src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <script>
    $("#post img:not(.no-zoom img, img[no-zoom])").each(
      function () {
        var element = document.createElement("a");
        $(element).attr("data-fancybox", "images");
        $(element).attr("href", $(this).attr("src"));
        $(this).wrap(element);
      }
    );
  </script>



  

  
    <!-- MathJax -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
              processEscapes: true,
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
          }
      });
      MathJax.Hub.Register.StartupHook("End Jax",function () {
        var BROWSER = MathJax.Hub.Browser;
        var jax = "HTML-CSS";
        if (BROWSER.isMSIE && BROWSER.hasMathPlayer) jax = "NativeMML";
        return MathJax.Hub.setRenderer(jax);
      });
      MathJax.Hub.Queue(function() {
          var all = MathJax.Hub.getAllJax(), i;
          for(i=0; i < all.length; i += 1) {
              all[i].SourceElement().parentNode.className += ' has-jax';
          }
      });

    </script>

    <script  src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML" ></script>

  










</body>
</html>
