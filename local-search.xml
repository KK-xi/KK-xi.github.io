<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>paper reading 2| A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation</title>
    <link href="/2020/05/31/aper-reading-2-A-Unifying-Contrast-Maximization-Framework-for-Event-Cameras-with-Applications-to-Motion-Depth-and-Optical-Flow-Estimation/"/>
    <url>/2020/05/31/aper-reading-2-A-Unifying-Contrast-Maximization-Framework-for-Event-Cameras-with-Applications-to-Motion-Depth-and-Optical-Flow-Estimation/</url>
    
    <content type="html"><![CDATA[<h1 id="a-unifying-contrast-maximization-framework-for-event-cameras-with-applications-to-motion-depth-and-optical-flow-estimation-2018">A Unifying Contrast Maximization Framework for Event Cameras, with Applications to Motion, Depth, and Optical Flow Estimation (2018)</h1><p>Guillermo Gallego, Henri Rebecq, Davide Scaramuzza Dept. of Informatics and Neuroinformatics, University of Zurich and ETH Zurich(苏黎世大学和苏黎世联邦理工)</p><p><strong>Abstract：</strong>　本文提出了一个<font color="red">统一的框架</font>来解决事件摄像机的几种计算机视觉问题：运动，深度和光流估计。框架的主要思想是<font color="red">通过最大化目标函数：扭曲事件图像的对比度，在图像平面上找到与事件数据最佳对齐的点轨迹。</font>我们的方法隐式处理事件之间的数据关联，因此，它不依赖于有关场景的其他外观信息。除了准确地恢复问题的运动参数之外，我们的框架还可以生成具有高动态范围的<font color="red">运动校正边缘样图像</font>，可将其用于进一步的场景分析。所提出的方法不仅简单，而且更重要的是，这是第一种方法，可以成功地将事件摄像机应用于如此多样化的重要视觉任务集。</p><h1 id="一文章思路">一、文章思路</h1><p><strong>Motivation：</strong></p><p>（1）事件相机相比于传统相机的优点，但是需要技术方法来开发其优点；用group-of-events的方法，而不是event-by-event，因为每个事件所带的信息很少，并且有噪声，因此必须一起处理多个事件以产生足够的信噪比来解决所考虑的问题。</p><p>（2）Event-by-event：主要是基于扩展的卡尔曼滤波器框架；group-of-events：一般是针对特定的问题提出解决方案。（所以本文提出了一个统一的框架来处理事件组，同时利用它们的时间信息）。</p><p><strong>主要思想：</strong></p><p>　　本文的框架<font color="red">寻找图像平面上最适合事件数据的点轨迹，并通过这样做，能够恢复描述相机和场景之间的相对运动的参数。</font>对于事件的处理，是group-of-events的方式，<font color="red">利用时空和极性的信息。</font>不像逐事件处理的方法一样要依赖于其他的外观信息(可用灰度图像或场景光度图形式，这些数据可以从过去的事件中构建，也可以由其他传感器提供)，它既可以用于特征时间很短的估计问题(光流)，也可以用于估计时间较长的问题(单目深度估计)。处理了<font color="red">数据关联问题</font>，框架还生成运动校正的事件图像（近似于导致事件的图像梯度）。</p><h1 id="二-对比最大化网络构架">二、 对比最大化网络构架</h1><ul><li><strong>问题假设</strong>：光照恒定。</li><li><p><strong>问题提法：</strong>　在没有与产生事件相关的场景外观的附加信息（纹理信息）的情况下，从事件中提取信息的问题就变成了<font color="red">在事件之间建立对应关系的问题</font>（即数据关联：确定哪些事件是由同一场景的边缘触发的）。由于移动边缘反应图像平面上的点轨迹（轨迹为局部直线），因此希望沿着这些轨迹触发相应的事件。图1是一个简单的例子说明了这个想法，其中点的轨迹几乎是直线。 <img src="https://img-blog.csdnimg.cn/20200531145716377.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70)" srcset="/img/loading.gif" width="８0%"></p><p>图 1：（a）由运动边缘图案和图像平面的时空区域中的点轨迹引起的事件（点），根据事件极性进行着色（蓝色：正事件即亮度增加；红色：负事件即亮度降低）。（b）沿（a）中突出的点轨迹方向对事件进行可视化；相应的事件排成一行，得到了产生事件的边缘图案。我们的方法类似于（b）通过最大化扭曲事件图像的对比度来工作。</p><p>本文提议找到最适合事件数据的点轨迹，如图1b所示。下面用一个简单但重要的示例（光学流量估计）来描述本文的框架，然后将其泛化到其他估计问题。</p></li></ul><h2 id="例子光流估计">2.1 例子：光流估计</h2><ul><li><p><strong>符号</strong></p><p>ℇ={<span class="math inline">\(e_k\)</span>}<span class="math inline">\(_{k=1}^{N_e}\)</span>：如图1(a)，即假设的一系列事件（它们在在一个像素的时空邻域内）；</p><p>ek=(xk,yk,tk,pk)：每个事件，预先定义的亮度变化的时空坐标；</p><p>pk∈{−1，+1}：极性 (即亮度变化的标志)。</p><p>x(t) = x(0)+vt：位移近似的点的轨迹(局部笔直)，其中<span class="math inline">\(x=(x,y)^T\)</span>，v是点的速度即光流。<font color="red">因此希望对应的事件(由相同的边缘触发)位于这样的轨迹上(图1b)</font>；</p></li><li><p><strong>框架</strong></p><p>框架在图2中进行了概述，包括对事件进行计数或沿候选光流给出的直线轨迹对它们的极性求和，并计算所得和（H）的方差（f），该f测量了事件与候选者的吻合程度轨迹。 　　<img src="https://img-blog.csdnimg.cn/20200531150603364.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图 2：根据运动参数θ描述的点轨迹使事件变形，从而生成扭曲事件H(x;θ)的图像(即上述相关事件点沿候选光流给出的轨迹求和后的图像)。H的对比衡量了事件与候选点轨迹的吻合程度。</p><p><font color="red">具体计算步骤：</font></p><ol type="1"><li><p>根据局部光流常数假说[28]，假设光流在事件所跨越的时空邻域中是恒定的，并且使事件扭曲，即ek |→e′k，即： <img src="https://img-blog.csdnimg.cn/20200531150718836.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中，θ= v是候选速度。</p></li><li><p>然后构建扭曲事件的图像patch： <img src="https://img-blog.csdnimg.cn/20200531150758112.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中，每个像素x总和落入其中的扭曲事件<span class="math inline">\(x&#39;_k\)</span>的值bk（由狄拉克δ表示）,且<span class="math inline">\(δ(x-x&#39;_k)\)</span>体现了数据关联。如果bk = pk，则沿轨迹的事件的极性相加；而如果bk = 1，则计算沿轨迹的事件数。经过运动校正的图像斑块H，表示沿候选轨迹的亮度增量。</p></li><li><p>最后计算H的方差f，它是θ的函数： <img src="https://img-blog.csdnimg.cn/2020053115090760.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中NP是<span class="math inline">\(H=(h_{ij})\)</span>的像素数量，<span class="math inline">\(μ_H=(1/NP)∑_{i,j} h_{ij}\)</span>为H的均值。</p></li></ol><p>在图3b中显示了与图3a中的三个不同运动矢量<span class="math inline">\(θ_i\)</span>相对应的扭曲事件的图像（以伪彩色表示，从蓝色（很少事件）到红色（很多事件））。 　　<img src="https://img-blog.csdnimg.cn/20200531150944212.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图3：光流（基于patch）估计。正确的光流是使扭曲事件图像（图3b）的对比度（图3a）最大化的一种。(a). f(θ)作为光流θ≡v的函数，在(b)中显示了三个候选速度的扭曲事件H的相关图像加以说明；(b). 扭曲事件H(x;θ), 其<span class="math inline">\(θ_i\)</span>, i = 0,1,2 是(a)里的光流。</p><p>可以看出，扭曲事件在图像H中<font color="red">最佳对齐</font>时(指运动的方向，垂直于边缘的运动才会有事件产生，即水平方向的运动)，H有最大的方差，即b中最高的对比度(和清晰度)，达到<span class="math inline">\(θ∗= argmax(θ) ≈(−40,0)^⊤\)</span>pixel / s。因此，<font color="red">光流估计策略就是寻找最大化方差f的参数θ</font>。</p></li><li><p><strong>数据关联</strong></p><p>架还隐式定义了事件之间的数据关联。用平滑近似δ(x)≈δε(x)，例如高斯δε(x− µ)代替(2)中的增量。可以看到每个扭曲事件<span class="math inline">\(e&#39;_k\)</span>对每个其他事件<span class="math inline">\(e&#39;_n\)</span>都有影响，并且影响量由<span class="math inline">\(δε(x&#39;_n− x&#39;_k)\)</span>在高斯情况下，与欧几里得距离<span class="math inline">\(||x&#39;_n-x&#39;_k||\)</span>有关。这个内置的软数据关联，由它们之间的距离的函数隐式给出：扭曲事件越远，对应事件的可能性就越小。（<font color="fuchsia">重点：为了找出对应事件，数据关联</font>）</p></li></ul><h2 id="框架概述">2.2 框架概述</h2><p>以上是针对光流估计说的，下面是可以泛化到其他任务的通用框架。</p><p>如图3b所示，好的轨迹是对齐相应事件的轨迹，因此提出的目标函数可衡量事件沿候选轨迹的对齐程度。框架有两个辅助输出：（i）估计的点轨迹隐式地建立了事件之间的对应关系（即数据关联，判断是否为相同轨迹的对应事件），（ii）轨迹可用于校正边缘的运动。</p><ul><li><strong>步骤</strong></li></ul><ol type="1"><li>根据上述几何模型(2)和候选参数θ定义的点轨迹将事件扭曲成图像H：</li></ol><p>扭曲（如（1）中的W）将每个事件沿着通过它的点轨迹传输，直到达到参考时间（例如第一个事件的时间）：<span class="math inline">\(e_k=(x_k, y_k, t_k,p_k) |→(x&#39;_k，y&#39;_k，t_{ref}，p_k)= e&#39;_k\)</span>。</p><ol start="2" type="1"><li>根据扭曲事件的图像计算分数f：</li></ol><p>创建扭曲事件H（ℇ'）的图像或直方图（使用它们的极性pk或它们的事件数），然后计算目标函数(色散的度量) f(H(ℇ'))。使用H的方差作为色散度量，其在图像处理术语中被称为<font color="red">对比度</font>，并且我们力求使其最大化。目标函数根据候选模型参数θ表示扭曲事件ℇ'的统计量f，因此它衡量<span class="math inline">\(θ_t\)</span>对事件数据ℇ的拟合度。</p><ol start="3" type="1"><li>相对于模型参数优化得分或目标函数：</li></ol><p>用如梯度上升或牛顿法之类的优化算法来获得最佳模型参数(光流估计、深度估计结果)，<font color="red">即最能解释事件数据的图像平面上的点轨迹</font>。（该框架不依赖于任何特定的优化器）</p><ul><li><strong>最大化对比度</strong></li></ul><p>　　最大化扭曲事件H的图像方差，文章倾向于在图像平面上累积（即对齐）扭曲事件的点轨迹。在某些区域中扭曲事件会累积，则在其他区域中事件会分散（因为事件的总数N是恒定的），这样会产生较大范围的图像H，因此具有较高的对比度。从本质上讲，优化框架的目标是以类似于[30]中的分割方法的方式，“拉开”具有和不具有事件的区域的统计信息（例如，极性）。</p><ul><li><strong>计算复杂度</strong></li></ul><p>核心是扭曲事件图像的计算（2），其计算复杂度与要扭曲事件的数量呈线性关系。对比度（3）的计算通常可以忽略不计。该方法还取决于用于最大化对比度的算法的选择，这取决于应用程序。</p><h2 id="深度估计">2.3 深度估计</h2><ul><li><p><strong>问题假设：</strong></p><p>假设存在问题，则每次时间 t 已知事件相机P(t)的姿势，其中P表示相机的投影矩阵；假设相机的固有参数也是已知的，并且镜头失真已被消除。</p></li><li><p><strong>问题提法：</strong></p></li></ul><p>　　在这种情况下，通过3D点投影获得的图像点的轨迹通过摄像机的已知6自由度运动和3D点相对于参考视图的深度进行参数化。这将在图像平面中生成一维曲线系列，并按深度进行参数化。每个深度值都会给出不同的曲线，然后是图像平面中的点，如图4所示。 <img src="https://img-blog.csdnimg.cn/20200531153036656.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图4：深度估计。左：相对于相机的3D点（鸟瞰）的轨迹。离摄像机较近的点具有较大的视在运动，因此通常在像平面上描述较长的轨迹。右：相机进行6自由度运动时，相对于参考视图的不同深度值的图像点的轨迹（图像中心，黑色）。每个深度值都会产生不同的点轨迹，产生一维曲线系列(深度值越小，场景离相机越近，所以运动轨迹的范围看起来更大，但是都会经过同一个中心点)。这是来自事件摄像机数据集的2s段的示例。</p><p>　　与[21]的“空间扫描”方法一样，文章考虑虚拟摄像机在某个位置（例如，沿事件摄像机轨迹的一点）提供的参考视图（参见图5）。 　　<img src="https://img-blog.csdnimg.cn/20200531153116629.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图5：深度估计。对于相对于参考视图（RV）测量的不同深度值θ≡Z，扭曲事件<span class="math inline">\(x&#39;_k(θ)\)</span>的对准。在(b)中，扭曲事件patch用伪彩色表示，从对齐的几个事件(蓝色)到很多事件(红色)。 在正确的深度，色块具有最高的方差（即图像对比度大）。{图5说明：在图5a中，具有光学中心C(t)的事件相机在带有物体(灰色框)的场景前移动。使用三个候选深度值通过扭曲(4)将两个事件<span class="math inline">\(e_i =(x_i，t_i，p_i)\)</span>，i = {1,2}从事件相机传输到参考视图（RV）}。</p><p>在参考视图中为一个patch制定问题，为简单起见，假设面片内的所有点都具有相同的深度，即某些候选值θ=Z。方法的三个主要步骤如下：</p><ol type="1"><li><p>（a）使用候选深度参数将事件传输到参考视图上，如图5所示，使用扭曲（W）将事件ek传输到参考视图上。事件<span class="math inline">\(e&#39;_k =（x&#39;_k，t_{ref}，p_k）\)</span>其中<img src="https://img-blog.csdnimg.cn/2020053115325957.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中P（t）是事件摄像机在时间t的姿态和<span class="math inline">\(Pv= P(t_{ref})\)</span>是虚拟摄像机的姿态。扭曲与空间扫描多视图立体图中的扭曲相同：点是使用平面单应性传输的。由平行于参考视图像平面并在给定深度处的平面所诱导（见图5）;（b）计算沿候选点轨迹(例如图4)的事件数，创建扭曲事件(2)的图像(patch)。</p></li><li><p>通过扭曲事件图像的平均软方差（即对比度）测量事件与深度值θ之间的拟合优度（3）;</p></li><li><p>通过更改深度参数θ来最大化对比度。</p></li></ol><ul><li><strong>结果</strong></li></ul><p>如下图，是一个灰度图(a)中不同的patch(对应真实depth分别为1.1m、1.8m)，及其patch的H方差随深度参数的变化而变化的曲线(b)（可以看出：在准确深度处的方差最大），还有不同深度处对应的扭曲事件图H(c)（扭曲事件图恢复了patch中引起事件的边缘图近似值）。 <img src="https://img-blog.csdnimg.cn/2020053115344390.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 上述过程在参考视图中生成patch中心的深度值。对参考像机中有存在边缘的每个像素重复这一过程，生成一个半密度深度图。类似下图： <img src="https://img-blog.csdnimg.cn/20200531153505905.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/20200531153511168.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><h2 id="旋转运动估计">2.4 旋转运动估计</h2><ul><li><strong>问题假设</strong></li></ul><p>假设相机经过校准(已知本征参数，无镜头畸变)。</p><ul><li><p><strong>问题提法</strong></p><p>事件相机在静态场景中旋转，则目标是：使用事件估计相机的自运动。本文的框架通过最大程度地允许一组轨迹上的对比度来对齐事件：这些轨迹与旋转运动兼容。</p></li><li><p><strong>步骤</strong></p><p>考虑一个较小的时间窗口[0，∆t]上的所有事件ℇ，在足够小的时间窗口内，<font color="red">角速度ω可以认为是恒定的</font>，并且令<span class="math inline">\(t_{ref}\)</span> =0。在校准坐标中，图像点根据<span class="math inline">\(\tilde{x}\)</span>(t)∝ R(t)<span class="math inline">\(\tilde{x}\)</span>(0)进行变换，其中<span class="math inline">\(\tilde{x}\)</span>∝(x⊤，1)⊤是齐次坐标，R(t)= exp<span class="math inline">\(\hat{ω}\)</span>(t)是（3D）运动的旋转矩阵：exp是旋转群SO(3)的指数图，<span class="math inline">\(\hat{ω}\)</span>是与ω相关的叉积矩阵。</p></li></ul><ol type="1"><li>根据点轨迹模型将事件扭曲到<span class="math inline">\(t_{ref}：x&#39;_k = W(x_k, t_k;θ)\)</span>，其中θ=ω的角速度为 <img src="https://img-blog.csdnimg.cn/20200531154316812.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li><p>计算(2)中的扭曲事件图（不需要极性）；</p></li><li><p>使用标准优化算法（例如非线性共轭梯度）将目标函数（3）最大化。</p></li></ol><ul><li><p><strong>结果</strong></p><p>图8显示了文章的方法对一组Ne = 30000的事件ℇ进行处理的结果，该事件是在相机围绕其光轴旋转时捕获的。可以看到，本文的方法会从扭曲的事件图像中估计出消除运动模糊的运动参数，从而提供最清晰的图像。 <img src="https://img-blog.csdnimg.cn/20200531154413839.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图9a显示了估计的和ground truth，几乎没有差别。图9b分析了在15s的四个子间隔中，随着角速度的增加，它们之间的误差（因此误差也增加了）。本文的方法非常精确，相对于670º/s的峰值偏移，RMS误差约为20o/s，这意味着3％的误差。而且不需要场景估计运动的全景图，它也不需要在拟合3D运动之前估计光流。 <img src="https://img-blog.csdnimg.cn/20200531154436446.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li></ul><h2 id="平面场景中的运动估计">2.5 平面场景中的运动估计</h2><ul><li><p><strong>问题提法</strong></p><p>在这一节中，假设为<font color="red">平面场景下的运动估计问题(平面单应性估计)</font>，它要获取摄像机的自运动参数(<font color="red">旋转和平移</font>)以及包含场景结构的平面参数。</p><p>图像点根据<span class="math inline">\(\tilde{x}\)</span>(t)∝ H(t)(0)进行变换，其中<span class="math inline">\(\tilde{x}∝(x^⊤，1)^⊤\)</span>是齐次坐标，H(t)是3×3单应性矩阵。为简单起见，令<span class="math inline">\(t_{ref}\)</span>= 0，因此<span class="math inline">\(H(0)= I_d\)</span>是恒等式。x(t)描述的点轨迹具有与H(t)相同的DOF数量，在短时间内，认为H为常数，DOF的数量为8-DOF。沿由候选单应性H（t）定义的点轨迹x（t）聚集事件，<font color="red">并最大化扭曲事件的结果图像的对比度，以恢复能最好地解释事件数据的单应性</font>。</p></li><li><p><strong>步骤</strong></p><p>在<span class="math inline">\(π=(n^⊤，d）^⊤\)</span>坐标平面引起的单应性的情况下，有<span class="math inline">\(H(t)∝ R(t)− (1 /d)t(t)n^⊤\)</span>。在短时间间隔内的t∈[0，∆t]，假设v和ω在t内是恒定的：R(t)= exp(<span class="math inline">\(\hat{ω}\)</span>t)和t(t)= vt，所以可以通过<span class="math inline">\(θ=(ω^⊤，v^⊤/ d，φ，ψ)^⊤∈R^8\)</span>来参数化H(t)≡H(t;θ)（<font color="fuchsia">即通过参数扭曲事件</font>），其中2个自由度(φ，ψ)参数化了平面n。参数v / d解释了在没有其他信息的情况下，存在尺度模糊性：平面单应性的分解仅提供了平移的方向，<font color="red">但没有提供其大小</font>。</p></li></ul><ol type="1"><li>虑短时间间隔[0，Δt]中的事件ℇ，并使用在校准坐标中指定的扭曲将它们映射到参考视图的图像平面上： <img src="https://img-blog.csdnimg.cn/2020053115504859.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li><p>通过(2)求得扭曲事件图；</p></li><li><p>求取方差即对比度，以评估事件对齐时参数θ的质量。</p></li></ol><ul><li><p><strong>结果</strong></p><p>在图10a中，Ne＝50000，每个像素表示其内事件的数量，即在(2)中使用扭曲<span class="math inline">\((x′_k＝x_k)\)</span>。图10b是对比度最大化的结果：对于最优参数θ，扭曲事件有最佳对齐，从而图像(2)具有比图10a中更高的对比度。该过程会产生运动校正图像，其纹理边缘特明显：在图10a(无运动校正)中边缘模糊，而在10b中边缘清晰。<img src="https://img-blog.csdnimg.cn/20200531155159575.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中(b)使用<font color="red">使图像对比度最大化的平面单应性参数θ= {ω，v / d，n}扭曲事件</font>：<span class="math inline">\(ω=(0.086,0.679,0.439)^⊤\)</span>，<span class="math inline">\(v / d =(0.613, −0.1,0.333)^⊤\)</span>，<span class="math inline">\(n＝(0.07, 0.075, -0.995)^⊤\)</span>。</p><p>手持事件摄像机，当一个人在砖石地面上行走时，向下看。由单应性估计得出的经过运动校正的图像（参见图10b）产生更好的结果，这可以通过代表场景中地面的更多平点云看到。 <img src="https://img-blog.csdnimg.cn/20200531155355739.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图11：平面场景中的运动估计。通过视觉惯性算法获得的<font color="red">场景结构(黑点)</font>和摄像机运动(绿色轨迹)，带有和不带有经过运动校正的事件图像。</p></li></ul><h1 id="三优点和缺点">三、优点和缺点</h1><p>　　这篇文章是第一次对任务用统一的框架进行集成估计，是18年发表的。</p><p>　　优点就是：第一次有人这么做，一个统一的框架完成了多个估计任务，寻找使得事件轨迹与产生事件的边缘最佳对齐的估计参数，在这过程中对事件进行了数据关联，最后得到的某些效果还不错。</p><p>　　其缺点：对事件的处理方式(事件计数/汇总)，忽略了大部分时间信息；用的是传统的方法，并且不能进行end-to-end的计算，方法上相比19年的无监督集成估计那篇文章，没用到深度网络来求解；而且没有与其他文章的估计结果对比，也没有定量的结果，是好是坏或许还需要进一步评估。</p><p>　　用无监督的end-to-end的深度网络完成对这几个任务的集成估计的工作，还是可以进一步进行。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>event-based camera</category>
      
      <category>paper reading</category>
      
      <category>任务集成</category>
      
    </categories>
    
    
    <tags>
      
      <tag>event-camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>paper reading 1| Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion</title>
    <link href="/2020/05/30/aper-reading-1-Unsupervised-Event-based-Learning-of-Optical-Flow-Depth-and-Egomotion/"/>
    <url>/2020/05/30/aper-reading-1-Unsupervised-Event-based-Learning-of-Optical-Flow-Depth-and-Egomotion/</url>
    
    <content type="html"><![CDATA[<h1 id="unsupervised-event-based-learning-of-optical-flow-depth-and-egomotion2019">Unsupervised Event-based Learning of Optical Flow, Depth, and Egomotion(2019)</h1><p>Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, Kostas Daniilidis University of Pennsylvania {alexzhu, lzyuan, chaneyk, kostas}<span class="citation" data-cites="seas.upenn.edu">@seas.upenn.edu</span></p><p><strong>Abstract：</strong> 在这项工作中，提出了一种用于事件相机无监督学习的新颖框架，该框架仅从事件流中学习运动信息。特别地，以离散量的形式提出事件的输入表示，该离散量保持事件的时间分布，通过神经网络来预测事件的运动。此运动用于尝试消除事件图像中的任何运动模糊。然后，我们提出一种应用于运动补偿事件图像的损耗函数，该函数可测量该图像中的运动模糊。我们在此框架下训练了两个网络，一个用于预测光学流，一个用于预测自我运动和深度，并在“多车立体视觉事件摄像机”数据集上评估这些网络，以及来自各种不同场景的定性结果。</p><h2 id="一文章思路">一、文章思路</h2><p>首先event-camera需要开发新的方法去解决传统SLAM/机器人系统中的一些问题。</p><p>现在基于事件相机的SLAM 的方法还比较少，作者针对两篇最近的paper[24,20]存在的缺陷进行了改进。<strong>缺点一：</strong> 这两篇文章都用了颜色一致性原理，分别用于灰度图像和事件图像，前者依赖于灰度图像，后者的颜色一致性假设可能在很模糊的场景中不成立；<strong>缺点二：</strong> 两项工作都将时间数据进行了汇总，这样一来就丢失了时间信息。</p><p><font color="red">文章提出的方法思路：</font><strong>其一</strong>提出了一个无监督网络构架；<strong>其二</strong>针对上述缺陷（一个用了灰度图，一个用事件图，且忽略时间信息）提出了一种新的输入表示，包含所有的时空信息（事件和时间信息都是离散化的），然后以类似于插值的线性加权方式来将事件积累到一起；<strong>其三</strong>提出了新的无监督Loss函数；<strong>其四</strong>同时训练两个网络，分别预测光流，深度和自运动，将预测结果用以消除在事件投影到二维图像时产生的运动模糊，如下图所示。关于无监督方式，无监督损失会测量去模糊后的事件图像中的运动模糊量，从而为网络提供训练信号，另外还将去模糊事件图像可与edge maps比较，对这它们的人口普查变换应用了立体损失，以使我们的网络能够了解度量姿态和深度。 <img src="https://img-blog.csdnimg.cn/20200530082857175.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图 1：网络学习通过从一组事件的输入，模糊，事件（左）中预测光流（顶部）或自运动和深度（底部），并通过对图像进行去模糊后使运动模糊量最小化，来从运动模糊中预测运动。 产生去模糊图像的预测运动（右）。 彩色效果最佳。</p><h2 id="二文章贡献">二、文章贡献</h2><ol type="1"><li><p>事件表示上：提出新的离散的事件表示形式，将事件输入到神经网络中；</p></li><li><p>无监督损失：一个基于运动模糊损失函数的新应用，只允许从事件中无监督地学习运动信息（改编自[13]）；</p></li><li><p>损失：一种新的立体视觉相似度损失应用于一对去模糊事件图像的人口普查变换；</p></li><li><p>评估方法上：对Multi Vehicle Stereo Event Camera dataset[26]进行定量评估，并对各种夜间和其他具有挑战性的场景进行定性和定量评估。</p></li></ol><h2 id="三相关的工作">三、相关的工作</h2><p><strong>1. 光流估计</strong></p><p><strong>基于模型:</strong></p><p>[2]：通过将平面与x-y-t空间中的事件拟合，可以估计出正常的流。</p><p>[1]：流量估计可以写成一个联合求解图像强度和流量的凸优化问题。</p><p><strong>无模型的，深度网络：</strong></p><p>[8]：建立了一个可以从亮度稳定性和平滑度中学习光流的网络。</p><p>[12]：扩展[8]的工作，通过双向的普查损失提高估计流的质量。</p><p><strong>2. 相机姿态(运动)和深度估计：</strong></p><p>[9]：在SFM和VIO中，[9]证明卡尔曼滤波器可以重建相机位姿和局部地图。</p><p>[23]：网络使用摄像机重投影和颜色一致性损失来学习相机的自运动和深度。</p><p>[22][18]：增加了立体约束，使网络能够学习绝对尺度。</p><p>[19]：将上述概念与递归神经网络一起应用。</p><p><strong>3. 视觉里程计：</strong></p><p>[25]：使用基于EM（期望最大化）的特征跟踪方法进行视惯性测程。</p><p>[16]：使用运动补偿对事件图像进行去模糊，并运行标准的基于图像的特征跟踪来进行视惯性测程。</p><p><strong>4. 最近的工作:</strong></p><p>[4、5、13、25 、?]：可以从事件的时空量，通过沿着光流的方向传播，试图将事件图像的运动模糊最小化，以此来估计光流和其他类型的运动信息。(运动模糊作为一种损失的概念可以被看作是一种类似于帧的光度误差，应用于事件。)</p><h2 id="四网络构架">四、网络构架</h2><p>网络构架的Overview如下： <img src="https://img-blog.csdnimg.cn/20200530083302800.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p><strong>1、Input：离散事件量</strong></p><p>（以往的表示：通过对每个像素的事件数求和来生成事件图像；总结每个像素上的事件数量，以及每个像素上的最后时间戳和平均时间戳。这两种表示输入到网络中，都能准确预测光流。存在的问题：虽然这保留了一些时间信息，但是通过总和事件中的高分辨率时间信息仍然丢失了很多信息。）</p><p><font color="red">本文设计的输入表示的目的：</font>为了提高沿时域的分辨率超过bin的数量，使用类似于双线性插值的线性加权累加将事件插入到该量中。</p><p><font color="red">好处：</font>在没有事件重叠的情况下，能准确得出事件集；有重叠时，求和确实会损失一些信息，但保留了窗口跨时空维度的事件分布。</p><p>对于N个输入事件，以及一个集合B 组距来离散时间维度，将时间戳扩展到[0,B−1]范围内。表示的公式如下： <img src="https://img-blog.csdnimg.cn/20200530083435487.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> kb(a)是等价于[7]中定义的双线性采样核。(相机不失真或者校正的时候，必须在x,y维上插值，会导致非整数像素) 将时域作为传统2D图像中的通道，并在x,y空间维度上执行二维卷积。</p><p><strong>2、运动补偿监督</strong></p><p>解决的问题：当事件相机记录对数强度的变化时，颜色一致性的标准模型并不直接适用于事件。</p><p>运动补偿概念：[16]</p><p>运动补偿目的：利用每个事件的运动模型对事件图像进行去模糊处理，如图： <img src="https://img-blog.csdnimg.cn/20200530083521550.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 图 2 网络从运动模糊中通过从一组输入、模糊、事件(2)，预测光流、自运动或者深度(1)来学习预测运动，减少预测运动的去模糊后的运动模糊来产生去模糊图像(3)。流的颜色标识方向，在颜色图(4)中。</p><p>对于最一般的逐像素光流量u(x,y)，v(x,y)，可以传播事件<span class="math inline">\({(x_i,y_i,t_i,p_i)}_{i=1,2,...,N}\)</span>到单个时间<span class="math inline">\(t&#39;\)</span>: <img src="https://img-blog.csdnimg.cn/20200530083604496.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>如果输入的光流是正确的，将反转事件中的运动，并去除运动模糊；如果不正确，则可能导致进一步的运动模糊。</p><p><font color="red">评估去模糊效果作为监督信号：</font>[4]里用传播事件生成的图像上的图像方差，缺点就是通过预测将图像每个区域内的所有事件推到一条直线上的流值，网络很容易过拟合这种损失；文章采用[13]中的损失，就是最小化每个像素的平均时间戳的平方和，但这种损失不可微（时间戳是四舍五入来产生2D图像的），本文用双线性插值代替四舍五入。具体做法：</p><ol type="1"><li>首先通过将事件按极性分开，并生成每个像素的平均时间戳<span class="math inline">\((T+，T-)\)</span>的图像来应用损失:</li></ol><p><img src="https://img-blog.csdnimg.cn/20200530083934259.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 上式中<span class="math inline">\(t&#39;\)</span>改变，式中使用的时间戳不会变。损失就是两幅图像的和的平方 <img src="https://img-blog.csdnimg.cn/20200530084005374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p>然而，使用单个<span class="math inline">\(t&#39;\)</span>来表示这种损失会造成规模问题。在公式(4)里面，输出的流u、v用<span class="math inline">\((t&#39;-t_i)\)</span>缩放，在反向传播的时候，这将权衡时间戳距t'更高的事件的梯度，而时间戳非常接近t'的事件将被忽略。为减轻这个缩放，文章计算了前向和向后的损失，<span class="math inline">\(t’=t_1,t’=t_N\)</span>: <img src="https://img-blog.csdnimg.cn/20200530084143569.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p><strong>3、光流预测网络</strong></p><p>使用类似[24]中的编码-解码器。网络以像素/bin为单位输出流值，应用到(4)并最终计算(9)。预测光流的网络使用了(7)里面的时间损失，结合局部光滑正则化： <img src="https://img-blog.csdnimg.cn/20200530084212560.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中<span class="math inline">\(\rho(x)=\sqrt{x^2+\epsilon^2}\)</span>是[3]里面的Charbonnier损失，N(x,y)是(x,y)附近的4连通邻域。总的光流预测的损失为： <img src="https://img-blog.csdnimg.cn/20200530084520841.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><p><strong>4、自运动和深度估计</strong></p><p>类似[22,18]中的预测相机自运动和场景结构：</p><p>给定一对时间同步的离散事件量，分别将每个量传递到网络中，但是在训练时使用这两个量来应用立体视差损失，使网络能够学习度量尺度。还用了运动补偿中的时间戳损失以及一个鲁棒的去模糊事件图像的人口普查变换间的相似性损失[21,17]。</p><p>网络预测欧拉角（ψ，β，φ），平移T和每个像素的视差di。视差是使用与流网络中相同的编码器-解码器体系结构生成的，不同之处在于最终激活函数是S型，并通过图像宽度进行缩放。姿态与视差共享编码器网络，并且由跨步卷积生成，这些卷积通过6个通道将空间尺寸从16×16减小到1×1。</p><ul><li><strong>时间投影损失</strong></li></ul><p>给定网络输出，相机的内部构造, K，以及两个相机之间的基线，b，每个事件在像素位置(xi,yi)的光流(ui,vi)为： <img src="https://img-blog.csdnimg.cn/20200530084612420.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中，f是相机的焦距，R是对应于（ψ，β，φ）的旋转矩阵，并且π是投影函数<span class="math inline">\(π((X Y Z)^T)=(\frac{X}{Z} \frac{Y}{Z})^T\)</span>。请注意，由于网络仅在输入处为离散量，因此它不知道时间窗口的大小。最后我们计算的光流以像素/bin为单位，其中B是用于生成输入量的像素数量，然后将光流代入（4）中以计算损失。</p><ul><li><strong>立体视差损失</strong></li></ul><p>1、从光流中，可以使用（4）对左右摄像机的事件进行模糊处理，并生成一对事件图像，该图像对应于去模糊后每个像素处的事件数。给定正确的流，这些图像代表相应灰度图像的边缘图，在该图像上可以应用光度损失。</p><p>2、但是，两个摄像机之间的事件数量也可能有所不同，因此对图像的普查变换[21]应用了相似性损失。</p><p>3、另外，如[6]所定义的，在两个预测的视差之间应用了左右一致性损失。</p><p>4、最后，如（8）所示，我们对视差应用了局部平滑度正则化器。</p><p>SFM模型总损失：</p><figure><img src="https://img-blog.csdnimg.cn/20200530085045874.png" srcset="/img/loading.gif" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="五实验">五、实验</h2><p><strong>1、Train</strong></p><p>MVSEC[26]的整个户外day2序列中训练：由11分钟的立体视觉事件数据在公共道路上行驶组成。</p><p>（在训练中，每个输入包含N = 30000个事件，这些事件将转换为具有256x256分辨率（集中裁剪）和B = 9 bins的离散事件量。每个损失的权重为：<span class="math inline">\((λ1，λ2，λ3，λ4) =( 1.0,1.0,0.1,0.2)\)</span>。）</p><p><strong>2、Test</strong></p><p>光流：在MVSEC的室内流动和室外白天序列上测试了光流网络，ground truth由[24]提供。</p><p>自运动：MVSEC评估我们的户外day1序列的自我运动估计网络。</p><p><strong>3、 Evaluate metric</strong></p><p><font color="red">光流：</font>平均端点误差（AEE）、AEE大于3个像素点的百分比、超过具有有效ground truth且至少有一个事件的像素点的百分比。</p><p><font color="red">自运动：</font>相对位姿误差(RPE)和相对旋转误差(RRE) <img src="https://img-blog.csdnimg.cn/2020053008523364.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 其中，Rpred是与输出的欧拉角对应的旋转矩阵，logm是矩阵对数。</p><p><font color="red">深度：</font>度不变深度度量。</p><h2 id="六结果">六、结果</h2><p><strong>1、光流估计</strong></p><p><img src="https://img-blog.csdnimg.cn/20200530085345635.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 表1光流估计结果</p><p>对于每个序列，平均端点误差(AEE)以像素计算，%离群值以AEE &gt; 3 pix的点的百分比计算。dt=1是在两个连续的灰度帧之间的一个时间窗口计算得到的，dt=4是在四个灰度帧之间。 <img src="https://img-blog.csdnimg.cn/20200530085407198.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> <img src="https://img-blog.csdnimg.cn/20200530085421678.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 从左到右：灰度图像，事件图像，带有航向的深度预测，带有航向的ground truth。 前两个是流量结果，后一个是深度结果。 对于深度，越近越亮。航向绘制为圆形。 在户外夜晚的结果中，由于闪光灯发出的事件导致航向偏向。</p><p><strong>2、自运动估计结果</strong></p><p><img src="https://img-blog.csdnimg.cn/20200530085536727.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 本文的相对姿态和旋转误差明显优于SFM-Learner，但比ECN差。但是，ECN只能预测5dof姿态，但要达到比例因子，而我们的网络必须学习具有比例的完整6dof姿态。</p><p><strong>3、深度估计</strong> <img src="https://img-blog.csdnimg.cn/20200530085554800.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 序列中，都比Monodepth表现得更好，这可能是因为事件没有强度信息，因此网络被迫学习对象的几何属性。 <img src="https://img-blog.csdnimg.cn/20200530085610171.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> 根据ECN[20]对深度网络的标准深度指标进行定量评估从左到右，度量标准是:绝对相对距离、RMSE对数、比例不变对数，以及预测深度大于或小于地面真实值1.25、1.252和1.253倍的点的百分比</p><h2 id="七优点及问题">七、优点及问题</h2><p>1、室外夜晚的自运动估计结果不好，因为夜晚有日光灯照射，会产生虚假事件——&gt;有必要进行今后的工作以滤除此类异常(例如，如果先验闪光速率已知，则可以通过检测以所需频率产生的事件来简单地过滤光)。</p><p>2、但是，深度估计中的δ百分比低于预期，或许将来用在其他数据集上的效果会好些。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>event-based camera</category>
      
      <category>paper reading</category>
      
      <category>任务集成</category>
      
    </categories>
    
    
    <tags>
      
      <tag>event-camera</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision for Autonomous Vehicles | Chapter 6 Object Tracking</title>
    <link href="/2020/05/07/Computer-Vision-for-Autonomous-Vehicles-Chapter-6-Object-Tracking/"/>
    <url>/2020/05/07/Computer-Vision-for-Autonomous-Vehicles-Chapter-6-Object-Tracking/</url>
    
    <content type="html"><![CDATA[<h2 id="computer-vision-for-autonomous-vehicles-problems-datasets-and-state-of-the-art">《Computer Vision for Autonomous Vehicles Problems, Datasets and State of the Art》</h2><p>Author：Joel Janai, Fatma Guney, Aseem Behl, Andreas Geiger</p><p>Date：December 18, 2019</p><p><font color="navy"><strong>声明：</strong> 这是以上作者的关于自动驾驶的综述文章，写得非常好，里面涉及的方法也几乎是近几年比较流行比较好的。我这里只是阅读笔记，方便以后查阅，如有侵权会立即删除。</font> <br></p><p><font color="navy"><strong>Disclaimer:</strong> This is a review article by the above authors on autonomous driving. It is very well written, and the methods involved are almost popular and better in recent years. I just record the reading notes here for future reference, if there is any infringement, it will be deleted immediately.</font></p><h3 id="chapter-6-object-tracking">Chapter 6 Object Tracking</h3><h4 id="problem-definition">6.1 Problem Definition</h4><p>　　目标跟踪的目的是随着时间的推移根据传感器的测量去跟踪一个或者多个目标的状态，状态包括位置、速度和加速度。在自动驾驶中，目标跟踪极其重要，对未来的轨迹做出预测，才能提前制动，防止碰撞，尤其是对于行人和非机动车辆的突然变动。所以在他们周围行驶及其小心，跟踪与交通参与者的分类相结合可以相应地调整车辆的速度。</p><p>　　跟踪系统的难点在于背景的复杂，机动的变化和复杂性以及遮挡问题。随着时间的推移，由于不同对象(尤其是相同类)的相似性，将同一对象的实例关联起来的问题变得特别具有挑战性，因为他们缺乏辨识度，而且同一类的在不同时间可能因为遮挡会看起来不太相似。对象之间的互动，增加了遮挡的可能性，更困难的情况就是光照条件以及镜子或窗户的反光的影响。</p><h4 id="methods">6.2 Methods</h4><p>　　长久以来，跟踪问题都被看作是贝叶斯推理问题来解决，其目标是在给定当前观测和先前状态的情况下估计状态的后验概率密度函数。后验通常以递归的方式更新，包括使用运动模型的预测步骤和使用观察模型的校正步骤，在每次迭代中，都要解决数据关联问题，将新的观察值分配给被跟踪的对象。递归的方法很难从检测错误中恢复，并通过遮挡去跟踪目标，所以非递归的方法，根据时间窗口内的所有轨迹优化全局能量函数，得到了广泛的应用。然而，在一个场景中，每个对象可能的目标轨迹的数量和潜在对象的数量都非常大，这就导致了一个非常大的搜索空间。</p><h5 id="tracking-by-detection">6.2.1 Tracking by Detection</h5><p>　　考虑到静态对象检测器的成功，跟踪中通常使用的一种范式是通过检测跟踪，这个方法通常有两个步骤：先检测人，再是同一个人的关联检测。由于跟踪问题被归结为数据关联问题，因此通过检测跟踪变得非常流行，但是跟踪系统仍然需要处理和从检测系统的错误中恢复，例如错误和丢失检测。</p><p><strong>Tracking on Graphs：</strong> 图表示的方法如下图所示，用多切分公式求解的基于图的表示法，通过对上面图像的检测，生成图，再通过求解多切分问题，得到图的着色和连接，这样的方法广泛用在推理关系中：</p><p><img src="https://img-blog.csdnimg.cn/20200507082658143.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" width="70%"> 　　在网络流方法中，首先用节点来表示检测，用边来表示检测之间的空间和时间链接，然后定义一些简单的约束条件作为一个整数程序，再将其放宽为一个线性程序，以避免整数程序的NP难题。各种动态规划方法，例如使用线性规划，k最短路径或集合覆盖优化，都已经被提出来解决网络流。</p><p>　　另一个研究方向是图形短语跟踪聚类问题。Minimum Clique和 Minimum Cost Multicut 的方法 找到一个具有最小成本和的图的分解。最大权值独立集公式首先独立求解成对关联问题，并使用学习距离度量将两两关联的解联系起来。图形化模型最小化定义在具有成对和高阶势的节点上的全局能量函数。</p><p><strong>Continuous Optimization：</strong> 连续得能量最小化的提出是为了替代离散化。对于这个高度非凸的问题，Andriyenko 和 Schindler使用了一个具有<font color="red">重复跳跃动作的启发式能量最小化方案</font>来防止局部极小，并更好地探索了变维搜索空间。其能量函数的不同分量的影响如图所示，上排和下排显示的是能量更高、更小的构型，较暗的灰度值对应较高的目标概率:</p><p><img src="https://img-blog.csdnimg.cn/20200507093424855.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" width="70%"></p><p>Milan等扩展了上述的连续能量函数，将目标动力学、互斥和跟踪持久性等物理约束考虑在内。将每个观察值分配给数据关联中的特定目标本质上是一个离散的优化问题，所以 Andriyenko 等人认为联合离散和连续公式更自然地描述了跟踪问题。Milan等人又提出了一种混合的离散-连续条件随机场模型，解决了数据关联和轨迹估计中的互斥问题，在数据关联过程中，每次观测最多分配到一个目标，而在轨迹估计中，两条轨迹始终保持空间分离。</p><p><strong>Multiple Cues：</strong> 在数据关联中，为了提高跟踪系统的鲁棒性，可以结合各种互补的线索。Giebel 等研究了一种基于不同线性子空间模型的时空形状表示方法，在粒子滤波的观测模型中，他们通过结合形状、纹理和立体深度来处理外观变化。Gavrila 和 Munder 在检测和跟踪系统中使用具有一系列模块的相同线索，模块即感兴趣区域生成、基于形状的检测、基于纹理的分类和基于立体的验证。他们的系统可以专注于通过基于立体的感兴趣区域方法推断出的相关图像区域。他们根据形状匹配的结果，通过加权基于纹理的组件分类器，提出了一种新颖的专家混合架构。在他们基于外观的方法中，Choi等人提出组合的检测系统，每个检测系统专门处理不同的任务，例如行人和上半身、面部、肤色、基于深度的形状和运动。所有检测部分的结果都结合在观察可能性中，以提高匹配度。</p><h5 id="tracking-with-stereo">6.2.2 Tracking with Stereo</h5><p>　　一些工作已经研究了研究了一种用于目标跟踪和立体深度估计的联合公式，以便在估计场景中的对象轨迹的同时获得场景的结构，场景的结构使跟踪系统可以专注于更合理的解决方案。Leibe 等提出了一种集成场景几何估计、2D 对象检测、3D 定位、轨迹估计和跟踪的方法。 他们使用对象的检测和 top-down 分割来学习特定于对象的颜色模型，如图所示， <img src="https://img-blog.csdnimg.cn/20200507102605513.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 该场景的结构引导时空轨迹的物理合理提取，最终的全局优化准则考虑了对象之间的交互作用，以细化三维定位和轨迹估计结果。Ess 等使用图形模型联合估计摄像机的位置、立体深度、物体检测以及所有物体随时间的姿势， 因此，图形模型代表了不同部分之间的相互作用，并结合了对象间的相互作用。</p><p><strong>Tracking-Before-Detection：</strong> 除了便于跟踪问题，深度还允许分割场景到不同的对象，独立于他们的类。在检测前跟踪中，这些分割的类不可知对象直接被视为跟踪公式中的观察值， 这样，跟踪系统独立于分类器，因此能够跟踪以前从未见过的未知对象或仅存在少量训练数据的未知对象。此外，来自对象估计轨迹的运动信息可以用作检测特定类别对象的另一种提示。 Mitzel 和 Leibe 通过使用立体图像的深度对场景进行分割来提取对象的观察结果。利用紧凑的 3D 表示，它们可以很稳地跟踪已知和未知的对象类别，这种表示还允许他们检测异常形状，如携带的物品。</p><h5 id="pedestrian-tracking">6.2.3 Pedestrian Tracking</h5><p>　　识别行人非常困难，特别是由于检测系统的误报。Andriluka 等人用联合检测和链式人体姿态跟踪公式解决了这个问题。他们将现有的人检测器扩展到基于肢体的结构模型，并使用层次高斯过程潜在变量模型 ( hGPLVM ) 对被检测肢体的动力学进行建模，这使得它们比只考虑一帧的方法更可靠。另一些人对这个想法扩展到了单目 3D 位姿估计上，在第一阶段，他们估计 2D 清晰度和人的视角并在少数帧中将它们关联，这些累积的 2D 图像证据再用 hGPLVM估计3D位姿。种方法使他们能够从单目图像中准确地估计多人的 3D 姿态，结合隐马尔可夫模型(HMM)，这些方法可以在很长的序列中跟踪人。</p><h5 id="joint-detection-and-tracking">6.2.4 Joint Detection and Tracking</h5><p>　　尽管典型的按检测跟踪方法假定可以进行检测，但 Dehghan 等人（2002年）提出了检测方法 和Tian 提出通过学习每个目标的模型并修改图以编码目标和节点之间的分配概率，来与网络流量方法共同解决检测和关联问题。</p><p>　　Kang等引入 tubelet 提案模块，结合对象物的检测和用于视频对象检测跟踪，tubelet 表示在连续的帧中探测到相同的物体。首先生成静态对象建议作为空间锚点(例如，从区域建议网络中生成)，然后预测调整锚的相对运动，从而提高性能。此外，视频片段生成的时空建议直接取代了每帧建议。唐等人不用传播边界框，而在同一帧中链接对象并在帧之间传播框分数。</p><p>　　另一个方向是用光流来聚合视频中的特征，根据估计的光流对附近帧的特征图进行扭曲，并通过学习自适应加权进行聚合。这样是为了提高对快速移动对象的检测，因为这些对象在一些帧里面容易造成模糊。朱等提出了一种更有效的关键帧选择方法，即选择要聚合的帧或帧的部分。王等人也在不同的层次上使用光流，即通过逐像素的扭曲在像素层次上使用光流，通过预测实例的移动在实例层次上使用光流，然后根据观察到的运动模式，将这两个层次结合起来，例如，在非刚体运动的情况下，更多地依靠像素层次。为了避免大量的光流计算，Bertasius 等人提出了一种基于可变形卷积层的时空采样机制。</p><h5 id="deep-learning-for-multi-object-tracking">6.2.5 Deep Learning for Multi-Object Tracking</h5><p>　　深度学习在目标检测方面的成功对于目标跟踪有很大的帮助，此外，深度学习已被用于表示的学习，以验证属于同一人的检测或比较接近的，用于使用顺序模型的学习轨迹表征。与传统的关联模型相结合的学习顺序模型显示，相比之前的工作性能有所提高。例如外观，运动和交互作用的 LSTM 网络的组合，与具有人工特征的Markov决策过程跟踪相比，一种改进的双线性 LSTM 与具有 CNN 特征的多假设跟踪模型形成对比，以及采用 RNN 的基于小波相似性的分层聚类方法，与采用 Siamese 网络的提升式多割方法相比。在这些例子中，通常是学习到一个较好的跟踪表示，然后使用建立的方法进行关联。</p><p>　　最近，提出了几种端到端的学习方法来完成多目标的跟踪，挑战主要是缺乏有标签的数据，输入和输出空间问题的结构化本质以及组合搜索空间。Schulter 等人提出了一种基于手工设计的边界区域表示的网络层来学习网络损失函数。Milan 等人第一个提出用端到端的方法来跟踪目标，如下图中，用RNN网络估计目标的状态，LSTMs 用来数据关联。 　　<img src="https://img-blog.csdnimg.cn/20200507231311337.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" width="70%"> 然而，上面这个模型是基于合成数据进行训练的，缺乏一个外观模型，这使得它无法与之前的方法的性能相匹配。于是 Frossard 和 Urtasun 提提出了一种用于检测和跟踪车辆的三维端到端学习方法，使用深度结构化损失通过线性程序反向传播，解决了关联问题。相反，Feichtenhofer 等提出了一种更通用的检测和跟踪端到端学习方法，该方法通过扩的其他方法得卷积目标检测器而具有跟踪损失，该损失使对象坐标跨帧回归。 但是，他们仅对ImageNet VID 进行评估，该数据集主要由在视频中心具有一个或几个对象的序列组成。</p><h4 id="datasets">6.3 Datasets</h4><p>　　早期的多目标跟踪的数据集包括独立的序列，例如PETS，TUD 以及 ETHZ，对这些序列的单独评估导致跟踪算法对其中一些序列过拟合，而对另一些序列表现较差。虽然有些序列( 如 PETS 和 TUD) 是从静态观察者捕获的，但其他与自动驾驶更相关的序列是从移动平台获取的。KITTI数据集提供了特定于自动驾驶的跟踪数据，并对车辆和行人类别进行了单独的评估。</p><p>　　MOT基准测试连续三年发布，包括 MOT15、MOT16 和 MOT17，由一系列带有跟踪标签的序列组成，并提供基于 CLEAR 度量的官方评估协议。最早的MOT15使用基于聚合通道特征(ACF)的经典对象检测器，在MOT16中，利用可变形部件模型 (Deformable Parts Model, DPM) 进行检测，而在MOT17中，利用 DPM、Faster R-CNN 和 Scale Dependent Pooling (SDP) 提供了三组不同的目标检测。提供检测集可以根据方法跟踪对象的能力进行比较，而与由不同检测器引起的错误无关。对于MOT16，表一中提供了使用公共（DPM）检测方法的排行榜，表二中显示了使用专用检测器的方法，对于MOT17，表三显示了同一组序列上三个检测器的平均结果。</p><p><strong>表一</strong> 　　<img src="https://img-blog.csdnimg.cn/20200508100543778.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><p><strong>表二</strong> 　　<img src="https://img-blog.csdnimg.cn/20200508100643921.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><p><strong>表三</strong> 　　<img src="https://img-blog.csdnimg.cn/20200508101435516.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><p>表中给出了多目标跟踪精度(MOTA)、识别检测的F1分数(IDF1)、主要跟踪轨迹(MT)和主要丢失轨迹(ML)的比率、ID开关(IDS)的数量和跟踪分段(FRAG)以及运行时间。</p><p>　　对于自动驾驶应用，KITTI 提供了两个基准，一个基准用于表四中的汽车( KITTI ) 汽车跟踪，另一个基准用于表五中的行人跟踪。 标有星号的方法使用 Regionlet 检测来独立比较跟踪性能。 汽车和行人面临的不同挑战允许分别关注每个类别，并深入研究特定于类别的问题。</p><p><strong>表四</strong> 　　<img src="https://img-blog.csdnimg.cn/20200508103410653.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><p><strong>表五</strong> 　　<img src="https://img-blog.csdnimg.cn/20200508103500379.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><h4 id="metrics">6.4 Metrics</h4><p>　　从上面的表中可以看到，用到的度量指标有：多目标跟踪精度 (MOTA)和多目标跟踪精度 (MOTP) 由引入，主要跟踪轨迹 (MT) 和主要丢失轨迹 (ML) 的比例，ID开关的数量 (IDS) 和跟踪分段 (FRAG) 。对于 MOT 的结果，给出的是 IDF1 而不是 MOTP，IDF1 分数为识别精度和召回率的 F1 分数，即正确识别的检测数与ground truth和计算出的检测数的平均数之比。跟踪轨迹和丢失轨迹分别显示了一个假设在至少80%或至多20%的时间内覆盖轨迹的百分比<font color="green">（这里不太理解）</font>。</p><h4 id="state-of-the-art-on-mot-kitti">6.5 State of the Art on MOT &amp; KITTI</h4><p><strong>MOT16 Benchmark：</strong> 经典的方法，如近在线多目标跟踪方法、多假设跟踪方法、基于马尔科夫决策过程的跟踪等，与新提出的方法相比，在 MOT 基准上仍然表现良好，而在第6.2.5中所说的有更好的外观模型的深度学习模型表现得更好。表三中的 MHT bLSTM 中提出的基于学习的方法在 ground truth 上训练，但由于噪声 DPM 检测影响了总体性能，因此在MOT17 (表三) 方面的表现不如 MHT DAM 。</p><p>　　单目标跟踪器 (SOT) 的成功引出了一些方法，这些方法通过学习每个对象的跟踪器来组合多个 MOT 单目标检测器。这些方法在高度可靠地检测到新对象时会初始化一个新的单对象跟踪器，他们根据运动模型限制搜索空间，并使用二进制分类器选择最佳的检测候选，从而将已知目标的检测分配给每个单一目标跟踪器。Chu等人提出了一种时空注意机制来处理目标之间的遮挡和相互作用引起的漂移，还提出了另一种方法，对对象模型内部和之间的感知进行编码，并提出了一种自适应模型更新策略来消除模型初始化中的噪声。</p><p>　　Ma等人提出的自定义跟踪器是利用所提供的检测在MOT16上发表最好的方法(表一中)，这种基于图的聚类公式虽然是离线的，并不直接适用于自动驾驶，但是在 MOT 上表现得非常好。与以前的方法相比，他们通过使用测试序列微调重新识别网络来学习序列特异性跟踪器，他们假设不重叠的轨迹代表不同的个体，从而在测试序列上采用通用的重新识别CNN。</p><p>　　比较 MOT16 上的公共和专用检测器(表一和表二)，可以看出好的对象检测器的重要性，专用的检测器性能要好一些。最近的目标检测器结合简单的跟踪算法，其性能明显优于任何具有公共检测的跟踪器，如基于 Hungarian 方法结合卡尔曼滤波的简单跟踪器 IOU 或 SORT。 Wojke 等人通过将深度特征合并到 pipeline 中进行外观匹配，进一步提高了SORT的性能，Yu等也采用了一种基于 Hungarian 算法和卡尔曼滤波的跟踪算法，结合深度特征进行外观匹配。然而，他们的探测器会根据额外的数据进行训练，包括一个自我收集的不公开的监控数据集。</p><p><strong>MOT17 Benchmark：</strong> 在 MOT17 上表现最好的方法（表三中）遵循图聚类方案，将小轨迹关联在一起，即较短的检测序列，可以轻松可靠地关联检测序列，而不是检测。Wang 等人首先在移动相机的情况下基于 IOU 和极外几何创建轨迹，轨迹表示图上的节点，然后根据基于贪婪搜索的聚类方法对节点进行聚类。但是这样的方法会有一个预处理生成轨迹，最近的一种方法称为 FAMNet ，它将特征提取、关联估计和分配问题结合在一个网络中，并且为了从失踪的检测中恢复过来，还将单目标跟踪融入到跟踪系统中。</p><p>　　最近，一些方法提出使用额外的线索，如头部检测和运动分割来改进跟踪，MOT17（表三中）表现好的两种方法将头部、身体和关节检测器融合到跟踪系统中。Keuper 等人提出的另一种方法是通过自顶向下的边界盒聚类和自底向上的点轨迹运动分割来处理多目标跟踪。</p><p><strong>KITTI Benchmark：</strong> 与 MOT 相比，KITTI 基准测试关注的是在交通场景中跟踪行人(表五)和汽车(表四)的挑战性场景。与MOT相似，经典方法的效果也相当不错，例如基于 Markov 决策过程（IMMDP）的目标跟踪，改进的最小损失网络流量 flow 或近在线多目标跟踪算法（NOMT）。在 IMMDP 中，学习策略采用强化学习，这相当于学习一个数据关联的相似函数。区域建议网络的改进是汽车跟踪任务中表现最好的方法。Lenz等人提出了一种基于计算和内存的最小损失网络流公式，这种方法取得了良好的准确性和精度，同时是对 KITTI 汽车跟踪中最快的方法。NOMT 提出了对相对运动模式进行编码的聚合局部流描述符 (ALFD)。正是由于这些特点，可以可靠地匹配远距离检测，并在 KITTI 汽车跟踪的表现上优于其他的方法。</p><p>　　最近的方法利用了特定域的信息例如汽车的动作或者场景的结构。Yoon等人通过构建一个网络来描述物体之间的相对运动，从而提取出相机的运动，他们进一步改进，利用了由两个对象之间的位置和速度差定义的结构运动约束，如图所示，正确的检测用红色和黄色的方框标出： 　　<img src="https://img-blog.csdnimg.cn/20200508154607627.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 这种结构的联合推理使他们能够缓解 2D 目标跟踪常见的问题 (例如遮挡) ，尤其是跟踪汽车。Frossard 和 Urtasun 提出学习基于 3D 检测的网络 flow 跟踪方法，采用结构化的铰链损耗通过整数规划进行反向传播。其他表现比较好的基于 3D 的方法中，其一使用新的 2D-3D卡尔曼滤波器耦合图像和世界空间估计，其二提出了 Poisson 多伯努利混合（PMBM）跟踪器。另外，Sharma等利用城市道路场景的几何形状来推断 3D 线索，以便基于对象的单视图重建来跟踪 3D 姿态和形状， 此方法在 KITTI 汽车跟踪中的准确性（MOTA）和准确性（MOTP）优于其他所有方法。</p><h4 id="discussion">6.6 Discussion　</h4><p>　　　可靠的跟踪检测只能通过使用非常精确的目标检测来实现。检测系统的影响可以对比 KITTI中有星号和没有星号的方法(表四，表五)。在 MOT16的 leader board中，当比较使用表一中的公共检测和表二中的专用对象检测器的方法的表时，可以观察到这一点。于对象检测一样，行人的跟踪具有挑战，因为行人的复杂运动很难预测，这与汽车的刚性运动相反，因为汽车运动会受道路区域约束并由于其较大的质量和动力学约束而遵循较少不稳定行为。3D 推理可以通过根据几何关系识别合理的解决方案来帮助改善跟踪性能，尤其是对于汽车。</p><p>　　　在交通场景中，检测器经常会对部分或完全遮挡的对象失效，在这些情况下，跟踪系统需要在之后的时间重新识别被跟踪的对象，但是由于光照条件的变化或与附近其他对象的相似性，这可能会很困难。这些问题会导致轨迹重新初始化，这在体现在 MOT 和 KITTI 基准测试中的FRAG 和 ID开关（IDS）值比较高。此外，大多数跟踪系统包含复杂的 pipeline，并且在文献中很少提出端到端的多目标跟踪算法。 以通用和端到端可训练模型为目标，缩小从检测到跟踪的差距，将是该领域未来研究的重要方向。</p><p>　　　总的说来，未来的发展方向，网络需具有的功能：端到端的训练网络，多目标跟踪，泛化能力强，减小从检测到跟踪时由于检测的失效而带来的对跟踪的影响。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>Autonomous Vehicles</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Autonomous Vehicles</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision for Autonomous Vehicles | Chapter 14 Scene Understanding</title>
    <link href="/2020/05/06/omputer-Vision-for-Autonomous-Vehicles-Chapter-14-Scene-Understanding/"/>
    <url>/2020/05/06/omputer-Vision-for-Autonomous-Vehicles-Chapter-14-Scene-Understanding/</url>
    
    <content type="html"><![CDATA[<h2 id="computer-vision-for-autonomous-vehicles-problems-datasets-and-state-of-the-art">《Computer Vision for Autonomous Vehicles Problems, Datasets and State of the Art》</h2><p>Author：Joel Janai, Fatma Guney, Aseem Behl, Andreas Geiger</p><p>Date：December 18, 2019</p><p><font color="navy"><strong>声明：</strong> 这是以上作者的关于自动驾驶的综述文章，写得非常好，里面涉及的方法也几乎是近几年比较流行比较好的。我这里只是阅读笔记，方便以后查阅，如有侵权会立即删除。</font> <br></p><p><font color="navy"><strong>Disclaimer:</strong> This is a review article by the above authors on autonomous driving. It is very well written, and the methods involved are almost popular and better in recent years. I just record the reading notes here for future reference, if there is any infringement, it will be deleted immediately.</font></p><h3 id="chapter-14-scene-understanding">Chapter 14 Scene Understanding</h3><h4 id="problem-definition">14.1 Problem Definition</h4><p>　　在室外场景理解中的任务一般有深度估计、场景分类、目标检测和跟踪，以及事件分类等，这些人物都是场景的一个特定的方面，所以为了对场景更全面的理解，同时获取到这些任务中的互补的场景线索（布局、交通参与者以及它们之间的关系）是有益的。3D 场景比 2D 场景更容易进行几何场景的理解，其结果可以得到 3D 物体模型、布局以及遮挡关系，所以这一章节关注自动驾驶任务中 3D 场景理解的技术，包括场景图估计和图像标注即道路存在可变性，并且模糊的视觉特征、遮挡，和挑战性的照明条件也是难点。</p><h4 id="methods">14.2 Methods</h4><p>　　场景理解的方法分为传统和现代的方法。早期的方法主要从不一样的视角来处理这个问题，大多数方法依赖于启发法而不是学习，并且不能推广到复杂的现实场景；现在的方法主要是从数据中学习复杂的关系。</p><h5 id="road-topology-and-traﬃc-participants">14.2.1 Road Topology and Traﬃc Participants</h5><p>　　现有的方法有：第一，用语义分割作为中间的表示来获取道路拓扑，并且检测十字路口和其他的交通对象，语义分割同时还对空间布局进行了编码；第二，用基于线性运动模型的时间滤波器检测和跟踪车辆，还估计相机的运动，并使用动态条件随机场（CRF）模型的对象和场景类的联合标签传播到下一帧，但是这个只用到了拓扑模型而没用到几何模型。</p><p>　　基于以上的不足，就有人提出利用几何模型的方法：首先是对上面第一个方法的改进，扩展为包含多类目标检测、目标跟踪、场景标记和几何关系推理的概率型三维场景模型；另一个团队推导了十字路口的三维场景布局以及车辆在场景中的位置和方向，他们提出了一个概率生成模型，通过利用车辆轨迹、语义标签、场景和占据网格来捕捉场景的拓扑、几何和交通活动。</p><p>　　除了基于 3D 原始的表示外，还有其他的方法来表示街景。比如 Topfer 等人提出了一种更细粒度的道路模型，将复杂的道路场景分层分解为道路、车道、道路边缘和车道标记，这样可以得到更有表现力的道路模型；Steff 和 Xiao 定义了道路布局属性列表，如车道数、可行驶方向、到十字路口的距离等。</p><h5 id="physical-and-temporal-relationships">14.2.2 Physical and Temporal Relationships</h5><p>　　场景理解系统旨在将目标检测和跟踪与物理约束相结合，并对交通参与者和场景之间的时间行为和关系建模。Pellegrini 等人采用多目标跟踪公式对行人(社会行为)与场景(碰撞)之间的交互作用进行了建模；Kuettel 等通过学习复杂动态场景中移动代理之间的协同活动和时间规则，对移动代理的时空依赖性进行建模，然而这两种方法都假设有一个静态观察者和一个很长的观察周期，即在作出决定之前，必须观察相当长的一段时间，因此它不适用于自治系统。</p><p>　　有人在上述缺陷上，考虑一个移动的车辆作为观察者，通过对遮挡和交通模式的推理来构建三维场景模型。</p><h4 id="discussion">14.3 Discussion</h4><p>　　场景理解从早期的工作到基于学习的更具表现力的理解方法，经历了从简单的 2D 表示道路拓扑和对象的方法，到更加复杂的包含物理和时间约束的 3D 模型。然而，自动驾驶所需的表达能力水平仍然是一个开放的问题，而通过最先进的场景理解模型实现的准确性仍然有限。此外，由于模型的复杂性和它们所处理的挑战不同，很难对场景理解方法进行统一的评估。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>Autonomous Vehicles</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Autonomous Vehicles</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算机视觉论文该怎么读？</title>
    <link href="/2020/05/04/%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%AE%BA%E6%96%87%E8%AF%A5%E6%80%8E%E4%B9%88%E8%AF%BB%EF%BC%9F/"/>
    <url>/2020/05/04/%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E8%AE%BA%E6%96%87%E8%AF%A5%E6%80%8E%E4%B9%88%E8%AF%BB%EF%BC%9F/</url>
    
    <content type="html"><![CDATA[<h4 id="后悔自己没早点看到的一片公众号文章"><font color="red">后悔自己没早点看到的一片公众号文章</font></h4><p>感谢【极市平台】公众号给我启发，献花~</p>####<center><p>计算机视觉的论文应该怎么读 　　阅读论文时，分门别类地去做笔记，以至于以后再接触到自己不熟悉的领域时不会从头去阅读，我们只需要去查询检索自己的论文阅读笔记就好。分门别类就是说，每当读到一篇文章，要知道属于哪个领域，例如目标检测领域：综述、人脸检测、目标检测、样本不平衡问题、one-stage检测等，然后每篇文章的阅读等级、解决的问题、创新点、可能存在的问题等都写下来，方便随时随地去查询这些知识点。可以说是用一根绳子把这一领域的给串起来，阅读的时候要学会纵向横向的去思考问题：</p><ul><li><strong>要点一</strong></li></ul><p>　　每篇论文都不会说自己的缺点，只会放大优点。但是引用别人的论文时，却总放大别人工作的缺点。当进行<font color="red">对比阅读</font>时，形成一个知识串，才会对某个问题有更清晰的认识。可以再看当前文章的时候，看看别人引用这篇文章的是怎么评价的。</p><ul><li><strong>要点二</strong></li></ul><p>　　论文为了出成果，一般只会选择对自己模型有力的数据集验证。<font color="red">对某一领域数据集特征了解，再也不会被作者蒙蔽双眼了。</font>比如 NAS ( Neural Architecture Search )，很多论文喜欢在CIFAR-10/ CIFAR-100/SVHN等小数据集比实验结果，ImageNet性能表现避重就轻避而不谈；很多论文写state-of-art的性能，对实时性不谈；论文没有说的没有做的可能是个大坑。所以要了解该问题经常用到的数据集的特征，然后根据文章的结果分析存在的缺陷，并且对比几篇文章，大家都给出了怎么样的一些指标，为什么这一篇给了，而另一篇不给呢。</p><ul><li><strong>要点三</strong></li></ul><p>　　论文因为要投稿和发表顶会，故意会云里雾里引入很多概念和公式，<font color="red">当对比代码，关键trick</font>，才能返璞归真。<font color="red">当Code+paper，才是论文最佳的阅读方式</font>。</p><ul><li><strong>要点四</strong></li></ul><p>　　对于自己关注的领域，可能每篇有影响的，实验结果不是state-of-art也要关注，因为工作可能会撞车。<font color="red">对横向领域的论文，要关注 state-of-art ，说不定很多trick可以直接迁移到自己的工作。</font></p><ul><li><strong>要点五</strong></li></ul><p>　　重点关注数著名实验室/老师/三大顶会（CVPR,ICCV,ECCV）的连续剧。2020年的CVPR投稿量都破万，各种水文鱼目混杂，实在是难以鉴别，推荐 paper+code 模式。敢于开源code的论文，真金不怕火炼，作者有底气。 没有code的论文，也许是商业或者其他授权暂时没有发布，但是发布了一两年还在遮遮掩掩，这些论文不看也罢。</p><ul><li><strong>要点六</strong></li></ul><p>　　<font color="red">最重要一点：拒绝二手知识。</font>阅读一篇论文, google 搜索题目可能有1000+篇的阅读笔记，阅读笔记的数量比论文的引用量都多；很多博客/笔记也喜欢摘抄，google 翻译+复制粘贴造就阅读笔记的虚假繁荣。有些问答还是具有参考意义，比如知乎中常见的“如何评价 Google Brain 团队最新检测论文 SpineNet ？”，在这些如何评价的思想碰撞中，还是有些很好的火花。不管是做科研学术工业界做项目，要摒弃完全重二手知识中学习，<font color="red">直接从原文阅读思考、和作者邮箱联系寻找答案。</font></p>####<center><p>接下来是阅读论文的技巧</p><p>　　S. Keshav 教授的《How to Read a Paper 》文章中的过三关的方法来阅读一篇文章。他的关键思想是，在读一篇论文时将过程分成三个关卡，而不是从论文第一个单词开始然后一直读到结尾。每一关都会在上一关的基础上，完成一个特定的目标：第一关会让你了解论文的大体想法。第二关让你抓住论文的内容， 但不包括细节。第三关帮助你深入理解论文。 ###### 第一关：</p><p>第一关是快速浏览，获得一个全览鸟瞰图，你同时可以决定是否继续读下去。这一步应该花费 5-10 分钟，包括以下几个步骤：</p><p>1、仔细阅读论文题目，摘要和介绍</p><p>2、阅读各个大节和小节的标题，但忽略其他所有内容</p><p>3、浏览数学内容(如果有的话), 判断论文背后的理论基础</p><p>4、阅读结论</p><p>5、浏览参考文献，在你已经读过的文献上做个标记</p><p>经过第一关之后，应该可以回答以下的5个问题：</p><p>1、类别 (Category): 这篇论文是什么类型？是测量？是分析已有系统？是描述一种研究原型？</p><p>2、背景 (Context): 与它相关的其他论文有哪些？分析问题时使用了哪些理论框架？</p><p>3、正确性 (Correctness): 假设看起来有效吗？</p><p>4、贡献 (Contributions): 这篇论文主要的贡献是什么？</p><p>5、清晰 (Clarity): 这篇论文本身的写作语言清楚吗？</p><h6 id="第二关">第二关：</h6><p>在第二关，你需要更加仔细地读论文，但是忽略那些证明细节。这样可以帮助你记住重点，或者在边上写下批注。</p><p>1、仔细阅读论文中的图表或其他展示元素，尤其要注意统计图。坐标轴的标签正确吗？误差线会让结论在统计学意义上成立，而图里有误差线吗？这样的一些小错误，会把匆忙完成的劣质论文与真正优秀的论文区分开来。</p><p>2、标记那些相关但没读过的参考文献，以备将来阅读 (这是了解这篇论文背景资料的一个好方法)。</p><p>对于一个有经验的读者，第二关应该会花费一个小时。经过这一关，应该可以掌握论文的内容，能够向其他人总结文中的主要推理过程，并且证据充分。这对一篇你感兴趣但并非自己研究领域的论文来说，已经够了。</p><h6 id="第三关">第三关：</h6><p>第三关的核心是用虚拟的方法，<font color="red">自己重新实现一遍这篇论文</font>。也就是说，与作者作出相同的假设，重新创造出这篇论文。通过比较重造出来的结果和原来的论文，你不仅会很容易看到论文中的创新之处，而且也会看到背后隐藏的失败和假设。</p></center></center>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>paper reading tricks</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper reading技巧</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第十四章</title>
    <link href="/2020/05/03/%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0/"/>
    <url>/2020/05/03/%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第十四章-slam现在与未来">第十四章 SLAM：现在与未来</h3><p>记录一小部分：</p><h4 id="未来的slam话题">14.2 未来的SLAM话题</h4><p>　　大体来说，SLAM 将来的发展趋势一共有两个大类：一是往轻量级、小型化方向发展，让 SLAM 能够在嵌入式或手机等小型设备上良好的运行，然后考虑以它为底层功能的应用。毕竟大部分场合中，真正目的都是实现机器人、AR/VR 设备的功能，比如说运动、导航、教学、娱乐， 而 SLAM 是为上层应用提供自身的一个位姿估计。在这些应用中，不希望 SLAM 占 据所有计算资源，所以对 SLAM 的小型化和轻量化有非常强烈的要求。另一个方面，则是利用高性能计算设备，实现精密的三维重建、场景理解等功能。在这些应用中，我们的目的是完美地重建场景，而对于计算资源和设备的便携性则没有多大限制。由于可以利用 GPU，这个方向和深度学习亦有结合点。</p><h5 id="视觉-惯导-slam">14.2.1 视觉 + 惯导 SLAM</h5><p>　　实际的机器人也好，硬件设备也好，通常都不会只携带一种传感器，往往是多种传感器的融合。惯性传感器（IMU）能够测量传感器本体的角速度和加速度，被认为与相机传感器具有明显的互补性，而且十分有潜力在融合之后得到更完善的 SLAM 系统，原因如下：</p><ul><li><p>IMU 虽然可以测得角速度和加速度，但这些量都存在明显的漂移（Drift），使得积分两次得到的位姿数据非常不可靠。好比说，我们将 IMU 放在桌上不动，用它的读数 积分得到的位姿也会漂出十万八千里。但是，对于短时间内的快速运动，IMU 能够提供一些较好的估计，这正是相机的弱点。当运动过快时，（卷帘快门的）相机会出现运动模糊，或者两帧之间重叠区域太少以至于无法进行特征匹配，所以纯视觉 SLAM 非常害怕快速的运动。而有了 IMU，即使在相机数据无效的那段时间内，我们还能保持一个较好的位姿估计，这是纯视觉 SLAM 无法做到的。</p></li><li><p>相比于 IMU，相机数据基本不会有漂移，如果相机放在原地固定不动，那么（在静态场景下）视觉 SLAM 的位姿估计也是固定不动的。所以，相机数据可以有效地估计并修正 IMU 读数中的漂移，使得在慢速运动后的位姿估计依然有效。</p></li><li><p>当图像发生变化时，本质上我们没法知道是相机自身发生了运动，还是外界条件发生了变化，所以纯视觉 SLAM 难以处理动态的障碍物。而 IMU 能够感受到自己的运动信息，从某种程度上减轻动态物体的影响。</p></li></ul><p>　　但是不管是理论还是实践，VIO（Visual Inertial Odometry）都是相当复杂的，其复杂性主要来源于 IMU 测量加速度和角速度这两个量的事实，所以不得不引入运动学计算。目前 VIO 的框架已经定型为两大类：松耦合（Loosely Coupled）和 紧耦合（Tightly Coupled）。松耦合是指，IMU 和相机分别进行自身的运动估计，然后对它们的位姿估计结果进行融合。紧耦合是指，把 IMU 的状态与相机的状态合并在一起，共同构建运动方程和观测方程，然后进行状态估计，可以预见到，<font color="red">紧耦合理论也必将分为基于滤波和基于优化的两个方向</font>。在滤波方面，传统的 EKF 以及改进的 MSCKF（Multi-State Constraint KF）都取得了一定的成果，研究者们对 EKF 也进行了深入的讨论（例如能观性）；优化方面亦有相应的方案。尽管在纯视觉 SLAM 中，优化方法已经占了主流， 但在 VIO 中，由于 IMU 的数据频率非常高，对状态进行优化需要的计算量就更大，<font color="red">因此 VIO 领域目前仍处于滤波与优化并存的阶段</font>。</p><p>　　<font color="red">VIO 为将来 SLAM 的小型化与低成本化提供了一个非常有效的方向</font>。而且结合稀疏直接法，有望在低端硬件上取得良好的 SLAM 或 VO 效果，是非常有前景的。</p><h5 id="语义-slam">14.2.2 语义 SLAM</h5><p>　　<font color="red">SLAM 另一个大方向就是和深度学习技术进行结合</font>。到目前为止，SLAM 的方案都处于特征点或者像素的层级。这使得计算机视觉中的 SLAM 与我们人类的做法很不相似，至少我们自己从来看不到特征点，也不会去根据特征点判断自身的运动方向。我们看到的是一个个物体，通过左右眼判断它们的远近，然后基于它们在图像当中的运动，推测相机的移动。</p><p>　　语义SLAM工作大概有：将物体信息结合到 SLAM 中。例如把物体识别与视觉 SLAM 结合起来，构建带物体标签的地图；或是把标签信息引 入到 BA 或优化端的目标函数和约束中，结合特征点的位置与标签信息，进行优化。综合来说，SLAM 和语义的结合点主要有两个方面：</p><ul><li><p><strong>语义帮助 SLAM。</strong> 传统的物体识别、分割算法往往只考虑一个图，而在 SLAM 中我们拥有一台移动的相机。如果我们把运动过程中的图片都带上物体标签，就能得到一个带有标签的地图，另外，物体信息亦可为回环检测、BA 优化带来更多的条件。</p></li><li><p><strong>SLAM 帮助语义。</strong> 物体识别和分割都需要大量的训练数据。要让分类器识别各个角度的物体，需要从不同视角采集该物体的数据，然后进行人工标定。而 SLAM 中，由于我们可以估计相机的运动，可以自动地计算物体在图像中的位置，节省人工标志的成本。如果有自动生成的带高质量标注的样本数据，能够很大程度上加速分类器的训练过程。</p></li></ul><p>　　随着深度学习的发展，我们开始使用网络，越来越准确地对图像进 行识别、检测和分割。这为<font color="red">构建准确的语义地图</font>打下了更好的基础 [150]。目前逐渐开始有学者将神经网络方法引入到 SLAM 中的物体识别和分割，甚至 SLAM 本身的位姿估计与回环检测中。虽然这些方法目前还没有成为主流，但将 SLAM 与深度学习结合来处理图像，亦是一个很有前景的研究方向。</p><h5 id="slam-的未来">14.2.3 SLAM 的未来</h5><p>　　除了这两个大方向之外，基于线/面特征的 SLAM、动态场景下的 SLAM、多机器人的 SLAM等等，都是研究者们感兴趣并发力的地方。</p><center><p>相信 SLAM 会越来越成熟，但永远不老~</p></center>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SlAM十四讲 第十三章</title>
    <link href="/2020/05/03/%E8%A7%89SlAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0/"/>
    <url>/2020/05/03/%E8%A7%89SlAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第十三讲-建图">第十三讲 建图</h3>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第十二章</title>
    <link href="/2020/05/03/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0/"/>
    <url>/2020/05/03/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第十二章-回环检测">第十二章 回环检测</h3>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第十一章</title>
    <link href="/2020/05/03/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0/"/>
    <url>/2020/05/03/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者：高翔 | 张涛</p><h3 id="第十一章-后端2">第十一章 后端2</h3>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>不只生活</title>
    <link href="/2020/05/03/%E7%94%9F%E6%B4%BB/"/>
    <url>/2020/05/03/%E7%94%9F%E6%B4%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="section">- 2020.05.03</h1><p>FIND：</p><ul><li><p>翼を持っているのに、羽ばたかずに一人でいるのは、怖くて辛いだけだ。でも私は自分と闘って、自分でも見たことのない自分に戻って頑張りましょう。 <img src="https://img-blog.csdnimg.cn/20200503183515952.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="sky"></p></li><li><p>最近好きな歌:音楽は私を解放します。音楽が私を解放させたわけではありませんが、私は自分が何を望んでいるのか、早くはっきり見えます。 <img src="https://img-blog.csdnimg.cn/20200503183417302.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="音楽は私を解放します"></p></li></ul>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
      <category>释放</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第十章</title>
    <link href="/2020/05/03/%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%20%E7%AC%AC%E5%8D%81%E7%AB%A0/"/>
    <url>/2020/05/03/%E8%A7%86%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%20%E7%AC%AC%E5%8D%81%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第九章</title>
    <link href="/2020/05/02/%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B9%9D%E7%AB%A0/"/>
    <url>/2020/05/02/%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B9%9D%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第九章-实践章设计前端">第九章 实践章：设计前端</h3><p><strong>本章目标：</strong></p><ul><li><p>设计一个视觉里程计前端。</p></li><li><p>理解SLAM软件框架搭建过程。</p></li><li><p>理解在前端设计中容易出现的问题以及解决办法。</p></li></ul><h4 id="搭建-vo-框架">9.1 搭建 VO 框架</h4>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第八章</title>
    <link href="/2020/05/01/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%85%AB%E7%AB%A0/"/>
    <url>/2020/05/01/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%85%AB%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者：高翔 | 张涛</p><h3 id="第八章-视觉里程计2">第八章 视觉里程计2</h3><h4 id="直接法的引出">8.1 直接法的引出</h4><p>特征点法的缺点：</p><ul><li><p>关键点的提取与描述子的计算非常耗时。</p></li><li><p>使用特征点时，忽略了除特征点以外的所有信息,一张图像有几十万个像素，而特征点只有几百个。</p></li><li><p>相机有时会运动到特征缺失的地方，往往这些地方没有明显的纹理信息 (例如有时会面对一堵白墙，这场景下特征点数量会明显减少， 可能找不到足够的匹配点来计算相机运动) 。</p></li></ul><p>　　针对以上问题，有三种思路：第一是保留特征点，但只计算关键点，不计算描述子，同时，使用光流法（Optical Flow）来跟踪特征点的运动；第二是只计算关键点，不计算描述子，同时使用直接法（Direct Method）来计算特征点在下一时刻图像的位置，这同样可以跳过描述子的计算过程，而且直接法的计算更加简单；第三种既不计算关键点、也不计算描述子，而是根据像素灰度的差异，直接计算相机运动。第一种方法仍然使用特征点，只是把匹配描述子替换成了光流跟踪，估计相机运动时仍使用对极几何、PnP 或 ICP 算法。而在后两个方法中，我们会根据图像的像素灰度信息来计算相机运动，它们都称为<font color="maroon">直接法</font>。直接法根据像素的亮度信息，估计相机的运动，可以完全不用计算关键点和描述子，于是既避免了特征的计算时间，也避免了特征缺失的情况。只要场景中存在明暗变化（可以是渐变，不形成局部的图像梯度），直接法就能工作。根据使用像素的数量，直接法分为稀疏、稠密和半稠密三种，相比于特征点法只能重构稀疏特征点（稀疏地图），直接法还具有恢复稠密或半稠密结构的能力。</p><h4 id="光流-optical-flow">8.2 光流 (Optical Flow)</h4><p>　　直接法是从光流演变而来的， 光流描述了像素在图像中的运动，而直接法则附带着一个相机运动模型。计算部分像素运动的 称为稀疏光流，计算所有像素的称为稠密光流，稀疏光流以 Lucas-Kanade 光流为代表，并 可以在 SLAM 中用于跟踪特征点位置。下图为LK光流法示意图： <img src="https://img-blog.csdnimg.cn/20200501231933270.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><h5 id="lucas-kanade-光流">8.2.1 Lucas-Kanade 光流</h5><p>在 LK 光流中，图像可以看作时间的函数：<span class="math inline">\(I(t)\)</span>，那么一个在 t 时刻，位于<span class="math inline">\((x,y)\)</span>处的像素，它的灰度可以写成<span class="math inline">\(I(x,y,t)\)</span>。要估计图像坐标，先引入光流法的假设： * 灰度不变假设：同一个空间点的像素灰度值，在各个图像中是固定不变的。（实际中这一假设可能不成立） * 在 LK 光流中，我们假设某一个窗口内的像素具有相同的运动。 　　对于t 时刻位于<span class="math inline">\((x,y)\)</span>处的像素，我们设<span class="math inline">\(t +d_t\)</span>时刻，它运动到<span class="math inline">\((x +d_x,y +d_y)\)</span>处。 由于灰度不变，有: <span class="math display">\[I(x+d_x,y+d_y,t+d_t)=I(x,y,t)\]</span> 左边进行泰勒展开保留一阶项，并由灰度不变得：<span class="math display">\[\frac{∂I}{∂x}dx + \frac{∂I}{∂y}dy + \frac{∂I}{∂t}dt = 0\]</span> 两边同除以<span class="math inline">\(dt\)</span>得：<span class="math display">\[\frac{∂I}{∂x}\frac{dx}{dt} + \frac{∂I}{∂y}\frac{dy}{dt} =- \frac{∂I}{∂t}\]</span> 其中 <span class="math inline">\(dx/dt\)</span> 为像素在 x 轴上运动速度，而 <span class="math inline">\(dy/dt\)</span> 为 y 轴速度，把它们记为<span class="math inline">\(u,v\)</span>，<span class="math inline">\(∂I/∂x\)</span>为图像在该点处 x 方向的梯度，另一项则是在 y 方向的梯度，记为<span class="math inline">\(Ix,Iy\)</span>。上式写为矩阵形式有：<span class="math display">\[ \begin{bmatrix} I_x &amp; I_y  \\ \end{bmatrix} \begin{bmatrix} u \\ v \\ \end{bmatrix}=-I_t\]</span> 其中<span class="math inline">\(u,v\)</span>就是想要计算的运动，考虑在一个<span class="math inline">\(w×w\)</span>得窗口中，像素量为 <span class="math inline">\(w^2\)</span> 。由于该窗口内像素具有同样的运动，所以共有 <span class="math inline">\(w^2\)</span> 个方程： <span class="math display">\[ \begin{bmatrix} I_x &amp; I_y  \\ \end{bmatrix}_k \begin{bmatrix} u \\ v \\ \end{bmatrix}=-I_{tk}, k=1,...,w^2\]</span> 记 <span class="math display">\[A= \begin{bmatrix}  \begin{bmatrix} I_x , I_y  \\ \end{bmatrix}_1 \\ \vdots \\  \begin{bmatrix} I_x , I_y  \\ \end{bmatrix}_k  \\ \end{bmatrix} , b= \begin{bmatrix} I_{t1} \\ \vdots \\ I_{tk} \\ \end{bmatrix}\]</span><br>于是整个方程为： <span class="math display">\[ A\begin{bmatrix} u\\ v  \\ \end{bmatrix}=-b\]</span> 对以上方程用最小二乘解： <span class="math display">\[\begin{bmatrix} u\\ v  \\ \end{bmatrix}^*=-(A^TA)^{-1}A^Tb\]</span> 由于像素梯度仅在局部有效，所以如果一次迭代不够好的话，会多迭代几次这个方程，在 SLAM 中，LK 光流常被用来跟踪角点的运动。</p><h4 id="实践光流">8.3 实践：光流</h4><h5 id="使用-tum-公开数据集">8.3.1 使用 TUM 公开数据集</h5><p>数据集来自于慕尼黑工业大学（TUM）提供的公开 RGB-D 数据集，它含有许 多个 RGB-D 视频，可以作为 RGB-D 或单目 SLAM 的实验数据。它还提供了用运动捕捉系统测量的精确轨迹，可以作为标准轨迹以校准 SLAM 系统。</p><h5 id="使用-lk-光流">8.3.2 使用 LK 光流</h5><p>编写程序使用 OpenCV 中的 LK 光流，使用 LK 的目的是跟踪特征点，对第一张图像提取 FAST 角点，然后用 LK 光流跟踪它们。</p><p>　　 #### 8.4 直接法（Direct Methods）</p><h5 id="直接法推导">8.4.1 直接法推导</h5><p>涉及李导数的扰动模型，待续...</p><h5 id="直接法的讨论">8.4.2 直接法的讨论</h5><p>　　在上面的推导中，P 是一个已知位置的空间点，在 RGB-D 相 机下，可以把任意像素反投影到三维空间，然后投影到下一个图像中。如果在单目相机中，这件事情要更为困难，因为还需考虑由 P 的深度带来的不确定性。现在先来考 P 深度已知的情况，根据 P 的来源，可以把直接法进行分类：</p><ul><li><p>P 来自于稀疏关键点，我们称之为<font color="maroon">稀疏直接法</font>。通常我们使用数百个至上千个关键 点，并且像 L-K 光流那样，假设它周围像素也是不变的。这种稀疏直接法不必计算描述子，并且只使用数百个像素，因此速度最快，但<font color="maroon">只能计算稀疏的重构</font>。</p></li><li><p>P 来自部分像素。考虑只使用带有梯度的像素点，舍弃像素梯度不明显的地方（这些地方的雅可比为零，不会对计算运动增量有任何贡献）。这称之为半稠密（Semi-Dense）的直接法，可以<font color="maroon">重构一个半稠密结构</font>。</p></li><li><p>P 为所有像素，称为稠密直接法。稠密重构需要计算所有像素，因此需要 GPU 的加速，但是度不明显的点，在运动估计中不会有太大贡献，在重构时也会难以估计位置。</p></li></ul><p>　　可以看到，从稀疏到稠密重构，都可以用直接法来计算。 稀疏方法可以快速地求解相机位姿，而稠密方法可以建立完整地图。在低端的计算平台上，稀疏直接法可以做到非常快速的效果，适用于实时性较高且计算资源有限的场合 。</p><h4 id="实践rgb-d-的直接法">8.5 实践：RGB-D 的直接法</h4><p>待续...</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Computer Vision for Autonomous Vehicles | Chapter 1 Introduction</title>
    <link href="/2020/04/30/mputer-Vision-for-Autonomous-Vehicles-Chapter-1-Introduction/"/>
    <url>/2020/04/30/mputer-Vision-for-Autonomous-Vehicles-Chapter-1-Introduction/</url>
    
    <content type="html"><![CDATA[<h2 id="computer-vision-for-autonomous-vehicles-problems-datasets-and-state-of-the-art">《Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art》</h2><p>Author：Joel Janai | Fatma Gu ney | Aseem Behl | Andreas Geiger Date：December 18, 2019</p><p><font color="navy"><strong>声明：</strong> 这是以上作者的关于自动驾驶的综述文章，写得非常好，里面涉及的方法也几乎是近几年比较流行比较好的。我这里只是阅读笔记，方便以后查阅，如有侵权会立即删除。</font> <br></p><p><font color="navy"><strong>Disclaimer:</strong> This is a review article by the above authors on autonomous driving. It is very well written, and the methods involved are almost popular and better in recent years. I just record the reading notes here for future reference, if there is any infringement, it will be deleted immediately.</font></p><h3 id="abstract">Abstract</h3><p>　　现在还没有关于 CV 的自动驾驶中问题、数据集和方法的综合性研究，这篇综述给出了几乎是最新的数据集和方法。文章不仅呈现以前的文献资料，还有目前的几个 topics，涉及 识别、重建、运动估计、跟踪、场景理解以及自动驾驶端到端的学习。书中分析了当前状态在几个具有挑战性的基准数据集上的表现，包括 KITTI、MOT和 Cityscapes。此外，还讨论了开放的问题和当前的研究挑战。</p><h3 id="chapter-1-introduction">Chapter 1 Introduction</h3><p>人们想要实现的全自动导航到现在还没有实现，原因归结为以下两个方面：</p><p>1、首先，在复杂的动态环境中运行的自治系统需要模型，这些模型能够及时地归纳出不可预测的情况和原因。</p><p>2、其次，模型明智的决策需要准确的感知，然而大多数现有的计算机视觉模型仍然不如人类的感知和推理。</p><p>　　有的自动驾驶方法大致可以分为模块化 pipeline 和单块 end-to-end 的学习方法。两种方法大致的流程对比如下图所示： 　　<img src="https://img-blog.csdnimg.cn/2020043008241786.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" width="80%"> 模块化 pipeline 是自动驾驶中的标准方法，在行业中经常用到，主要思想就是将高维输入到低维控制变量的复杂映射函数分解成可独立开发、训练和测试的模块。现有的方法通常利用机器学习(例如深度神经网络)来提取 feature 或将 scene 解析为单独的组件，路径规划和车辆控制则由经典的状态机、搜索算法和控制模型主导。这种方法的主要<font color="blue">优点</font>是，首先它们的中间 representations 可由人来解释，如检测到的对象或自由空间信息，从而能够洞察系统的故障模式，其次它的开发可以很容易地在公司内部并行化，而且可以很容易的将基本原理和先验知识集合到系统中。其他人类难以手工处理的方面，例如行人的外观检测，可以从大型带注释的数据集中学习。他的主要<font color="blue">缺陷</font>在于，一是人类设计的中间的 representations 并不一定就是最好的，二是模块都是利用辅助损失单独进行训练验证的（物体检测的例子）。</p><p>　　为解决以上问题，提出了端到端的学习策略，就是用一种模块（深度网络）完成从观察到行动的函数。但是缺点是整体的网络不易解释，类似一个“黑盒”，不能准确说明哪里出了问题。</p><p>　　综述注重讲自动驾驶中的感知，一些文章也讨论了关于自动驾驶的一些问题，但没有关于各种视觉任务的先进技术深入的综述，所以这本书的目标是通过提供一些的概述和比较，包括环境感知的工作，来缩短机器人、智能车辆和计算机视觉之间的差距。这篇综述提供了一个<a href="http://www.cvlibs.net/projects/autonomous_vision_survey" target="_blank" rel="noopener">在线交互工具</a>，它可以方便人们访问相关的主题，以及提供额外的信息。综述首先提供了自动驾驶的简要历史，然后介绍了相机模型和校准技术。接着，综述介绍了与自动驾驶相关的数据集（重点关注与感知相关的数据集）、相关的感知任务和最新的解决方法。尤其是，综述回顾了目标检测、目标跟踪、语义（实体）分割、重建、运动估计和场景理解。每个章节包含了问题定义、重要方法和主要设计选择、顶尖技术在流行数据集上的定性和定量分析以及关于领域最先进技术的讨论。最终，综述提供了关于最先进端到端自动驾驶模型的概览。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>Autonomous Vehicles</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Autonomous Vehicles</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第七章</title>
    <link href="/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B8%83%E7%AB%A0/"/>
    <url>/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B8%83%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第七章-视觉里程计-1">第七章 视觉里程计 1</h3><p><strong>主要内容</strong>：本讲关注基于特征点方式的视觉里程计算法，将介绍什么是特征点，如何提取和匹配特征点，以及如何根据配对的特征点估计相机运动。</p><h4 id="特征点法">7.1 特征点法</h4><p>　　VO 的实现方法，按是否需要提取特征，分为特征点法的前端以及不提特征的直接法前端。</p><h5 id="特征点">7.1.1 特征点</h5><p>　　VO 的主要问题是如何根据图像来估计相机运动，对于图像的处理，我们习惯于这样的做法：首先，从图像中选取比较<strong>有代表性的点</strong>。这些点在相机视角发生少量变化后会保持不变，所以我们会在各个图像中找到相同的点。然后，在这些点的基础上，讨论相机位姿估计问题，以及这些点的定位问题。在经典 SLAM 模型中，把它们称为路标， 而在视觉 SLAM 中，路标则是指图像特征（Features）。 图像的特征点是一些比较特别的地方，如下图所示的角点、边缘和区块： 　　<img src="https://img-blog.csdnimg.cn/20200430104056565.png?ype_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 一种比较直观的提取特征的方式就是在不同图像间辨认角点，确定它们的对应关系，就是所谓的特征，因为角点相比边缘和像素块区更加“特别”，它们不同图像之间的辨识度更强。但是角点的外观会发生变化（例如当相机由远及近、旋转时观察到的会变化），所以 CV 研究者就设计出了很多稳定的局部图像特征（SIFT、SURF、ORB等），这些人工设计的特征点具有：</p><p>1、可重复性：相同的“区域”可以在不同的图像中被找到。</p><p>2、可区别性：不同的“区域”有不同的表达。</p><p>3、高效率：：同一图像中，特征点的数量应远小于像素的数量。</p><p>4、本地性：特征仅与一小片图像区域相关。</p><p>　　特征点由关键点（Key-point）和描述子（Descriptor）两部分组成：关键点 是指<font color="maroon">该特征点在图像里的位置</font>，有些特征点还具有朝向、大小等信息；描述子通常是一个向量，按照某种人为设计的方式，描述了<font color="maroon">该关键点周围像素的信息</font>。描述子是按照“外观相似的特征应该有相似的描述子”的原则设计的。因此，只要两个特征点的描述子在向量空间上的距离相近，就可以认为它们是同样的特征点。其中 ORB 在保持了特征子具有旋转，尺度不变性的同时，计算速度方面提升明显，对于实时性要求很高的 SLAM 来说是一个很好的选择。</p><h5 id="orb-特征">7.1.2 ORB 特征</h5><p>先看一下ORB特征点检测结果： <img src="https://img-blog.csdnimg.cn/20200430112546517.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 　　ORB 特征由关键点和描述子两部分组成，关键点称为“Oriented FAST”，是 一种改进的 FAST 角点，它的描述子称为 BRIEF （Binary Robust Independent Elementary Features）。因此，提取 ORB 特征分为两个步骤：</p><p>1、FAST 角点提取：找出图像中的” 角点”。ORB 中计算了特征点的主方向，为后续的 BRIEF 描述子增加了旋转不变特性。</p><p>2、BRIEF 描述子：对前一步提取出特征点的周围图像区域进行描述（例如关键点附近两个像素（比如说 p 和 q）的大小关系：如果 p 比 q 大，则取 1，反之就取 0）。</p><p>关于 FAST 和 BRIEF 的介绍略。</p><h5 id="特征匹配">7.1.3 特征匹配</h5><p>特征匹配是视觉 SLAM 中极为关键的一步，解决了 SLAM 中的数据关联问题（data association），即确定当前看到的路标与之前看到的路标之间的对应关系。由于图像特征的局部特性，误匹配的情况广泛存在，目前已经成为视觉 SLAM 中制约性能提升的一大瓶颈。最简单的匹配方法就是<font color="maroon">暴力匹配</font>：计算一幅图中的每一个特征点与另一幅图中所有特征点的距离（距离表示特征之间的相似程度，对于浮点型描述子常用欧氏距离，对于二进制描述子常用汉明距离），最近的为匹配点。由于暴力匹配法的运算量很大，特别是当我们想要匹 配一个帧和一张地图的时候，这不符合我们在 SLAM 中的实时性需求。此时<font color="maroon">快速近似最近邻（FLANN）算法</font>更加适合于匹配点数量极多的情况。</p><h4 id="实践特征提取和匹配">7.2 实践：特征提取和匹配</h4><p>目前主流的几种图像特征在 OpenCV 开源图像库中都已经集成完毕，用的时候可以很方便地进行调用。然后希望根据匹配的点对，估计相机的运动。这里由于相机的原理不同，有以下几种情况：</p><p>1、当相机为单目时只知道 2D 的像素坐标，因而问题是根据两组 2D 点估计运 动。该问题用<font color="maroon">对极几何</font>来解决。</p><p>2、当相机为双目、RGB-D 时，或者我们通过某种方法得到了距离信息 (depth information)，那问题就是根据两组 3D 点估计运动。该问题通常用 ICP 来解决。</p><p>3、如果有 3D 点和它们在相机的投影位置，也能估计相机的运动。该问题通过 PnP 求解。</p><h4 id="d-2d对极几何">7.3 2D-2D：对极几何</h4><h5 id="对极约束">7.3.1 对极约束</h5><p>如下图匹配好的两幅图的特征点： <img src="https://img-blog.csdnimg.cn/20200430134931383.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 　　如果有很多这样的匹配点，就可以通过这些二维图像点的对应关系，恢复出在两帧之间摄像机的运动。设第一帧到第二帧的运动为<span class="math inline">\(R,t\)</span>，两个相机的<font color="teal">中心</font>分别为<span class="math inline">\(O_1,O_2\)</span>。现在，考虑<span class="math inline">\(I_1\)</span>中有一个特征点<span class="math inline">\(p_1\)</span>，它在<span class="math inline">\(I_2\)</span>中对应着特征点<span class="math inline">\(p_2\)</span>，如果匹配正确，说明它们确实是同<font color="teal">一个空间点在两个成像平面上的投影</font>。图中<span class="math inline">\(\overrightarrow{O_1p_1}\)</span>和<span class="math inline">\(\overrightarrow{O_2p_2}\)</span>在三维空间中会相交于点 P，<span class="math inline">\(O_1,O_2,P\)</span>就确定一个<font color="teal">极平面（Epipolar plane）</font>，<span class="math inline">\(O_1,O_2\)</span>连线 <font color="teal">(基线 Baseline )</font> 与像平面<span class="math inline">\(I_1,I_2\)</span>交于<font color="teal">极点 (Epipoles)</font> <span class="math inline">\(e_1,e_2\)</span>。则图中两个平面的交线<span class="math inline">\(l_1,l_2\)</span>为<font color="teal">极线（Epipolar line）</font>。</p><p>　 　从第一帧的角度看，射线<span class="math inline">\(\overrightarrow{O_1p_1}\)</span>是某个像素可能出现的空间位置，因为该射线上的所有点都会投影到同一个像素点。同时在匹配正确的情况下，如果不知道 P 的位置，那么在第二个图像上看时，第二个图像中的极线就是 P 可能出现的投影的位置，也就是<span class="math inline">\(\overrightarrow{O_1p_1}\)</span>在第二个相机中的投影。现在，由于特征点匹配确定了<span class="math inline">\(p_2\)</span>的像素位置，所以能够推断 P 的空间位置，以及相机的运动。下面从几何关系来理解：</p><p>　 　两个像素点<span class="math inline">\(p_1,p_2\)</span>的像素位置为<span class="math display">\[s_1p_1=KP\]</span> <span class="math display">\[s_2p_2=K(RP+t)\]</span> 其中 P 为空间点，K为相机参数，R 和 t 为相机的旋转平移，齐次坐标下 表示为 <span class="math display">\[p_1=KP\]</span> <span class="math display">\[p_2=K(RP+t)\]</span> 取两个像素点的归一化平面上的坐标 <span class="math display">\[x_1=K^{-1}p_1 \]</span> <span class="math display">\[x_2=K^{-1}p_2\]</span> 则得到 <span class="math display">\[x_2=Rx_1+t\]</span> 两侧同时与 t 做外积得 <span class="math display">\[t∧x_2=t \hat{ }Rx_1\]</span> 再同时左乘<span class="math inline">\(x_2^T\)</span>得 <span class="math display">\[x_2^Tt∧x_2=x_2^Tt∧Rx_1\]</span> 上式可以简化为 <span class="math display">\[x_2^Tt∧Rx_1=0\]</span> 重新代入<span class="math inline">\(p_1,p_2\)</span>，有 <span class="math display">\[p_2^TK^{-T}t∧RK^{-1}p_1=0\]</span> 上两式均为<font color="maroon">对极约束</font>，几何意义在于<span class="math inline">\(O_1,P,O_2\)</span>三点共面，约束中包含了平移和旋转。再利用本质矩阵<span class="math inline">\(E=t∧R\)</span>基础矩阵<span class="math inline">\(F=K^{-T}EK^{-1}\)</span>简化对极约束得到：<span class="math display">\[x_2^TEx_2=p_2^TFp_1=0\]</span> 所以相机位姿估计问题变为以下两步：</p><p>1、根据配对点的像素位置，求出 E 或者 F；</p><p>2、根据 E 或者 F，求出 <span class="math inline">\(R,t\)</span>（会用到奇异值分解）。</p><h4 id="实践对极约束求解相机运动">7.4 实践：对极约束求解相机运动</h4><p>通过Essential 矩阵来求解相机运动。</p><h4 id="三角测量">7.5 三角测量</h4><p>在利用对极约束估计了相机运动之后，下一步需要用相机的运动估计特征点的空间位置。在单目 SLAM 中，仅通过单张图像无法获得像素的深度信息，我们需要通过<font color="maroon">三角测量</font>（Triangulation）（或三 角化）的方法来估计地图点的深度。 　　三角测量是指，通过在两处观察同一个点的夹角，确定该点的距离。在 SLAM 中，我们主要用三角化来估计像素点的距离。以7.3.1中的图为例，理论上直线<span class="math inline">\(O_1p_1\)</span>与<span class="math inline">\(O_2p_2\)</span>会交于一个点 P，即对应的地图点在三维场景中的位置。但是由于噪声的影响，这两条直线往往无法相交，所以要用最小二乘法求解。前面知道特征点的归一化坐标 <span class="math display">\[s_1x_1=s_2Rx_2=+t\]</span> ,相机运动 <span class="math inline">\(R,t\)</span>已经估计出来了，现在要求的是两个特征点的深度。这两个深度可以分开求，要算<span class="math inline">\(s_2\)</span>，则在两侧的左边乘以<span class="math inline">\(x_1\)</span>外积可使等式归0，则右侧可看成是关于<span class="math inline">\(s_2\)</span>的方程，可以求出来。于是两个帧下的点的深度都求出来了，便可以确定空间坐标了，但是由于噪声的存在，估得的<span class="math inline">\(R,t\)</span>不一定精确使等式为零，所以更常见的做法求最小二乘解而不是零解。</p><h4 id="实践三角测量">7.6 实践：三角测量</h4><p>编程调用 OpenCV 提供的 triangulation 函数进行三角化。</p><p>三角化的矛盾：在增大平移，会导致匹配失效；而平移太小，则三角化精度不够。</p><h4 id="d-2dpnp">7.7 3D-2D:PnP</h4><p>　　PnP（Perspective-n-Point）是求解 3D 到 2D 点对运动的方法，它描述了当我们知道 n 个 3D 空间点以及它们的投影位置时，如何估计相机所在的位姿。特征点的 3D 位置可以由三角化，或者由 RGB-D 相机的深度图确定。因此，在双目或 RGB-D 的视觉里程计中，我们可以直接使用 PnP 估计相机运动。而在单目视觉里程计中必须先进行初始化，然后才能使用 PnP。3D-2D 方法不需要使用对极约束，又可以在很少的匹配点中获得较好的运动估计，是最重要的一种姿态估计方法。下面介绍PnP问题的多种求解方法：</p><h5 id="直接线性变换">7.7.1 直接线性变换</h5><p>涉及数学知识的后面再补充，待续...</p><h5 id="p3p">7.7.2 P3P</h5><p>P3P 是 PnP 的另一种解法，它仅使用三对匹配点，对数据要求较少。下图是P3P问题示意图： <img src="https://img-blog.csdnimg.cn/20200501095410636.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 　　它的输入数据为三对 3D-2D 匹配点，记 3D 点为 <span class="math inline">\(A,B,C\)</span>（世界坐标系中），2D 点为 <span class="math inline">\(a,b,c\)</span>，其中小写字母代表的点为大写字母在相机成像平面上的投影，如图 7-11 所示。此外，P3P 还需要使用一对验证点，以从可能的解出选出正确的那一 个（类似于对极几何情形）。记验证点对为 D−d，相机光心为 O。一旦 3D 点在相机坐标系 下的坐标能够算出，就得到了 3D-3D 的对应点，把 PnP 问题转换为了 ICP 问题。根据三角形之间的对应关系：<span class="math display">\[\Delta Oab-\Delta OAB, \Delta Obc-\Delta OBC, \Delta Oac-\Delta OAC\]</span> 再有余弦定理以及一些变换得如下两式：<span class="math display">\[(1-u)y^2-ux^2-cos⟨b,c⟩y +2uxycos⟨a,b⟩+1=0 \]</span> <span class="math display">\[(1-w)x^2-wy^2-cos⟨a,c⟩x +2wxycos⟨a,b⟩+1=0 \]</span> 其中<span class="math inline">\(x=\frac{AC}{OC}, y=\frac{OB}{OC}\)</span> ,<span class="math inline">\(v=\frac{AB^2}{OC^2}, uv=v=\frac{BC^2}{OC^2}, wv=\frac{AC^2}{OC^2}\)</span>。由于2D 点的图像位置已知，那么三个余弦是已知的，同时<span class="math inline">\(u,w\)</span>可以通过<span class="math inline">\(A,B,C\)</span>在世界坐标系下的坐标算出，变换到相机坐标系下之后，并不改变这个比值。式中的 <span class="math inline">\(x,y\)</span> 是未知的，随着相机移动会发生变化，所以该方程组是关于<span class="math inline">\(x,y\)</span>的一个二元二次方程（多项式方程）。</p><p>　　P3P 的原理利用了三角形相似性质，求解投影点 <span class="math inline">\(a,b,c\)</span> 在相机坐标系下的 3D 坐标，最后把问题转换成一个 3D 到 3D 的位姿估计问题。然而，P3P 也存在着一些问题：</p><p>1、 P3P 只利用三个点的信息。当给定的配对点多于 3 组时，难以利用更多的信息。</p><p>2、如果 3D 点或 2D 点受噪声影响，或者存在误匹配，则算法失效。</p><p>　　所以后续人们还提出了许多别的方法，利用更多的信息，而且用迭代的方式对相机位姿进行优化，以尽可能地消除噪声的影响。在 SLAM 当中，通常的做法是先使用 P3P/EPnP 等方法估计相机位姿，然后构建最小二乘优化问题对估计值进行调整（Bundle Adjustment）。接下来从非线性优化角度 来看一下 PnP 问题。</p><h5 id="bundle-adjustment">7.7.3 Bundle Adjustment</h5><p>（这里将用到李代数上的非线性最小二乘问题，李代数还没看呢，待续...）</p><h4 id="实践求解pnp">7.8 实践：求解PnP</h4><h5 id="使用-epnp-求解位姿">7.8.1 使用 EPnP 求解位姿</h5><h5 id="使用-ba-优化">7.8.2 使用 BA 优化</h5><h4 id="d-3dicp">7.9 3D-3D：ICP</h4><p>　　假设有一组配对好的 3D 点（比如对两个 RGB-D 图像进行了匹配）：<span class="math display">\[P = {p_1,...,p_n}, P′ = {p′ _1,...,p′_n}\]</span> 然后想找一个欧式变换<span class="math inline">\(R,t\)</span>，使得<span class="math display">\[∀i,p_i = R_{p′ _i} + t\]</span> 和 PnP 类似，ICP 的求解也分为两种方式：利用线性代数的求解（主要是 SVD），以及利用非线性优化方式的求解（类似于 Bundle Adjustment）。</p><h5 id="svd方法">7.9.1 SVD方法</h5><p>首先定义第 i 对点的误差项：<span class="math display">\[e_i=p_i-(R_{p&#39;_i}+t)\]</span> 然后构建最小二乘问题，求使得误差项的平方和达到最小的<span class="math inline">\(R,t\)</span>。推导过程省略。ICP 可以分为以下 三个步骤求解： 1、 计算两组点的质心位置 p,p′，然后计算每个点的去质心坐标：<span class="math inline">\(q_i = p_i −p, q′_i = p′_i −p′\)</span>。 2、根据以下优化问题计算旋转矩阵：<span class="math inline">\(R^∗ = arg\min_{R} \frac{1}{2}\sum_{i=1}^n∥q_i −Rq′_i∥^2\)</span>。 3、根据第二步的 R，计算 t：<span class="math inline">\(t^∗ = p-Rp′\)</span>。</p><h5 id="非线性优化方法">7.9.2 非线性优化方法</h5><p>非线性优化方法以迭代的方式去找最优值。该方法和前面讲述的 PnP 非常相似，以李代数表达位姿时，目标函数可以写成：<span class="math display">\[\min_{ξ}=\frac{1}{2}\sum_{i=1}^n∥p_i −exp(ξ∧)p′_i)∥^2_2\]</span> 单个误差项关于位姿导数已经在前面推导过了，使用李代数扰动模型即可。</p><p>　　在非线性优化中不断迭代就能找到极小值，不会遇到局部极小而非全局最小的情况，这意味着 ICP 求解可以任意选定初始值。 但这里讲的 ICP，是指<font color="teal">已经由图像特征给定了匹配的情况</font>下，进行位姿估计的问题。在匹配已知的情况下，这个最小二乘问题实际上具有解析解， 所以并没有必要进行迭代优化。不过，在 RGB-DSLAM 中，由于一个像素的深度数据可能测量不到，所以我们可以混合着使用 PnP 和 ICP 优化：对于深度已知的特征点，用建模它们的 3D-3D 误差；对于深度未知的特征点，则建模 3D-2D 的重投影误差。</p><h4 id="实践求解icp">7.10 实践：求解ICP</h4><p>待续... #### 7.10 小结 本节介绍了基于特征点的视觉里程计中的几个重要的问题。包括：</p><p>1、 特征点是如何提取并匹配的；</p><p>2、 如何通过 2D-2D 的特征点估计相机运动；</p><p>3、 如何从 2D-2D 的匹配估计一个点的空间位置；</p><p>4、3D-2D 的 PnP 问题，它的线性解法和 Bundle Adjustment 解法；</p><p>5、 3D-3D 的 ICP 问题，其线性解法和 Bundle Adjustment 解法。</p><p>　</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第六章</title>
    <link href="/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%85%AD%E7%AB%A0/"/>
    <url>/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%85%AD%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第四章</title>
    <link href="/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%9B%9B%E7%AB%A0/"/>
    <url>/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E5%9B%9B%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第五章</title>
    <link href="/2020/04/29/%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%BA%94%E7%AB%A0/"/>
    <url>/2020/04/29/%E8%A7%89SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%BA%94%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<p>第四章的李群与李代数现在看有点打脑壳，后面系统学习的时候再看~</p><h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第五章-相机与图像">第五章 相机与图像</h3><p><strong>本章目标：</strong> 　　1、理解针孔相机的模型、内参与径向畸变参数。 　　2、理解一个空间点是如何投影到相机成像平面的。 　　3、掌握 OpenCV 的图像存储与表达方式。 　　4、学会基本的摄像头标定方法。 　　 <strong>本章内容：</strong>　 　　前面几章讨论了机器人如何表示自身位姿，即变量的含义和运动方程部分，这章将介绍“机器人如何观测外部世界”，也就是观测方程部分 ——相机成像过程。在计算机中，一张照片由很多个像素组成，每个像素记录了色彩或亮度的信息。三维世界中的一个物体反射或发出的光线，穿过相机光 心后，投影在相机的成像平面上。相机的感光器件接收到光线后，产生了测量值，就得到了像素，形成了我们见到的照片。</p><h4 id="相机模型">5.1 相机模型</h4><p>　　相机将三维世界中的坐标点（单位为米）映射到二维图像平面（单位为像素）的过程可用一个几何模型进行描述。模型有很多种，其中最简单的为针孔模型，它描述了一束光线通过针孔之后，在针孔背面投影成像的关系，由于 相机镜头上的透镜的存在，会使得光线投影到成像平面的过程中会产生畸变。因此，使用针孔和畸变两个模型来描述整个投影过程。</p><h5 id="针孔相机模型">5.1.1 针孔相机模型</h5><p>　　模型结构如下图所示： 　　<img src="https://img-blog.csdnimg.cn/20200429170804981.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 　　如图所示的模型，习惯将z轴指向相机前方，，x 向右，y 向下，O 为摄像机的光心，也是针孔模型中的针孔。现实世界的空间点 P，经过小孔 O 投影之后，落在物理成像平面 <span class="math inline">\(O′−x′−y′\)</span>上成为像素点<span class="math inline">\(P&#39;\)</span>。设 P 的坐标为 <span class="math inline">\([X,Y,Z]^T\)</span>，<span class="math inline">\(P′\)</span> 为 <span class="math inline">\([X′,Y ′,Z′]^T\)</span>，并且设物理成像平面到小孔的距离为 f（焦距），把可以成像平面对称到相机前方，和三维空间点一起放在摄像机坐标系的同一侧(这样做是为了把原公式中的负号去掉，之所以可以这样，是因为其实大多数相机都有软件翻转小孔成像的倒像，这样做相当于是图像的预处理，所以并不影响研究他们的关系)，可以简化模型如下：<span class="math display">\[\frac{Z}{f}=\frac{X}{X&#39;}=\frac{Y}{Y&#39;}\]</span> 经过整理得<span class="math display">\[X&#39;=f\frac{X}{Z}\]</span> <span class="math display">\[Y&#39;=f\frac{Y}{Z}\]</span> 在相机中，最终获得的是 一个个的像素，这需要在成像平面上对像进行采样和量化。 　　</p><p>未完，待续...</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲 第三章</title>
    <link href="/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B8%89%E7%AB%A0/"/>
    <url>/2020/04/29/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%B8%89%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者： 高翔 | 张涛</p><h3 id="第三章-三维空间刚体运动">第三章 三维空间刚体运动</h3><p>本章将介绍SLAM的基本问题之一：<strong>一个刚体在三维空间中的运动是如何描述的</strong>。</p><h3 id="旋转矩阵">3.1 旋转矩阵</h3><h5 id="点和向量坐标系">3.1.1 点和向量，坐标系</h5><p>　　向量：它是线性空间中的一个元素，可以把它想象成从原点指向某处的一个箭头。对于<span class="math inline">\(a,b∈R^3\)</span>，内积可以写为：<span class="math display">\[a·b=a^Tb=\sum_{i=1}^3a_ib_i=|a||b|cos&lt;a,b&gt;\]</span> 而外积可以写成：<span class="math display">\[a×b=a\hat{}b=|a||b|sin&lt;a,b&gt;\]</span> 外积只对三维向量存在定义，我们还能用外积表示向量的旋转。</p><h5 id="坐标系间的欧氏变换">3.1.2 坐标系间的欧氏变换</h5><p>　　与向量间的旋转类似，我们同样可以描述两个坐标系之间的旋转关系，再加上平移，统称为坐标系之间的变换关系。在运动过程中，通常设定惯性坐标系<span class="math inline">\(p_w\)</span>（世界坐标系）和移动坐标系<span class="math inline">\(p_c\)</span>（相机或机器人），变换关系如图： 　　<img src="https://img-blog.csdnimg.cn/20200429153715435.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 　　相机运动是一个刚体运动，它保证了同一个向量在各个坐标系下的长度和夹角都不会发生变化，这种变换称为欧氏变换。这里有一个旋转矩阵<span class="math inline">\(R\)</span>的概念：这个矩阵由两组基之间的内积组成，刻 画了旋转前后同一个向量的坐标变换关系，换句话说，旋转矩阵可以描述相机的旋转。除了旋转，还有一个平移<span class="math inline">\(t\)</span>，世界坐标系中的向量<span class="math inline">\(a\)</span>，经过一次旋转（用<span class="math inline">\(R\)</span>描述）和一次平移<span class="math inline">\(t\)</span>后，得到了<span class="math inline">\(a&#39;\)</span>，于是有：<span class="math inline">\(a&#39;=Ra+t\)</span>。</p><h5 id="变换矩阵与齐次坐标">3.1.3 变换矩阵与齐次坐标</h5><p>　　当世界坐标系中的向量经过多次变换后，其变换后向量的表达形式会很复杂，引入齐次坐标和变换矩阵重写： 　　<span class="math display">\[ \begin{bmatrix} a&#39;  \\ 1 \\ \end{bmatrix}=\begin{bmatrix} R&amp;t \\ 0^T&amp;1 \\ \end{bmatrix} \begin{bmatrix} a  \\ 1 \\ \end{bmatrix} = T \begin{bmatrix} a  \\ 1 \\ \end{bmatrix}\]</span> 这是一个数学技巧：把一个三维向量<span class="math inline">\(a\)</span>的末尾添加1，变成了四维向量，称为齐次 坐标。对于这个四维向量，我们可以把旋转和平移写在一个矩阵里面，使得整个关系变成了线性关系其中<span class="math inline">\(T\)</span>为变换矩阵。在齐次坐标中，某个点 x 的每个分量同乘一个非零常数 k 后，仍然表示的是同一个点。因此，一个点的具体坐标值不是唯一的，但当最后一项不为零时，总可以把所有坐标除以最后一项，强制最后一项为 1，从而得到一个点唯一的坐标表示（也就是转换成非齐次坐标），即 <span class="math display">\[\tilde{x}=[x,y,x,w]^T=[x/w,y/w,z/w,1]^T\]</span> 然后忽略掉最后为1的项，这个点的坐标和欧氏空间就是一样的了。那么惯性坐标系向量<span class="math inline">\(a\)</span>经两次变换累加的形式可以写为：<span class="math display">\[\tilde{b}=T_1\tilde{a}，\tilde{c}=T_2\tilde{b}  \Rightarrow \tilde{c}=T_1T_2\tilde{a}\]</span> 其中<span class="math inline">\(\tilde{a}\)</span> 表示<span class="math inline">\(a\)</span>的齐次坐标。</p><h3 id="实践eigen">3.2 实践：Eigen</h3><p><strong>Eigen</strong>是一个 C++ 开源线性代数库。它提供了快速的有关矩阵的线性代数运算，还包括解方程等功能。许多上层的软件库也使用 Eigen 进行矩阵运算，包括 g2o、Sophus等。</p><h3 id="旋转向量和欧拉角">3.3 旋转向量和欧拉角</h3><h3 id="四元数">3.4 四元数</h3><h3 id="相似仿射射影变换">3.5 相似、仿射、射影变换</h3><p>在3D变换中，欧氏变换保持了向量的长度和夹角，相当于我们把一个刚体原封不动地进行了移动或旋转，不改变它自身的样子，而其他几种变换则会改变它的外形，它们都拥有类似的矩阵表示。</p><p><strong>1、 相似变换</strong> 　相似变换比欧氏变换多了一个自由度，它允许物体进行均匀的缩放，其矩阵表示为： <span class="math display">\[T_s= \begin{bmatrix} sR&amp;t  \\ 0^T&amp;1 \\ \end{bmatrix}\]</span> 缩放因子 s 表示在对向量旋转之后，可以在 <span class="math inline">\(x,y,z\)</span> 三个坐标上进行均匀的缩放，相似变换不再保持图形的面积不变。</p><p><strong>2、仿射变换</strong> 　其矩阵表示为： <span class="math display">\[T_A= \begin{bmatrix} A&amp;t  \\ 0^T&amp;1 \\ \end{bmatrix}\]</span> 仿射变换只要求 A 是一个可逆矩阵，而不必是正交矩阵，这与欧式变换不同，仿射变换也叫正交投影。经过仿射变换之后，立方体就不再是方的了，但是各个面仍然是平行四边形。</p><p><strong>3、射影变换</strong> 　其矩阵表示为： <span class="math display">\[T_P= \begin{bmatrix} A&amp;t  \\ a^T&amp;v \\ \end{bmatrix}\]</span> 它左上角为可逆矩阵 A，右上为平移 t，左下缩放<span class="math inline">\(a^T\)</span>。由于采用齐坐标，当 v ≠ 0 时，可以对整个矩阵除以 v 得到一个右下角为 1 的矩阵；否则，得到右下角 为 0 的矩阵。从真实世界到相机照片的变换可以看成一个射影变换，也就是说物体的形状会发生不规则变化。其实如果相机的焦距为无穷远，那么这个变换则为仿射变换。</p><h3 id="实践eigen几何模块">3.6 实践：Eigen几何模块</h3><h3 id="可视化演示">3.7 可视化演示</h3>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 视觉SLAM十四讲 第二章</title>
    <link href="/2020/04/28/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%BA%8C%E7%AB%A0/"/>
    <url>/2020/04/28/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2-%E7%AC%AC%E4%BA%8C%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者：高翔 | 张涛 <br> 最后更新于：2017.03.31</p><h3 id="第二章-初识slam">第二章 初识SLAM</h3><h4 id="小萝卜例子">2. 1 小萝卜例子</h4><p><img src="https://img-blog.csdnimg.cn/20200428103415425.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><p>小萝卜是一个组装机器人，设备有相机、轮子、笔记本，我们希望小萝卜有自主运动能力，它至少要知道两件事： <br> 1. 我在什么地方——定位。 <br> 2. 周围环境是什么样——建图。 　　</p><p>　　有些传感器可以解决定位问题了，而有些不能，分为两种：一类是在机器人人体上的（轮式编码器、相机、激光、惯性测量单元IMU等，所测得数据是间接物理量，例如轮子转动角度、角速度和加速度等）；另一类是安装于环境中的（导轨、二维码标志等）可直接测量机器人位置信息，即解决定位问题，<font color="red">但不能在所有环境中都安装</font>。所以没有GPS的地方定位就是接下来要解决的问题。下面说说视觉SLAM中的视觉，即小萝卜的眼睛——相机。 <br> 　　SLAM的相机与昂贵的单反不同。按工作方式，分为：单目(Monocular)、双目(Stereo)、深度相机(RGB-D)。从图中来看，小萝卜用的是双目相机。这几类相机用来做SLAM时各有特点： <br> * <strong>单目相机</strong> 单目相机的数据：照片，它以二维的形式反映了三维的世界，它少了场景的一个维度：深度（或距离）。我们的眼睛在多数情况下可以判断出物体的远近，当然也有时效的时候，由于单目相机是二维的，所以想要恢复三维结构，就必须移动相机的视角，通过视差定量地判断物体的远近，但这只是一个相对的距离，无法确定物体的具体尺度，称之为<font color="maroon">尺度不确定性</font>。 <br> * <strong>双目相机和深度相机</strong> 双目相机和深度相机的目的，在于通过某种手段测量物体离我们的距离，克服单目无法知道距离的缺点。双目相机由两个单目相机组成，但这两个相机之间的距离（称为基线 Baseline）是已知的，通过这个基线来估计每个像素的空间位置——这和人眼非常相似。双目相机的距离估计是比较左右眼的图像获得的，并不依赖其他传感设备，所以它既可以应用在室内，亦可应用于室外。<font color="red">但是其缺点是配置与标定均较为复杂，其深度量程和精度受双目的基线与分辨率限制，而且视差的计算非常消耗计算资源，需要使用 GPU 和 FPGA 设备加速后，才能实时输出整张图像的距离信息，因此在现有的条件下，计算量是双目的主要问题之一。</font> 深度相机（又称 RGB-D 相机），它最大的特点是可以通过红外结构光或 Time-of-Flight（ToF）原理，像激光传感器那样，通过主动向物体发射光并接收返回的光，测出物体离相机的距离。其计算量小于双目，但现在多数 RGB-D 相机还存在<font color="maroon">测量范围窄、噪声大、视野小、易受日光干扰、无法测量透射材质</font>等诸多问题，在 SLAM 方面，主要用于室内 SLAM，室外则较难应用。</p><h4 id="经典视觉slam框架">2.2 经典视觉SLAM框架</h4><p>下图是SLAM的框架： <img src="https://img-blog.csdnimg.cn/20200429111630228.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"></p><p>其流程主要分为以下几步： <br> * <font color="red">传感器信息读取</font>。在视觉 SLAM 中主要为<strong>相机图像信息的读取和预处理</strong>。 <br> * <font color="red">视觉里程计</font> (Visual Odometry, VO，又称为前端（Front End）)。视觉里程计任务是<strong>估算相邻图像间相机的运动</strong>， <strong>以及局部地图的样子</strong>。 <br> * <font color="red">后端优化</font>（Optimization，在VO之后又称为后端（Back End））。后端接受不同时刻视觉里程计测量的相机位姿，以及回环检测的信息，对它们进行<strong>优化</strong>，得到全局一致的<strong>轨迹和地图</strong>。 <br> * <font color="red">回环检测</font>（Loop Closing）。回环检测<strong>判断机器人是否曾经到达过先前的位置</strong>，如果检测到回环，它会把信息提供给后端进行处理。 <br> * <font color="red">建图</font>（Mapping）。它根据估计的轨迹，<strong>建立与任务要求对应的地图</strong>。</p><p>下面针对一些模块做简要介绍：</p><h5 id="视觉里程计">2.2.1 视觉里程计</h5><p><strong>VO知识铺垫</strong>：VO 能够通过相邻帧间的图像估计相机运动，并恢复场景的空间结构。叫它为“里程计”是因为它和实际的里程计一样，只计算相邻时刻的运动，而和再往前的过去的信息没有关联。 <br> 　　这里要解决的问题是：计算机是如何通过图像确定相机的运动呢？在视觉SLAM中，计算机中只有像素点，知道它们是某些空间点在相机的成像平面上投影的结果。要定量估计相机的运动就必须先知道相机与空间点的几何关系。 <br> 　　假设VO已经估计了两张图像间的相机运动，所以只要将每两个相邻时刻的运动串起来，就构成机器人的运动轨迹，即解决了定位问题。另一方面根据每个时刻相机的定位，计算出各像素点对应的空间位置，就得到了地图。但是由于运动估计误差的积累，将出现<font color="red">累计漂移（Accumulating Drift）</font>，它将导致我们无法建立一致的地图，为了解决漂移问题，我们还需要两种技术：后端优化和回环检测。</p><h5 id="后端优化">2.2.2 后端优化</h5><p>　后端优化主要处理SLAM过程中的噪声问题。后端优化要考虑的问题， 就是如何从这些带有噪声的数据中，估计整个系统的状态，以及这个状态估计的不确定性有多大——<font color="red">这称为最大后验概率估计（Maximum-a-Posteriori，MAP）</font>。这里的状态既包括机器人自身的轨迹，也包含地图。为解决SLAM对运动主体自身和周围环境空间不确定性的估计，我们需要状态估计理论，把定位和建图的不确定性表达出来，然后采用滤波器或非线性优化，去估计状态的均值和不确定性（方差）。</p><h5 id="回环检测">2.2.3 回环检测</h5><p>　　回环检测，又称闭环检测（Loop Closure Detection），主要解决位置估计随时间漂移 的问题。通俗一点来解释：就是通过某种方法，让机器人知道“回到原点”这件事，或是把原点识别出来，再把位置的估计值“拉”过去，就可以消除漂移了，即所谓的回环检测。为了实现回环检测，我们需要让机器人具有识别曾到达过的场景的能力。可以在环境中安装传感器，类似在环境中的一种标志，只要看到了标志就知道自己回到了原点，但更希望机器人自身携带传感器——图像。可以判断图像间的相似性（图像匹配）完成闭环检测。</p><h5 id="建图">2.2.4 建图</h5><p>　　建图（Mapping）是指构建地图的过程。地图是对环境的描述，但这个描述并不是固定的，需要视 SLAM 的应用而定。地图类型如下： <img src="https://img-blog.csdnimg.cn/20200429130401375.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="70%"> 大体上讲，它们可以分为度量地图（Metric Map）与拓扑地图（Topological Map）两种： <br> * <strong>度量地图</strong> 度量地图强调精确地表示地图中物体的位置关系，通常用稀疏（Sparse）与稠密 （Dense）对它们进行分类。稀疏地图进行了一定程度的抽象，并不需要表达所有的物体，例如，只选择一部分具有代表意义的东西，称之为路标（Landmark），那么一张稀疏地图就是由路标组成的地图，而不是路标的部分就可以忽略掉。相对的，稠密地图着重于建模所有看到的东西。对于定位来说，稀疏路标地图就足够了，导航时则需要稠密地图。其中稠密地图有许多小块组成，小块的状态有占据、空闲和未知来表达格子（Grid）内是否有物体，通过查询告知的方式判断是否可以通过该格。但这样需要大量存储空间，并且大规模的度量地图易出现一致性问题，很小的一点转向误差，可能会导致两间屋子的墙出现重叠，使得地图失效。 <br> * <strong>拓扑地图</strong> 相比于度量地图的精确性，拓扑地图则更强调地图元素之间的关系。拓扑地图是一个图（Graph），由节点和边组成，只考虑节点间的连通性。它不注重地图对精确位置的需要，去掉地图的细节问题， 是一种更为紧凑的表达方式。然而，拓扑地图不擅长表达具有复杂结构的地图，如何对地图进行分割形成结点与边，又如何使用拓扑地图进行导航与路径规划，仍是有待研究的问题。</p><h4 id="slam问题的数学表述">2.3 SLAM问题的数学表述</h4><p>　　假设小萝卜携带着某种传感器在未知环境里运动。 首先，由于相机通常是在某些时刻采集数据的，所以我们也只关心这些时刻的位置和地图。 因此把一段连续时间的运动变成了离散时刻 <span class="math inline">\(t = 1,...,K\)</span> 当中发生的事情，各时刻的位置就记为 <span class="math inline">\(x_1,...,x_K\)</span>，它们构成了小萝卜的轨迹。地图方面，设地图是由许多个路标（Landmark）组成的，而每个时刻，传感器 会得到一部分路标点的观测数据，设路标点一共有 N 个，用<span class="math inline">\(y_1,...,y_N\)</span>表示它们。 小萝卜的运动由运动和观测来描述：一是从k-1时刻到k时刻位置x的变化；二是在k时刻的<span class="math inline">\(x_k\)</span>处探测出路标<span class="math inline">\(y_j\)</span>，这一事情用数学语言来描述。因此可以描述为两个方程：<span class="math display">\[x_k=f(x_{k-1},u_k,w_k)\]</span> <span class="math display">\[z_{k,j}=h(y_j,x_k,v_{k,j})\]</span>第一个是运动方程，其中<span class="math inline">\(u_k\)</span>是运动传感器的读数（有时也叫输入)，<span class="math inline">\(w_k\)</span>为噪声。用一个一 般函数 f 来描述这个过程，整个函数可以指代任意的运动传感器。第二个是观测方程，程描述的是当小萝卜在<span class="math inline">\(x_k\)</span>位置上 看到某个路标点<span class="math inline">\(y_j\)</span>，产生了一个观测数据<span class="math inline">\(z_{k,j}\)</span>，这里<span class="math inline">\(v_{k,j}\)</span>是这次观测里的噪声。由于观测所用的传感器形式更多，这里的观测数据 z 以及观测方程 h 也许多不同的形式。考虑视觉 SLAM 时，传感器是相机，那么观测方程就是<font color="blue">“对路标点拍摄后，得到了图 像中的像素”</font>的过程。 <br> 　 　需要说明一点，针对不同的传感器，两个方程都有不同的参数化形式，涉及到<span class="math inline">\(x,y,z\)</span>的参数化，这里不赘述了。但是这两个方程描述了最基本的 SLAM 问题：当知道运动传感器的读数 u，以及观测传感器的读数 z 后，把 SLAM 问题建模成了一个<strong>状态估计问题</strong>：如何通过带有噪声的测量数据估计内部的、隐藏着的状态变量？更具体的估计的求解与噪声性质 (线性/非线性和高斯/非高斯)有关，并且涉及三维空间中六自由度位姿的表达和优化，以及观测方程如何参数化 (即空间中的路标点是如何投影到一张照片上)。</p><h4 id="实践编程基础">2.4 实践：编程基础</h4><p>这本书的所有变成都是在Linux操作系统上完成的，程序将主要以 Linux 上的 C++ 程序为主。作为windows用户，我在电脑上装了一个Ubuntu的子系统。编程部分就自己悄悄地完成了~</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 学习规划</title>
    <link href="/2020/04/27/%E4%B9%A0%E8%A7%84%E5%88%92/"/>
    <url>/2020/04/27/%E4%B9%A0%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h5 id="迷茫的我接下来该干什么">迷茫的我接下来该干什么</h5><hr><p>记录一下我从充满热情到颓废无比的前半段研究生状态。</p><p>从去年10月份开始接触CV，慢慢的接触到图像匹配辅助定位方向，并为之看了将近20篇文章，并产生了两个idea，但是最后被否决了。为什么这个否定不来得早一些，偏偏在这个时候，我感觉真的对生活和学习都失去了兴趣，我真的完蛋了吗。</p><p>我不想，于是我打算梳理一下我做过的事学过的东西，并尝试着寻求下一条出路：</p><ul><li>Tools： python（tensorflow、pytorch）</li><li>深度网络： AlexNet、ResNet、VGG、RNN(LSTM)、GoogleNet、GAN、encoder-decoder</li><li>Others： attention mechanism</li><li>Papr reading：GAN交叉视图生成、cross-view geo-localization、depth estimation、attention module</li></ul><p>那么现在我的cross-view geo-localization方向要暂且放一放了，这其中学到的值得用的大概就是他们处理feature的方法、设计的Loss、几个特别的网络以及加入注意力机制的概念。 为了不让工作停滞，现在为自己规划一下接下来的工作：</p><ul><li>学技术：C++、keras、JAVA（每天学一些） <br></li><li>跑程序：训练GAN交叉视图的生成(CVUSA&amp;VHdataset)+attention moduel <br></li><li>其他：啃完《SLAM十四讲》（每天都要看一些并做笔记） <br></li><li>方向：继续看顶会上各个方向的文章（Long-term、多传感器融合等） 找找灵感或者感兴趣方向，记录想法，切记多看。</li></ul><p>以上工作或需要多线程进行了，成功的第一步大概就是走出舒适圈吧~</p><hr><p>小声逼逼：我不是这么轻易放弃的人，路还很长，梦想还是很遥远。</p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
      <category>planning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>GAN跑mnist数据集</title>
    <link href="/2020/04/27/AN%E8%B7%91mnist%E6%95%B0%E6%8D%AE%E9%9B%86/"/>
    <url>/2020/04/27/AN%E8%B7%91mnist%E6%95%B0%E6%8D%AE%E9%9B%86/</url>
    
    <content type="html"><![CDATA[<h2 id="gan网络在手写数字数据集mnist上的运用">GAN网络在手写数字数据集mnist上的运用</h2><p>前言：最近学了几种GAN网络，想自己动手跑一跑，就搭了个很简单的普通GAN网络。</p><p><strong>一、网络架构</strong><br>GAN的原理就不详细说明了，有很多很好资源都可以学习到，在这里定义的GAN网络结构如下（图来自灵魂画手<a href="https://zhuanlan.zhihu.com/p/85908702" target="_blank" rel="noopener">知乎</a>）： <img src="https://img-blog.csdnimg.cn/20200316163116180.png?,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="framework"> <font color="red">结构</font> ：input --&gt; 全连接层 --&gt; leak Relu --&gt; dropout20% --&gt; 全连接层 --&gt; tanh --&gt;output。（生成器和判别器的网络架构一样） <br> <font color="red">判别器的目的是</font>：1. 对于真实图片，D要为其打上标签1；对于生成图片，D要为其打上标签0。 <br> <font color="red">生成器的目的是 </font>：对于生成的图片，G希望D打上标签1。</p><p><strong>二、源代码</strong><br>part1，import~~</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> tensorflow <span class="im">as</span> tf</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-3" data-line-number="3"><span class="im">import</span> pickle</a><a class="sourceLine" id="cb1-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a><a class="sourceLine" id="cb1-5" data-line-number="5"><span class="im">from</span> tensorflow.examples.tutorials.mnist <span class="im">import</span> input_data</a></code></pre></div><p>part2,定义generator 和 discriminator * Generator</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw">def</span> generator(noise_img,reuse):</a><a class="sourceLine" id="cb2-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a><a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co">    生成器</span></a><a class="sourceLine" id="cb2-4" data-line-number="4"><span class="co">    :param noise_img:</span></a><a class="sourceLine" id="cb2-5" data-line-number="5"><span class="co">    :return:</span></a><a class="sourceLine" id="cb2-6" data-line-number="6"><span class="co">    &quot;&quot;&quot;</span></a><a class="sourceLine" id="cb2-7" data-line-number="7">    alpha <span class="op">=</span> <span class="fl">0.01</span>      <span class="co"># parameter of leaky ReLU</span></a><a class="sourceLine" id="cb2-8" data-line-number="8">    out_dim <span class="op">=</span> <span class="dv">784</span>     <span class="co"># the output size of the generator, MNIST images are 28*28, into a length of 784 array.</span></a><a class="sourceLine" id="cb2-9" data-line-number="9">    g_units <span class="op">=</span> <span class="dv">128</span>    <span class="co"># hidden layer nodes</span></a><a class="sourceLine" id="cb2-10" data-line-number="10"></a><a class="sourceLine" id="cb2-11" data-line-number="11">    <span class="cf">with</span> tf.variable_scope(<span class="st">&quot;generator&quot;</span>, reuse<span class="op">=</span>reuse):</a><a class="sourceLine" id="cb2-12" data-line-number="12">        <span class="co"># hidden layer</span></a><a class="sourceLine" id="cb2-13" data-line-number="13">        hidden1 <span class="op">=</span> tf.layers.dense(noise_img, g_units)</a><a class="sourceLine" id="cb2-14" data-line-number="14"></a><a class="sourceLine" id="cb2-15" data-line-number="15">        <span class="co"># leaky ReLU</span></a><a class="sourceLine" id="cb2-16" data-line-number="16">        hidden1 <span class="op">=</span> tf.maximum(alpha <span class="op">*</span> hidden1, hidden1)</a><a class="sourceLine" id="cb2-17" data-line-number="17"></a><a class="sourceLine" id="cb2-18" data-line-number="18">        <span class="co"># dropout</span></a><a class="sourceLine" id="cb2-19" data-line-number="19">        hidden1 <span class="op">=</span> tf.layers.dropout(hidden1, rate<span class="op">=</span><span class="fl">0.2</span>)</a><a class="sourceLine" id="cb2-20" data-line-number="20"></a><a class="sourceLine" id="cb2-21" data-line-number="21">        <span class="co"># logits &amp; outputs,tf.layers.dense——full connection</span></a><a class="sourceLine" id="cb2-22" data-line-number="22">        logits <span class="op">=</span> tf.layers.dense(hidden1, out_dim)</a><a class="sourceLine" id="cb2-23" data-line-number="23">        <span class="co"># the pixel range of MNIST  dataset is 0-1, the generated image range is (-1,1).</span></a><a class="sourceLine" id="cb2-24" data-line-number="24">        outputs <span class="op">=</span> tf.tanh(logits)</a><a class="sourceLine" id="cb2-25" data-line-number="25"></a><a class="sourceLine" id="cb2-26" data-line-number="26">    <span class="cf">return</span> logits, outputs</a></code></pre></div><ul><li>Discriminator</li></ul><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="kw">def</span>  discriminator(img,reuse):</a><a class="sourceLine" id="cb3-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a><a class="sourceLine" id="cb3-3" data-line-number="3"><span class="co">    discriminator</span></a><a class="sourceLine" id="cb3-4" data-line-number="4"><span class="co">    :param img:</span></a><a class="sourceLine" id="cb3-5" data-line-number="5"><span class="co">    :return:</span></a><a class="sourceLine" id="cb3-6" data-line-number="6"><span class="co">    &quot;&quot;&quot;</span></a><a class="sourceLine" id="cb3-7" data-line-number="7">    alpha<span class="op">=</span><span class="fl">0.01</span>      </a><a class="sourceLine" id="cb3-8" data-line-number="8">    d_units<span class="op">=</span><span class="dv">128</span>    </a><a class="sourceLine" id="cb3-9" data-line-number="9"></a><a class="sourceLine" id="cb3-10" data-line-number="10">    <span class="cf">with</span> tf.variable_scope(<span class="st">&quot;discriminator&quot;</span>, reuse<span class="op">=</span>reuse):</a><a class="sourceLine" id="cb3-11" data-line-number="11">        <span class="co"># hidden layer</span></a><a class="sourceLine" id="cb3-12" data-line-number="12">        hidden2 <span class="op">=</span> tf.layers.dense(img, d_units)</a><a class="sourceLine" id="cb3-13" data-line-number="13">        <span class="co"># leaky ReLU</span></a><a class="sourceLine" id="cb3-14" data-line-number="14">        hidden2 <span class="op">=</span> tf.maximum(alpha <span class="op">*</span> hidden2, hidden2)</a><a class="sourceLine" id="cb3-15" data-line-number="15">        <span class="co"># dropout</span></a><a class="sourceLine" id="cb3-16" data-line-number="16">        hidden2 <span class="op">=</span> tf.layers.dropout(hidden2, rate<span class="op">=</span><span class="fl">0.2</span>)</a><a class="sourceLine" id="cb3-17" data-line-number="17">        <span class="co"># logits &amp; outputs</span></a><a class="sourceLine" id="cb3-18" data-line-number="18">        logits <span class="op">=</span> tf.layers.dense(hidden2,<span class="dv">1</span>)</a><a class="sourceLine" id="cb3-19" data-line-number="19">        outputs <span class="op">=</span> tf.tanh(logits)</a><a class="sourceLine" id="cb3-20" data-line-number="20"></a><a class="sourceLine" id="cb3-21" data-line-number="21">    <span class="cf">return</span> logits, outputs</a></code></pre></div><p>part3，mian() —初始化、训练、抽样评估</p><div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">def</span> GAN_mnist():</a><a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="co">&quot;&quot;&quot;</span></a><a class="sourceLine" id="cb4-3" data-line-number="3"><span class="co">    simple GAN generate handwritten digital picture</span></a><a class="sourceLine" id="cb4-4" data-line-number="4"><span class="co">    :return:</span></a><a class="sourceLine" id="cb4-5" data-line-number="5"><span class="co">    &quot;&quot;&quot;</span></a><a class="sourceLine" id="cb4-6" data-line-number="6">    mnist <span class="op">=</span> input_data.read_data_sets(<span class="st">&#39;./mnist_dataset/&#39;</span>,one_hot<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb4-7" data-line-number="7">    img_size <span class="op">=</span> mnist.train.images[<span class="dv">0</span>].shape[<span class="dv">0</span>]</a><a class="sourceLine" id="cb4-8" data-line-number="8">    noise_size <span class="op">=</span> <span class="dv">100</span></a><a class="sourceLine" id="cb4-9" data-line-number="9"></a><a class="sourceLine" id="cb4-10" data-line-number="10">    <span class="co"># 1、parameters preparing</span></a><a class="sourceLine" id="cb4-11" data-line-number="11">    learning_rate <span class="op">=</span> <span class="fl">0.001</span></a><a class="sourceLine" id="cb4-12" data-line-number="12">    smooth <span class="op">=</span> <span class="fl">0.1</span>    <span class="co"># label smoothing</span></a><a class="sourceLine" id="cb4-13" data-line-number="13">    n_sample <span class="op">=</span> <span class="dv">25</span> <span class="co"># number of samples</span></a><a class="sourceLine" id="cb4-14" data-line-number="14">    samples<span class="op">=</span>[]</a><a class="sourceLine" id="cb4-15" data-line-number="15"></a><a class="sourceLine" id="cb4-16" data-line-number="16">    tf.reset_default_graph()</a><a class="sourceLine" id="cb4-17" data-line-number="17">    <span class="co"># 2、dataset preparing</span></a><a class="sourceLine" id="cb4-18" data-line-number="18">    real_img <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, img_size], name<span class="op">=</span><span class="st">&#39;real_img&#39;</span>) <span class="co"># image size784=28*28</span></a><a class="sourceLine" id="cb4-19" data-line-number="19">    <span class="co"># noise size input generator</span></a><a class="sourceLine" id="cb4-20" data-line-number="20">    noise_img <span class="op">=</span> tf.placeholder(tf.float32, [<span class="va">None</span>, noise_size], name<span class="op">=</span><span class="st">&#39;noise_img&#39;</span>)</a><a class="sourceLine" id="cb4-21" data-line-number="21"></a><a class="sourceLine" id="cb4-22" data-line-number="22">    <span class="co"># 3、Generator generate image</span></a><a class="sourceLine" id="cb4-23" data-line-number="23">    g_logits,g_outputs <span class="op">=</span> generator(noise_img,reuse<span class="op">=</span><span class="va">False</span>)</a><a class="sourceLine" id="cb4-24" data-line-number="24">    <span class="co"># generator的loss</span></a><a class="sourceLine" id="cb4-25" data-line-number="25"></a><a class="sourceLine" id="cb4-26" data-line-number="26">    <span class="co"># 4、discriminator discriminates between true and false</span></a><a class="sourceLine" id="cb4-27" data-line-number="27">    d_logits_real, d_outputs_real <span class="op">=</span> discriminator(real_img , reuse<span class="op">=</span><span class="va">False</span>)</a><a class="sourceLine" id="cb4-28" data-line-number="28">    d_logits_fake, d_outputs_fake <span class="op">=</span> discriminator(g_outputs , reuse<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb4-29" data-line-number="29"></a><a class="sourceLine" id="cb4-30" data-line-number="30">    <span class="co"># 5、calculate loss</span></a><a class="sourceLine" id="cb4-31" data-line-number="31">    <span class="co"># discriminator_loss：discriminate real image + discriminate fake image</span></a><a class="sourceLine" id="cb4-32" data-line-number="32"></a><a class="sourceLine" id="cb4-33" data-line-number="33">    <span class="co"># test the discriminant discriminant ability</span></a><a class="sourceLine" id="cb4-34" data-line-number="34">    d_loss_real <span class="op">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</a><a class="sourceLine" id="cb4-35" data-line-number="35">        logits<span class="op">=</span>d_logits_real,labels<span class="op">=</span>tf.ones_like(d_logits_real)) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> smooth))</a><a class="sourceLine" id="cb4-36" data-line-number="36"></a><a class="sourceLine" id="cb4-37" data-line-number="37">    <span class="co"># test the discriminant discriminant ability</span></a><a class="sourceLine" id="cb4-38" data-line-number="38">    d_loss_fake <span class="op">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</a><a class="sourceLine" id="cb4-39" data-line-number="39">        logits<span class="op">=</span>d_logits_fake,labels<span class="op">=</span>tf.zeros_like(d_logits_fake)))</a><a class="sourceLine" id="cb4-40" data-line-number="40"></a><a class="sourceLine" id="cb4-41" data-line-number="41">    <span class="co"># sum_loss</span></a><a class="sourceLine" id="cb4-42" data-line-number="42">    d_loss <span class="op">=</span> tf.add(d_loss_real, d_loss_fake)</a><a class="sourceLine" id="cb4-43" data-line-number="43"></a><a class="sourceLine" id="cb4-44" data-line-number="44">    <span class="co"># generator_loss</span></a><a class="sourceLine" id="cb4-45" data-line-number="45">    g_loss <span class="op">=</span> tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(</a><a class="sourceLine" id="cb4-46" data-line-number="46">        logits<span class="op">=</span>d_logits_fake,labels<span class="op">=</span>tf.ones_like(d_logits_fake)) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> smooth))</a><a class="sourceLine" id="cb4-47" data-line-number="47"></a><a class="sourceLine" id="cb4-48" data-line-number="48">    <span class="co"># 6、Gradient descent loss optimization</span></a><a class="sourceLine" id="cb4-49" data-line-number="49">    train_vars <span class="op">=</span> tf.trainable_variables()</a><a class="sourceLine" id="cb4-50" data-line-number="50">    <span class="co"># generator_tensor</span></a><a class="sourceLine" id="cb4-51" data-line-number="51">    g_vars <span class="op">=</span> [var <span class="cf">for</span> var <span class="kw">in</span> train_vars <span class="cf">if</span> var.name.startswith(<span class="st">&quot;generator&quot;</span>)]</a><a class="sourceLine" id="cb4-52" data-line-number="52">    <span class="co"># discriminator_tensor</span></a><a class="sourceLine" id="cb4-53" data-line-number="53">    d_vars <span class="op">=</span> [var <span class="cf">for</span> var <span class="kw">in</span> train_vars <span class="cf">if</span> var.name.startswith(<span class="st">&quot;discriminator&quot;</span>)]</a><a class="sourceLine" id="cb4-54" data-line-number="54">    <span class="co"># optimizer</span></a><a class="sourceLine" id="cb4-55" data-line-number="55">    d_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list<span class="op">=</span>d_vars)</a><a class="sourceLine" id="cb4-56" data-line-number="56">    g_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list<span class="op">=</span>g_vars)</a><a class="sourceLine" id="cb4-57" data-line-number="57"></a><a class="sourceLine" id="cb4-58" data-line-number="58">    <span class="co"># 7、trianing part</span></a><a class="sourceLine" id="cb4-59" data-line-number="59">    <span class="co"># train parameters</span></a><a class="sourceLine" id="cb4-60" data-line-number="60">    batch_size <span class="op">=</span> <span class="dv">64</span></a><a class="sourceLine" id="cb4-61" data-line-number="61">    epoch <span class="op">=</span> <span class="dv">5</span></a><a class="sourceLine" id="cb4-62" data-line-number="62">    n_batch<span class="op">=</span> mnist.train.num_examples <span class="op">//</span> batch_size   <span class="co"># Calculate how many lots there are</span></a><a class="sourceLine" id="cb4-63" data-line-number="63"></a><a class="sourceLine" id="cb4-64" data-line-number="64">    losses <span class="op">=</span> [] <span class="co"># store loss</span></a><a class="sourceLine" id="cb4-65" data-line-number="65"></a><a class="sourceLine" id="cb4-66" data-line-number="66">    <span class="co"># start training</span></a><a class="sourceLine" id="cb4-67" data-line-number="67">    <span class="cf">with</span> tf.Session() <span class="im">as</span> sess:</a><a class="sourceLine" id="cb4-68" data-line-number="68">        sess.run(tf.global_variables_initializer())</a><a class="sourceLine" id="cb4-69" data-line-number="69"></a><a class="sourceLine" id="cb4-70" data-line-number="70">        <span class="cf">for</span> e <span class="kw">in</span> <span class="bu">range</span>(epoch):</a><a class="sourceLine" id="cb4-71" data-line-number="71">            <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_batch):</a><a class="sourceLine" id="cb4-72" data-line-number="72">                batch <span class="op">=</span> mnist.train.next_batch(batch_size)</a><a class="sourceLine" id="cb4-73" data-line-number="73">                batch_images <span class="op">=</span> batch[<span class="dv">0</span>].reshape((batch_size,<span class="dv">784</span>))  <span class="co"># batch[0]represents image,reshape-&gt; batch_size*784</span></a><a class="sourceLine" id="cb4-74" data-line-number="74">                batch_images <span class="op">=</span> batch_images <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>   <span class="co"># scale the pixel of images for outputs of tanh are (-1,1)</span></a><a class="sourceLine" id="cb4-75" data-line-number="75">                batch_noise <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>(batch_size, <span class="dv">100</span>))</a><a class="sourceLine" id="cb4-76" data-line-number="76"></a><a class="sourceLine" id="cb4-77" data-line-number="77">                _ <span class="op">=</span> sess.run(d_train_opt, feed_dict<span class="op">=</span>{real_img: batch_images, noise_img: batch_noise})</a><a class="sourceLine" id="cb4-78" data-line-number="78">                _ <span class="op">=</span> sess.run(g_train_opt, feed_dict<span class="op">=</span>{noise_img: batch_noise})</a><a class="sourceLine" id="cb4-79" data-line-number="79"></a><a class="sourceLine" id="cb4-80" data-line-number="80">            <span class="co"># calculate the loss after each epoch</span></a><a class="sourceLine" id="cb4-81" data-line-number="81">            train_loss_d_real <span class="op">=</span> sess.run(d_loss_real,feed_dict<span class="op">=</span>{real_img:batch_images,noise_img:batch_noise})</a><a class="sourceLine" id="cb4-82" data-line-number="82">            train_loss_d_fake <span class="op">=</span> sess.run(d_loss_fake,feed_dict<span class="op">=</span>{real_img:batch_images,noise_img:batch_noise})</a><a class="sourceLine" id="cb4-83" data-line-number="83"></a><a class="sourceLine" id="cb4-84" data-line-number="84">            train_loss_d <span class="op">=</span> sess.run(d_loss,feed_dict<span class="op">=</span>{real_img: batch_images, noise_img: batch_noise})</a><a class="sourceLine" id="cb4-85" data-line-number="85"></a><a class="sourceLine" id="cb4-86" data-line-number="86">            train_loss_g <span class="op">=</span> sess.run(g_loss,feed_dict<span class="op">=</span>{noise_img: batch_noise})</a><a class="sourceLine" id="cb4-87" data-line-number="87"></a><a class="sourceLine" id="cb4-88" data-line-number="88">            <span class="bu">print</span>(<span class="st">&quot;Epoch </span><span class="sc">{}</span><span class="st">/</span><span class="sc">{}</span><span class="st">...&quot;</span>.<span class="bu">format</span>(e <span class="op">+</span> <span class="dv">1</span>, epoch),</a><a class="sourceLine" id="cb4-89" data-line-number="89">                  <span class="st">&quot;Discriminator Loss: </span><span class="sc">{:.4f}</span><span class="st">(Real: </span><span class="sc">{:.4f}</span><span class="st"> + Fake: </span><span class="sc">{:.4f}</span><span class="st">)...&quot;</span>.<span class="bu">format</span>(train_loss_d, train_loss_d_real,</a><a class="sourceLine" id="cb4-90" data-line-number="90">                                                                                      train_loss_d_fake),</a><a class="sourceLine" id="cb4-91" data-line-number="91">                  <span class="st">&quot;Generator Loss: </span><span class="sc">{:.4f}</span><span class="st">&quot;</span>.<span class="bu">format</span>(train_loss_g))</a><a class="sourceLine" id="cb4-92" data-line-number="92"></a><a class="sourceLine" id="cb4-93" data-line-number="93">            <span class="co"># record the values of loss</span></a><a class="sourceLine" id="cb4-94" data-line-number="94">            losses.append((train_loss_d, train_loss_d_real, train_loss_d_fake, train_loss_g))</a><a class="sourceLine" id="cb4-95" data-line-number="95"></a><a class="sourceLine" id="cb4-96" data-line-number="96"></a><a class="sourceLine" id="cb4-97" data-line-number="97"></a><a class="sourceLine" id="cb4-98" data-line-number="98"></a><a class="sourceLine" id="cb4-99" data-line-number="99">            <span class="co"># sample to evaluate</span></a><a class="sourceLine" id="cb4-100" data-line-number="100">            sample_noise <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, size<span class="op">=</span>(n_sample, noise_size))</a><a class="sourceLine" id="cb4-101" data-line-number="101">            gen_samples <span class="op">=</span> sess.run(generator(noise_img, reuse<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb4-102" data-line-number="102">                                   ,feed_dict<span class="op">=</span>{noise_img: sample_noise})</a><a class="sourceLine" id="cb4-103" data-line-number="103">            samples.append(gen_samples)</a><a class="sourceLine" id="cb4-104" data-line-number="104"></a><a class="sourceLine" id="cb4-105" data-line-number="105"></a><a class="sourceLine" id="cb4-106" data-line-number="106">    <span class="co"># record the data of samples</span></a><a class="sourceLine" id="cb4-107" data-line-number="107">    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;train_samples.pkl&#39;</span>, <span class="st">&#39;wb&#39;</span>) <span class="im">as</span> f:</a><a class="sourceLine" id="cb4-108" data-line-number="108">        pickle.dump(samples, f)</a><a class="sourceLine" id="cb4-109" data-line-number="109"></a><a class="sourceLine" id="cb4-110" data-line-number="110">    <span class="co"># plot loss curve</span></a><a class="sourceLine" id="cb4-111" data-line-number="111">    losses <span class="op">=</span> np.array(losses)</a><a class="sourceLine" id="cb4-112" data-line-number="112">    plt.figure(<span class="dv">1</span>)</a><a class="sourceLine" id="cb4-113" data-line-number="113">    plt.subplot(<span class="dv">221</span>)</a><a class="sourceLine" id="cb4-114" data-line-number="114">    plt.plot(losses.T[<span class="dv">0</span>], label<span class="op">=</span><span class="st">&#39;Discriminator Total Loss&#39;</span>)</a><a class="sourceLine" id="cb4-115" data-line-number="115">    plt.subplot(<span class="dv">222</span>)</a><a class="sourceLine" id="cb4-116" data-line-number="116">    plt.plot(losses.T[<span class="dv">1</span>], label<span class="op">=</span><span class="st">&#39;Discriminator Real Loss&#39;</span>)</a><a class="sourceLine" id="cb4-117" data-line-number="117">    plt.subplot(<span class="dv">223</span>)</a><a class="sourceLine" id="cb4-118" data-line-number="118">    plt.plot(losses.T[<span class="dv">2</span>], label<span class="op">=</span><span class="st">&#39;Discriminator Fake Loss&#39;</span>)</a><a class="sourceLine" id="cb4-119" data-line-number="119">    plt.subplot(<span class="dv">224</span>)</a><a class="sourceLine" id="cb4-120" data-line-number="120">    plt.plot(losses.T[<span class="dv">3</span>], label<span class="op">=</span><span class="st">&#39;Generator&#39;</span>)</a><a class="sourceLine" id="cb4-121" data-line-number="121">    plt.title(<span class="st">&quot;Training Losses&quot;</span>)</a><a class="sourceLine" id="cb4-122" data-line-number="122">    plt.show()</a><a class="sourceLine" id="cb4-123" data-line-number="123"></a><a class="sourceLine" id="cb4-124" data-line-number="124">    <span class="cf">with</span> <span class="bu">open</span>(<span class="st">&#39;train_samples.pkl&#39;</span>, <span class="st">&#39;rb&#39;</span>) <span class="im">as</span> f:</a><a class="sourceLine" id="cb4-125" data-line-number="125">        samples <span class="op">=</span> pickle.load(f)</a><a class="sourceLine" id="cb4-126" data-line-number="126"></a><a class="sourceLine" id="cb4-127" data-line-number="127">    rows, cols <span class="op">=</span> <span class="dv">5</span>, <span class="dv">5</span></a><a class="sourceLine" id="cb4-128" data-line-number="128">    fig, axes <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">7</span>, <span class="dv">7</span>), nrows<span class="op">=</span>rows, ncols<span class="op">=</span>cols, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb4-129" data-line-number="129">    <span class="cf">for</span> ax, img <span class="kw">in</span> <span class="bu">zip</span>(axes.flatten(), samples[epoch <span class="op">-</span> <span class="dv">1</span>][<span class="dv">1</span>]):</a><a class="sourceLine" id="cb4-130" data-line-number="130">        ax.xaxis.set_visible(<span class="va">False</span>)</a><a class="sourceLine" id="cb4-131" data-line-number="131">        ax.yaxis.set_visible(<span class="va">False</span>)</a><a class="sourceLine" id="cb4-132" data-line-number="132">        ax.imshow(img.reshape((<span class="dv">28</span>, <span class="dv">28</span>)), cmap<span class="op">=</span><span class="st">&#39;Greys_r&#39;</span>)</a><a class="sourceLine" id="cb4-133" data-line-number="133">    plt.show()</a></code></pre></div><p>其中samples的部分就是抽样25个size为100的noise_samples送入生成器经过200epoch之后生成器生成图像，结果在第三部分。最后那几段是画图部分。</p><p>part4，you know</p><div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</a><a class="sourceLine" id="cb5-2" data-line-number="2">    GAN_mnist()</a></code></pre></div><p><strong>三、看结果</strong></p><p>训练时候的loss，这里epoch取的200： <br> 左上是Discriminator Total Loss，右上是Discriminator Real Loss，左下Discriminator Fake Loss，最后是Generator loss。 <img src="https://img-blog.csdnimg.cn/20200316161700318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="loss"></p><p>生成的数字示例： <img src="https://img-blog.csdnimg.cn/20200316223639385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="运行结果"> 只是200epoch的结果，感觉隐隐约约能看出来是啥数字，可以试试修改epoch，运行一次有点久了，下次跑cGAN网络。如果有什么错误欢迎指出，谢谢~~</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>GAN</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论文《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》</title>
    <link href="/2020/04/07/%E6%96%87%E3%80%8AGround-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss%E3%80%8B/"/>
    <url>/2020/04/07/%E6%96%87%E3%80%8AGround-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="ground-to-aeria-limage-geo-localization-with-a-hard-exemplar-reweighting-triplet-loss2019-iccv">《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》（2019-ICCV）</h3><p>Author：Sudong Cai， Yulan Guo，Salman Khan Jiwei Hu， Gongjian Wen</p><pre><code>中心思想：基于Siamese network提出了FCANet，在basic ResNet中加入空间注意力和通道注意力机制(FCAM)，且给出一种新的在线困难样本重新分配权值三元组损失，并在其他文献的成就基础上，加入方向回归损失作为辅助损失。</code></pre><p><font color="red"><strong>补充</strong></font><strong>:</strong>　<br> <strong>1、什么是Hard example (困难样本)?</strong> <br> 　　困难负样本是指哪些容易被网络预测为正样本的proposal，即假阳性(false positive,FP)，训练hard negative对提升网络的分类性能具有极大帮助。Hard example mining的核心思想就是用分类器对样本进行分类，把其中错误分类的样本(hard negative)放入负样本集合再继续训练分类器。</p><p><strong>2、Triplet loss原理</strong> <br> 　　Triplet Loss是深度学习中的一种损失函数，用于训练差异性较小的样本，通过优化锚示例(a)与正示例(p)的距离小于锚示例(a)与负示例(n)的距离，实现样本的相似性计算。 - 输入一个三元组 &lt;a, p, n&gt; a： anchor p： positive, 与 a 是同一类别的样本 n： negative, 与 a 是不同类别的样本</p><ul><li><p>公式 <span class="math display">\[L= max(d(a,p)-d(a,n)+margin,0)\]</span></p><p>优化目标就是拉近 a, p 的距离， 拉远 a, n 的距离。其中hard triplets是: <span class="math inline">\(d(a,n)&lt;d(a,p)\)</span>, 即a, p的距离远；easy triplets: <span class="math inline">\(L=0\)</span> 即 <span class="math inline">\(d(a,p)+margin&lt;d(a,n)\)</span>，这种情况不需要优化，a, p的距离本来就很近， a, n的距离远。</p></li></ul><h4 id="一backgroundhighlight">一、Background＆Highlight</h4><ul><li><p><strong>Background</strong> <br> 1、ground-to-ground image geo-localization没有可用的参考图像数据集； <br> 2、cross-view图像匹配的弊端：视点的剧烈变化、方向信息的不定性、光照变化等； <br> 3、 handcrafted features用于计算相似度，导致结果准确率低； <br> 4、已有的hard example mining仍然很难找出困难样本。</p></li><li><p><strong>Highlight</strong> <br> 1、提出了一个新的triplet loss(<font color="red">online exemplar reweighting triplet loss </font>)提高训练，可以实现online hard example mining； <br> 2、提出了lightweight attention module (FCAM)。</p></li></ul><h4 id="二framework">二、Framework</h4><p><strong>1、 Overview</strong> <br> 文章提出的构架概览如图，网络是基于Siamese network设计的: <img src="https://img-blog.csdnimg.cn/20200318172040954.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 　　　　</p><p>主要组成部分：① baseline是两个CNN，用于提取图像的特征，网络分支之间没有共享权重(因为在其他文献中已经说明共享权重的效果并没它好，有一个说法就是这两个分支本来就是各自训练的)。② Dual Attention，就是文中提出的FCAM模型，是在两个CNN网络中都加入了包括通道注意力和空间维度注意力的机制，用于加大特征的区别。③ Main Loss，即文章所提的困难样本重加权triplet loss，把大的权重分配给有用的hard triplets，而把小的权重分配给信息较少的easy triplets。④ Auxiliary Loss，即Orientation regression (OR) 学习分支的损失。</p><p><strong>2、Feature Context-Based Attention Module</strong><br>- <strong>Channel attention submodule</strong> <br> 　　通道注意力机制是用来强调信息相对丰富的通道。网络框架如下图所示，采用了卷积块注意模块(<a href="https://arxiv.org/pdf/1807.06521.pdf" target="_blank" rel="noopener">CBAM</a>)，输入特征图U(W×H×C)，应用max-pooling <span class="math inline">\(f_{max}\)</span>和 average-pooling <span class="math inline">\(f_{avg}\)</span> 作用于U产生一维的全局通道描述符<span class="math inline">\(v^1\)</span>和<span class="math inline">\(v^2\)</span>。然后，两个描述符经由 Multi-Layer Perceptron (MLP) <span class="math inline">\(f_{ext}(^.,^.)\)</span> 激活，分析通道之间的依赖关系。最后，用Sigmoid函数激活两个描述符的总和得到<span class="math inline">\(Z^C(U)\)</span>。输出channel attention map <span class="math inline">\(U&#39;\)</span>是通过<span class="math inline">\(Z^C(U)\)</span>和U之间的元素级乘法生成的。 <img src="https://img-blog.csdnimg.cn/20200318172206762.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><ul><li><p><strong>Spatial attention submodule</strong> <br> 空间注意力用于突出有意义的特征单元，网络构架如图所示。受 <a href="https://www.researchgate.net/publication/319535111_Learned_Contextual_Feature_Reweighting_for_Image_Geo-Localization" target="_blank" rel="noopener">Contextual Reweighting Network (CRN)</a> 启发，文章将特征上下文感知学习(特征reweighting strategy)集成到CBAM的基本控件注意子模块中。来自通道注意力的特征图<span class="math inline">\(U&#39;\)</span>作为输入，然后连接两个特征掩码<span class="math inline">\(f_{max}^c\)</span>和<span class="math inline">\(f_{avg}^c\)</span>(沿着通道轴线的最大池化和平均池化)生成S(W×H×2)。然后，为了利用特征元的上下文信息，使用了不同感受野的卷积（3×3，5×5和7×7）来生成中间特征掩码，再将masks连接起来生成特征掩码P(W×H×3)，再用1×1的卷积学习和积累权重生成<span class="math inline">\(Z^S(U&#39;)\)</span>。最后，spatial attention map <span class="math inline">\(U&#39;&#39;\)</span>是通过<span class="math inline">\(Z^S(U)\)</span>和<span class="math inline">\(U&#39;\)</span>之间的元素级乘法生成的。 <img src="https://img-blog.csdnimg.cn/20200318172347324.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li><li><p><strong>Building block</strong> <br> 集合FCAM到基本ResNet的每一个building block中就形成了CNN特征提取网络，如下图： <img src="https://img-blog.csdnimg.cn/2020031817270364.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li><li><strong>Hard Exemplar Reweighting Triplet Loss</strong> <br> 将基于triplet reweighting的在线困难样本挖掘策略集合到soft-margin triplet loss中，Loss定义如下： <span class="math display">\[L_{hard}(A_i,P_i,N_{i,k})= w_{hard}(A_i,P_i,N_{i,k})*log(1+exp(d_p(i)-d_n(i,k)))\]</span> 其中<span class="math inline">\(A_i\)</span>，<span class="math inline">\(P_i\)</span>和<span class="math inline">\(N_{i,k}\)</span>分别表示anchor，positive exemplar和 k-th negative exemplar。运用距离修正逻辑回归估计当前三元组的难度，根据<span class="math inline">\(gap(i,k)=d_n(i,k)-d_p(i)\)</span>的大小计算<span class="math inline">\(w_{hard}(A_i,P_i,N_{i,k})\)</span>，最终定义式如下： <img src="https://img-blog.csdnimg.cn/20200318160521251.png" srcset="/img/loading.gif" alt="困难样本权重"></li><li><p><strong>Orientation regression</strong> <br> 在现有的跨视图地理定位基准数据集中，锚点的方向及其对应的正样本在训练集中固定，而在测试集中打乱。所以文章随机旋转航空图像得到的不同角度可以作为训练的标签。为了解决方向未知的问题，在框架中增加一个方向回归，本文将重新分配权值的方向回归损失定义为： <span class="math display">\[L_{OR}(A_i,P_i,N_{i,k})= w_{hard}(A_i,P_i,N_{i,k})*(d^1_R(i)+d^2_R(i))\]</span> 其中<span class="math inline">\(d^1_R(i)\)</span>和<span class="math inline">\(d^2_R(i)\)</span> 分别表示sin和cosine值的回归误差。</p><p><font color="red"><strong>总的误差定义式如下：</strong></font> <span class="math display">\[L_{HER}(A_i,P_i,N_{i,k})= \lambda_1*L_{hard}(A_i,P_i,N_{i,k})+\lambda_2*L_{OR}(A_i,P_i,N_{i,k})\]</span></p></li></ul><h4 id="三dataset">三、Dataset</h4><ul><li><strong>CVUSA dataset</strong> <br> 包含35532个用于训练的地空图像对和8884个用于测试的图像对。地面图像为全景图像，并且地空图均为高分辨率。</li><li><strong>Vo and Hays’ (VH) dataset</strong> <br> 包含超过100万张由美国11个不同城市的谷歌地图收集的交叉视图图像对。使用8个子集作为训练集，并使用从Denver, Detroit, 和 Seattle捕获的其余3个子集进行评估，所有街道视图查询图像裁剪为固定大小230×230。另外，训练集中的方位角是固定的，测试集中是未知的。 <img src="https://img-blog.csdnimg.cn/20200318165049147.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="datasets"></li></ul><h4 id="四总结">四、总结</h4><ul><li><p><strong>Results</strong> <br> 1、匹配结果中top1%的recall达到了98.3%，但是top10左右的只有70%~80%；</p><p>2、性能排序：this approach&gt;add soft-margin ranking loss&gt; add additional OR&gt;pasitive pairs Euclidean loss;</p><p>3、CVUSA dataset&gt;VH dataset，因为前者是全景图像且分辨率高，包含的信息多；</p><p>4、直接学习方向的不变性作为辅助，比把所有图像特征聚集起来要好。</p></li><li><p><strong>Problems</strong> <br> 1、数据集的问题，放在正视图像上效果不太好；</p><p>2、网络有点复杂；</p><p>3、总的损失中权重<span class="math inline">\(\lambda\)</span>如何取值，对结果的影响。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>即使定位论文</category>
      
      <category>添加注意力机制</category>
      
    </categories>
    
    
    <tags>
      
      <tag>即使定位</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch学习记录</title>
    <link href="/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="课程名pytorch-动态神经网络">课程名：《Pytorch 动态神经网络》</h2><p>课程来源：<a href="https://www.bilibili.com/video/av15997678?p=35" target="_blank" rel="noopener">here</a><br>作者：莫烦</p><h2 id="day01-安装pytorch">day01 安装Pytorch</h2><p>前提：安装Anaconda参考别人的安装教程<a href="https://www.jianshu.com/p/742dc4d8f4c5" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/742dc4d8f4c5</a> - 在开始菜单找到Anaconda的命令提示行(Anaconda Prompt)，并输入conda create -n pytorch python=3.7(我自己的是3.7版本)，建立一个Pytorch的环境： <img src="https://img-blog.csdnimg.cn/20200318180532330.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 然后，出现以下情况，问是否安装等等工具包，选择[y]开始安装： <img src="https://img-blog.csdnimg.cn/20200318180734340.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 安装成功以后，会出现如下，就是激活环境的语句： <img src="https://img-blog.csdnimg.cn/20200318180919400.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入conda activate pytorch进入pytorch环境： <img src="https://img-blog.csdnimg.cn/20200318181124374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入pip list可以查看这个环境下的工具包，可以看到没有需要的pytorch，所以需要安装： <img src="https://img-blog.csdnimg.cn/2020031818131941.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_12,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 去官网查看自己适合哪个版本，比如我的是CPU，习惯用pip，如下图：<img src="https://img-blog.csdnimg.cn/20200322091319185.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">pip install torch<span class="op">==</span><span class="dv">1</span>.<span class="fl">4.0</span><span class="op">+</span>cpu torchvision<span class="op">==</span><span class="dv">0</span>.<span class="fl">5.0</span><span class="op">+</span>cpu <span class="op">-</span>f https:<span class="op">//</span>download.pytorch.org<span class="op">/</span>whl<span class="op">/</span>torch_stable.html</a></code></pre></div><ul><li>完了以后，在pytorch环境中进入&gt;&gt;python，测试一下是否安装成功，输入import torch即可。</li></ul><h2 id="day02">day02</h2><h3 id="一神经网络简介">一、神经网络简介</h3><p>1、 机器学习—梯度下降机制(optimization) <br> 2、神经网络黑盒：输入端-黑盒-输出端； <br> 　　　　　　黑盒：特征代表输入数据。</p><h3 id="二why-pytorch">二、why Pytorch？</h3><p>1、与tensorflow的区别</p><ul><li><p>tensorflow是静态的框架，构建好tensorflow的计算图之后，这个计算图是不能改变的，计算流程是固定的，类似C++，写代码时要用他自己的一些API。缺点之一例如训练的时候loss一直将不下来，模型很难得到优化，debug就很困难。</p></li><li><p>pytorch是动态的框架，和python一样，直接计算，不用开启会话。 <br></p></li></ul><h3 id="三variable变量">三、Variable变量</h3><ul><li>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置，里面的值会不停的变化，就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那里面的鸡蛋就是 Torch 的 Tensor 。</li><li>PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</li><li><p>from torch.autograd import Variable</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">ten<span class="op">=</span>torch.FloatTensor([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>]]) <span class="co"># tensor的类型</span></a><a class="sourceLine" id="cb2-2" data-line-number="2">variable<span class="op">=</span>Variable(tensor,requires_grad<span class="op">=</span><span class="va">True</span>) </a><a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co"># 将tensor传给variable，需要Variable来建立一个计算图纸，把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度，如果要就会计算Variable节点的梯度</span></a><a class="sourceLine" id="cb2-4" data-line-number="4">t_out <span class="op">=</span> torch.mean(ten<span class="op">*</span>ten) <span class="co"># 计算x^2</span></a><a class="sourceLine" id="cb2-5" data-line-number="5">v_out <span class="op">=</span> torch.mean(variable<span class="op">*</span>variable)</a><a class="sourceLine" id="cb2-6" data-line-number="6"></a><a class="sourceLine" id="cb2-7" data-line-number="7">v_out.backward() <span class="co"># v_outbackward时，variable也会变化，因为是一体的</span></a><a class="sourceLine" id="cb2-8" data-line-number="8"><span class="bu">print</span>(variable)</a><a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">#直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图),所以我们要转换一下, 将它变成 tensor 形式</span></a><a class="sourceLine" id="cb2-10" data-line-number="10"><span class="bu">print</span>(variable.data)</a><a class="sourceLine" id="cb2-11" data-line-number="11"><span class="bu">print</span>(variable.data.numpy())<span class="co"># variable.data为tensor的形式，tensor才能转换为numpy形式</span></a></code></pre></div></li><li>autograd根据用户对Variable的操作构建其计算图，这个图将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候， 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力。</li><li>variable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li><li>多次反向传播时，<font color="red">梯度是累加的</font>。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li><li><p>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</p></li></ul><h2 id="day-03">day 03</h2><h3 id="一激励函数activation">一、激励函数（Activation）</h3><ul><li><p>什么是Activation 非线性的函数激活网络的输出：ReLU、Sigmoid、Tanh、Softplus</p></li><li><p>Torch中的激励函数</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"></a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a><a class="sourceLine" id="cb3-3" data-line-number="3"><span class="im">from</span> torch.autograd <span class="im">import</span> Variable</a><a class="sourceLine" id="cb3-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a><a class="sourceLine" id="cb3-5" data-line-number="5"></a><a class="sourceLine" id="cb3-6" data-line-number="6">x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># x data (tensor), shape=(100, 1)</span></a><a class="sourceLine" id="cb3-7" data-line-number="7">x <span class="op">=</span> Variable(x)</a><a class="sourceLine" id="cb3-8" data-line-number="8">x_np <span class="op">=</span> x.data.numpy()   <span class="co"># numpy 数据才能用来画图</span></a><a class="sourceLine" id="cb3-9" data-line-number="9"></a><a class="sourceLine" id="cb3-10" data-line-number="10">y_relu <span class="op">=</span> torch.relu(x).data.numpy()</a><a class="sourceLine" id="cb3-11" data-line-number="11">y_sigmoid <span class="op">=</span> torch.sigmoid(x).data.numpy()</a><a class="sourceLine" id="cb3-12" data-line-number="12">y_tanh <span class="op">=</span> torch.tanh(x).data.numpy() <span class="co"># 计算出非线性函数输出后也要转化为numpy数据</span></a><a class="sourceLine" id="cb3-13" data-line-number="13"><span class="co"># y_softplus = F.softplus(x).data.numpy()   </span></a><a class="sourceLine" id="cb3-14" data-line-number="14"></a><a class="sourceLine" id="cb3-15" data-line-number="15"><span class="co">#画图</span></a><a class="sourceLine" id="cb3-16" data-line-number="16">plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</a><a class="sourceLine" id="cb3-17" data-line-number="17">plt.subplot(<span class="dv">221</span>)</a><a class="sourceLine" id="cb3-18" data-line-number="18">plt.plot(x_np, y_relu, c<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</a><a class="sourceLine" id="cb3-19" data-line-number="19">plt.ylim((<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>))</a><a class="sourceLine" id="cb3-20" data-line-number="20">plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>)</a></code></pre></div></li><li><p>结果 <img src="https://img-blog.csdnimg.cn/2020032211361126.png?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li></ul><h3 id="二regression回归">二、Regression回归</h3><p>直接放莫老师教的代码过来：</p><ul><li><p>Layer图搭建以及计算流程</p><div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">class</span> Net(torch.nn.Module):   <span class="co"># torch.nn.Module是Net的主模块</span></a><a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output): <span class="co"># 搭建层所需要的信息</span></a><a class="sourceLine" id="cb4-3" data-line-number="3">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()  <span class="co"># 继承Net的模块功能</span></a><a class="sourceLine" id="cb4-4" data-line-number="4">        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></a><a class="sourceLine" id="cb4-5" data-line-number="5">        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></a><a class="sourceLine" id="cb4-6" data-line-number="6"></a><a class="sourceLine" id="cb4-7" data-line-number="7">    <span class="kw">def</span> forward(<span class="va">self</span>, x): <span class="co"># 前向传递的过程，搭流程图</span></a><a class="sourceLine" id="cb4-8" data-line-number="8">           x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))      <span class="co"># activation function for hidden layer</span></a><a class="sourceLine" id="cb4-9" data-line-number="9">           x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></a><a class="sourceLine" id="cb4-10" data-line-number="10">     <span class="cf">return</span> x</a></code></pre></div></li><li><p>定义Net</p><div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">net <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">1</span>)     <span class="co"># define</span></a></code></pre></div></li><li><p>优化神经网络(torch.optim.),以及loss function定义</p><div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">optimizer <span class="op">=</span> torch.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)</a><a class="sourceLine" id="cb6-2" data-line-number="2">loss_func <span class="op">=</span> torch.nn.MSELoss() <span class="co"># 均方差作为loss</span></a></code></pre></div></li><li><p>开始训练</p><div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</a><a class="sourceLine" id="cb7-2" data-line-number="2">    prediction <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a><a class="sourceLine" id="cb7-3" data-line-number="3"></a><a class="sourceLine" id="cb7-4" data-line-number="4">    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></a><a class="sourceLine" id="cb7-5" data-line-number="5"></a><a class="sourceLine" id="cb7-6" data-line-number="6">    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></a><a class="sourceLine" id="cb7-7" data-line-number="7">    loss.backward()         <span class="co"># backpropagation, compute gradients</span></a><a class="sourceLine" id="cb7-8" data-line-number="8">    optimizer.step()        <span class="co"># apply gradients</span></a><a class="sourceLine" id="cb7-9" data-line-number="9">    <span class="co"># 以上三步为优化步骤</span></a></code></pre></div></li></ul><h3 id="三-classification-分类">三、 Classification 分类</h3><p>　与上面不同的是：</p><ul><li>构造的伪数据不一样，是包含有对应标签的数据；(数据不能是一维)</li><li>网络输入输出不同，有两个输入两个输出；</li><li>loss用到的是交叉熵cross entropy loss，out与标签y</li></ul><div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">loss_func <span class="op">=</span> torch.nn.CrossEntropyLoss() </a><a class="sourceLine" id="cb8-2" data-line-number="2">loss <span class="op">=</span> loss_func(out, y)</a></code></pre></div><ul><li><p>output是取值，转换成概率值需要加softmax(out)</p><div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">out <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a><a class="sourceLine" id="cb9-2" data-line-number="2">prediction <span class="op">=</span> F.softmax(out)  <span class="co">#将输出对应值转化成概率</span></a></code></pre></div></li></ul><h3 id="四快速搭建网络">四、快速搭建网络</h3><ul><li><p>method1—搭建网络、流程图，定义网络</p><div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">class</span> Net(torch.nn.Module):</a><a class="sourceLine" id="cb10-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</a><a class="sourceLine" id="cb10-3" data-line-number="3">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a><a class="sourceLine" id="cb10-4" data-line-number="4">        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></a><a class="sourceLine" id="cb10-5" data-line-number="5">        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></a><a class="sourceLine" id="cb10-6" data-line-number="6"></a><a class="sourceLine" id="cb10-7" data-line-number="7">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a><a class="sourceLine" id="cb10-8" data-line-number="8">        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))      <span class="co"># activation function for hidden layer</span></a><a class="sourceLine" id="cb10-9" data-line-number="9">        x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></a><a class="sourceLine" id="cb10-10" data-line-number="10">        <span class="cf">return</span> x</a><a class="sourceLine" id="cb10-11" data-line-number="11"></a><a class="sourceLine" id="cb10-12" data-line-number="12">net1 <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">2</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">2</span>)  </a></code></pre></div></li><li><p>method2—利用torch.nn.Sequential直接定义网络</p><div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1">net2<span class="op">=</span>torch.nn.Sequential(</a><a class="sourceLine" id="cb11-2" data-line-number="2">    torch.nn.Linear(<span class="dv">2</span>,<span class="dv">10</span>),</a><a class="sourceLine" id="cb11-3" data-line-number="3">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb11-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>,<span class="dv">2</span>)</a><a class="sourceLine" id="cb11-5" data-line-number="5">)</a></code></pre></div></li></ul><h3 id="五网络的保存和提取">五、网络的保存和提取</h3><ul><li><p>方法1：保存—提取</p><div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">torch.save(net1, <span class="st">&#39;net.pkl&#39;</span>) <span class="co"># 保存整个网络，以pkl形式保存</span></a></code></pre></div><div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1">net2 <span class="op">=</span> torch.load(<span class="st">&#39;net.pkl&#39;</span>)</a><a class="sourceLine" id="cb13-2" data-line-number="2">prediction <span class="op">=</span> net2(x)</a></code></pre></div></li><li><p>方法2：保存—提取</p><div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">torch.save(net1.state_dict(), <span class="st">&#39;net_params.pkl&#39;</span>) <span class="co"># 只保存网络中节点的参数 (速度快, 占内存少)</span></a></code></pre></div><div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1">net3 <span class="op">=</span> torch.nn.Sequential(</a><a class="sourceLine" id="cb15-2" data-line-number="2">    torch.nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a><a class="sourceLine" id="cb15-3" data-line-number="3">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb15-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a><a class="sourceLine" id="cb15-5" data-line-number="5">)</a><a class="sourceLine" id="cb15-6" data-line-number="6">net3.load_state_dict(torch.load(<span class="st">&#39;net_params.pkl&#39;</span>))</a><a class="sourceLine" id="cb15-7" data-line-number="7">prediction <span class="op">=</span> net3(x)</a></code></pre></div></li></ul><h3 id="六批数据训练mini_batch-training">六、批数据训练(mini_batch training)</h3><ul><li><p>将数据分批训练，一个epoch训练所有批次的数据：</p><div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a><a class="sourceLine" id="cb16-2" data-line-number="2">BATCH_SIZE <span class="op">=</span> <span class="dv">5</span> <span class="co"># 抽取训练的数据</span></a><a class="sourceLine" id="cb16-3" data-line-number="3"><span class="co"># BATCH_SIZE = 8</span></a><a class="sourceLine" id="cb16-4" data-line-number="4"></a><a class="sourceLine" id="cb16-5" data-line-number="5">x <span class="op">=</span> torch.linspace(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">10</span>)       <span class="co"># this is x data (torch tensor)</span></a><a class="sourceLine" id="cb16-6" data-line-number="6">y <span class="op">=</span> torch.linspace(<span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">10</span>)       <span class="co"># this is y data (torch tensor)</span></a><a class="sourceLine" id="cb16-7" data-line-number="7"></a><a class="sourceLine" id="cb16-8" data-line-number="8">torch_dataset <span class="op">=</span> Data.TensorDataset(data_tensor <span class="op">=</span> x, target_tensor <span class="op">=</span> y)</a><a class="sourceLine" id="cb16-9" data-line-number="9">loader <span class="op">=</span> Data.DataLoader(</a><a class="sourceLine" id="cb16-10" data-line-number="10">    dataset<span class="op">=</span>torch_dataset,      <span class="co"># torch TensorDataset format</span></a><a class="sourceLine" id="cb16-11" data-line-number="11">    batch_size<span class="op">=</span>BATCH_SIZE,      <span class="co"># mini batch size</span></a><a class="sourceLine" id="cb16-12" data-line-number="12">    shuffle<span class="op">=</span><span class="va">True</span>,               <span class="co"># random shuffle for training</span></a><a class="sourceLine" id="cb16-13" data-line-number="13">    num_workers<span class="op">=</span><span class="dv">2</span>,              <span class="co"># 多线程来读数据</span></a><a class="sourceLine" id="cb16-14" data-line-number="14">)</a><a class="sourceLine" id="cb16-15" data-line-number="15"></a><a class="sourceLine" id="cb16-16" data-line-number="16"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):   <span class="co"># 训练所有!整套!数据 3 次</span></a><a class="sourceLine" id="cb16-17" data-line-number="17">    <span class="cf">for</span> step, (batch_x, batch_y) <span class="kw">in</span> <span class="bu">enumerate</span>(loader):  <span class="co"># 每一步 loader 释放一小批数据用来学习</span></a><a class="sourceLine" id="cb16-18" data-line-number="18">        <span class="co"># 假设这里就是你训练的地方...</span></a><a class="sourceLine" id="cb16-19" data-line-number="19">        <span class="co"># 打出来一些数据</span></a><a class="sourceLine" id="cb16-20" data-line-number="20">        <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| Step: &#39;</span>, step, <span class="st">&#39;| batch x: &#39;</span>,</a><a class="sourceLine" id="cb16-21" data-line-number="21">              batch_x.numpy(), <span class="st">&#39;| batch y: &#39;</span>, batch_y.numpy())</a></code></pre></div></li><li><p>DataLoader 是PyTorch中数据读取的接口，PyTorch训练模型基本都会用到该接口，其目的：将dataset根据batch_size大小、shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。</p></li><li><p>enumerate()函数 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。在这里就是把是个数据分成size为5的两份数据后，将每一份数据对应的下标给step，数据给(batch_x, batch_y)。</p></li></ul><h2 id="day-04">day 04</h2><h3 id="一优化器optimizer加速神经网络训练深度学习">一、优化器Optimizer加速神经网络训练（深度学习）</h3><ul><li>数据分批送入网络，进行SGD优化；</li><li>Momentum更新参数方法：<span class="math inline">\(m=b1*m-Learningrate*dx,W+=m\)</span></li><li>AdaGrad：<span class="math inline">\(v+=dx^2，W+=-Learning rate*dx/\sqrt v\)</span></li><li>RMSProp方法(上述两种的合并)：<span class="math inline">\(v=b1*v+(1-b1)*dx^2,W+=-Learning_rate*dx/\sqrt v\)</span></li><li>Adam:<span class="math inline">\(m = b1*m+(1-b1)*dx\)</span>——&gt;Momentum 　　　 <span class="math inline">\(v = b2*v+(1-b2)*dx^2\)</span>——&gt;AdaGrad 　　　 <span class="math inline">\(W+=-Learning_rate*m/\sqrt v\)</span></li></ul><h3 id="二opttimizer优化器">二、Opttimizer优化器 　</h3><ul><li><p>几种常见优化器： 　</p><div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># different optimizers</span></a><a class="sourceLine" id="cb17-2" data-line-number="2">opt_SGD <span class="op">=</span> torch.optim.SGD(net_SGD.parameters(), lr<span class="op">=</span>LR)</a><a class="sourceLine" id="cb17-3" data-line-number="3">opt_Momentum <span class="op">=</span> torch.optim.SGD(net_Momentum.parameters(), lr<span class="op">=</span>LR, momentum<span class="op">=</span><span class="fl">0.8</span>)</a><a class="sourceLine" id="cb17-4" data-line-number="4">opt_RMSprop <span class="op">=</span> torch.optim.RMSprop(net_RMSprop.parameters(), lr<span class="op">=</span>LR, alpha<span class="op">=</span><span class="fl">0.9</span>)</a><a class="sourceLine" id="cb17-5" data-line-number="5">opt_Adam <span class="op">=</span> torch.optim.Adam(net_Adam.parameters(), lr<span class="op">=</span>LR, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.99</span>))</a><a class="sourceLine" id="cb17-6" data-line-number="6">optimizers <span class="op">=</span> [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</a></code></pre></div></li></ul><p><img src="https://img-blog.csdnimg.cn/20200323135943525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 　　从图中可以看出，目前性能最优的应该是Adam。</p><h3 id="三-卷积神经网络cnn">三、 卷积神经网络(CNN)</h3><p>　　图像处理中，不是对每个像素点卷积处理，而是对一小块区域进行计算，这样加强了图像信息的连续性，使得神经网络能看到图片信息而非一个点，同时加深了神经网络对图片的理解。批量过滤器每次对图像收集一小块信息，最后将这些整理出来得到边缘信息，再对这些信息进行类似的处理，得到更高层的信息结构(例如眼睛、鼻子等)，最后把总结出来的信息套入几层full connection进行分类等操作。卷积操作时，神经层会丢失一些信息，池化层可以将Layer中有用的信息筛选出来给下一层，因此图片的长宽不断压缩，压缩的工作是池化层进行的。</p><div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1">1、import需要的工具包和库：torch、torchvision、torch.nn、torch.utils.data</a><a class="sourceLine" id="cb18-2" data-line-number="2">2、超参数：EPOCH、BATCH_SIZE、LR</a><a class="sourceLine" id="cb18-3" data-line-number="3">3、下载mnist数据集：torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;./mnist/&#39;</span>,train<span class="op">=</span><span class="va">True</span>,transform<span class="op">=</span>torchvision.transform.ToTensor(),download<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb18-4" data-line-number="4"> <span class="co">#root是保存或提取的位置，transform是将数据集PIL.Image or numpy.ndarray转换成torch.FloatTensor(C×H×W)，训练的时候normalize成[0,1]间的值</span></a><a class="sourceLine" id="cb18-5" data-line-number="5"> test数据集处理：test—_x,test_y</a><a class="sourceLine" id="cb18-6" data-line-number="6"> 4、批训练train_loader定义：Data.DataLoader(dataset<span class="op">=</span>train_data,batch_size<span class="op">=</span>BATCH_SIZE,shuffle<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb18-7" data-line-number="7"> 5、定义网络构架CNN(nn.Module):conv1—conv2—RELU—pooling—conv2—ReLU—pooling—output</a><a class="sourceLine" id="cb18-8" data-line-number="8"> 网络计算流程：conv1(x)——conv2(x)——展平多维卷积图——计算输出</a><a class="sourceLine" id="cb18-9" data-line-number="9"> 6、定义optimizer和loss function</a><a class="sourceLine" id="cb18-10" data-line-number="10"> 7、训练和测试</a></code></pre></div><h3 id="四什么是lstm循环卷积网络rnn">四、什么是LSTM循环卷积网络(RNN)</h3><ul><li>LSTM(Long Short-Term Memory)——长短期记忆</li><li>RNN是在有序的数据上进行学习 <img src="https://img-blog.csdnimg.cn/20200323202037102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>分类问题(mnist数据集) 　我们将图片数据看成一个时间上的连续数据, 每一行的像素点都是这个时刻的输入, 读完整张图片就是从上而下的读完了每行的像素点。然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了。</li><li>回归问题 这部分内容参考<a href="https://blog.csdn.net/qq_41639077/article/details/105218095" target="_blank" rel="noopener">KiKi的另一篇blog</a>，包括分类问题和回归问题的pytorch实现。</li></ul><h3 id="五自编码非监督学习autoencoder">五、自编码/非监督学习（Autoencoder）　</h3><p>　　原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作。 所以, 何不<strong>压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了。</strong> 训练好的自编码中间这一部分就是能总结原数据的精髓，我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。(摘自莫烦python) * 代码</p><pre><code>```pythonself.encoder = nn.Sequential(    nn.Linear(28*28, 128),    nn.Tanh(),    nn.Linear(128, 64),    nn.Tanh(),    nn.Linear(64, 12),    nn.Tanh(),    nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt)self.decoder = nn.Sequential(    nn.Linear(3, 12),    nn.Tanh(),    nn.Linear(12, 64),    nn.Tanh(),    nn.Linear(64, 128),    nn.Tanh(),    nn.Linear(128, 28*28),    nn.Sigmoid(),       # compress to a range (0, 1))def forward(self, x):    encoded = self.encoder(x)    decoded = self.decoder(encoded)    return encoded, decodedautoencoder = AutoEncoder()```</code></pre><h3 id="六gan生成对抗网络">六、GAN—生成对抗网络</h3><p>　（原理已经学习过了，直接上代码）</p><ul><li><p>pytorch中实现：(代码中的对象不是图像，用到的是二次曲线) 　<br></p></li><li><p>超参数</p><pre><code>  ```python  BATCH_SIZE = 64  LR_G = 0.0001 # 生成器的学习率  LR_D = 0.0001 # 判别器的学习率  N_IDEAS = 5 # random_noise的个数  ART_COMPONENTS = 15  # 定义规格，一条曲线上有多少个点  PAINT_POINTS = np.vstack([np.linspace(-1,1,ART_COMPONENTS)for _ in range(BATCH_SIZE)]) # 规定整批画的点，从-1到1共15个点  ``` </code></pre></li><li><p>没有train data，自己伪造一些real data</p><pre><code>  ```python  def artist_works():     # painting from the famous artist (real target)      a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis] # 二次曲线的系数      paintings = a * np.power(PAINT_POINTS, 2) + (a-1)  # 二次曲线的参数，区间表示upper和      paintings = torch.from_numpy(paintings).float()      return paintings  ```     </code></pre></li><li><p>定义生成器和判别器</p><pre><code>  ```python  G = nn.Sequential(                      # Generator      nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)      nn.ReLU(),      nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas  )  D = nn.Sequential(                      # Discriminator      nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G      nn.ReLU(),      nn.Linear(128, 1),      nn.Sigmoid(),                       # tell the probability that the art work is made by artist  )  ```</code></pre></li><li><p>优化器</p><pre><code>  ```python      opt_D = torch.optim.Adam(D.parameters(), lr=LR_D)  opt_G = torch.optim.Adam(G.parameters(), lr=LR_G)  ```</code></pre></li><li><p>训练啦</p><pre><code>  ```python  for step in range(10000):      artist_paintings = artist_works()           # real painting from artist      G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas      G_paintings = G(G_ideas)                    # fake painting from G (random ideas)      prob_artist0 = D(artist_paintings)          # D try to increase this prob      prob_artist1 = D(G_paintings)               # D try to reduce this prob      D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))      G_loss = torch.mean(torch.log(1. - prob_artist1))      opt_D.zero_grad()      D_loss.backward(retain_graph=True)      # reusing computational graph      opt_D.step()      opt_G.zero_grad()      G_loss.backward()      opt_G.step()  ```</code></pre></li></ul><p><strong>补充：</strong> cGAN与GAN的区别在于多了一个类别标签，这个label会跟随noise一起输入到生成器中，并且也要跟随fake和real一起输入到判别其中，最终计算各自的loss。</p><h3 id="七为什么torch是动态的待补充">七、为什么Torch是动态的<font color="red">(待补充)</font></h3><p>　例子：RNN网络 　Tensorflow就是预先定义好要做的task的框架、步骤，然后开启会话之后喂数据一步到位的计算出结果，开启会话后便不能修改网络构架了，只能是照着计算流图跟着计算，所以是静态的；Torch也可以先定义好框架然后套进去，但计算的时候无论网络怎么变化每一个叶子节点的梯度都能给出，tensorflow就做不到这一点，并且torch是边给出计算图纸一边进行训练。torch就像是散装的一样，可以一块一块的制作好并进行计算，比较灵活，所以是动态的。</p><h3 id="八gpu加速">八、GPU加速</h3><p>　以之前CNN为例，对其代码进行修改</p><ul><li><p>dataset部分</p><div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1">test_x <span class="op">=</span> torch.unsqueeze(test_data.test_data, dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(torch.FloatTensor).cuda()<span class="op">/</span><span class="fl">255.</span>   <span class="co"># Tensor on GPU</span></a><a class="sourceLine" id="cb25-2" data-line-number="2">test_y <span class="op">=</span> test_data.test_labels.cuda()</a></code></pre></div></li><li><p>CNN网络的参数改为GPU兼容形式</p><div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">class</span> CNN(nn.Module):</a><a class="sourceLine" id="cb26-2" data-line-number="2">    ...</a><a class="sourceLine" id="cb26-3" data-line-number="3">cnn <span class="op">=</span> CNN()</a><a class="sourceLine" id="cb26-4" data-line-number="4"><span class="co">##########转换cnn到CUDA#########</span></a><a class="sourceLine" id="cb26-5" data-line-number="5">cnn.cuda()   <span class="co"># Moves all model parameters and buffers to the GPU.</span></a></code></pre></div></li><li><p>training data变成GPU形式</p><div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="cf">for</span> epoch ..:</a><a class="sourceLine" id="cb27-2" data-line-number="2">    <span class="cf">for</span> step, ...:</a><a class="sourceLine" id="cb27-3" data-line-number="3">        <span class="co">##########修改1###########</span></a><a class="sourceLine" id="cb27-4" data-line-number="4">        b_x <span class="op">=</span> x.cuda()    <span class="co"># Tensor on GPU</span></a><a class="sourceLine" id="cb27-5" data-line-number="5">        b_y <span class="op">=</span> y.cuda()    <span class="co"># Tensor on GPU</span></a><a class="sourceLine" id="cb27-6" data-line-number="6">        ...</a><a class="sourceLine" id="cb27-7" data-line-number="7"></a><a class="sourceLine" id="cb27-8" data-line-number="8">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a><a class="sourceLine" id="cb27-9" data-line-number="9">            test_output <span class="op">=</span> cnn(test_x)</a><a class="sourceLine" id="cb27-10" data-line-number="10"></a><a class="sourceLine" id="cb27-11" data-line-number="11">            <span class="co"># !!!!!!!! 修改2  !!!!!!!!! #</span></a><a class="sourceLine" id="cb27-12" data-line-number="12">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a><a class="sourceLine" id="cb27-13" data-line-number="13"></a><a class="sourceLine" id="cb27-14" data-line-number="14">            accuracy <span class="op">=</span> torch.<span class="bu">sum</span>(pred_y <span class="op">==</span> test_y) <span class="op">/</span> test_y.size(<span class="dv">0</span>)</a><a class="sourceLine" id="cb27-15" data-line-number="15">            ...</a><a class="sourceLine" id="cb27-16" data-line-number="16"></a><a class="sourceLine" id="cb27-17" data-line-number="17">test_output <span class="op">=</span> cnn(test_x[:<span class="dv">10</span>])</a><a class="sourceLine" id="cb27-18" data-line-number="18"></a><a class="sourceLine" id="cb27-19" data-line-number="19"><span class="co"># !!!!!!!! 修改3 !!!!!!!!! #</span></a><a class="sourceLine" id="cb27-20" data-line-number="20">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a><a class="sourceLine" id="cb27-21" data-line-number="21">...</a><a class="sourceLine" id="cb27-22" data-line-number="22"><span class="bu">print</span>(test_y[:<span class="dv">10</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div></li></ul><h2 id="day-05">day 05</h2><h3 id="一过拟合overfitting">一、过拟合(Overfitting)</h3><ul><li><p>过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。模型在训练集上效果好，然而在测试集上效果差，模型泛化能力差。</p></li><li><p>原因 <br> 　1）在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候； <br> 　2）权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。 　</p></li><li><p>解决方法</p><p>方法一： <strong>增加数据量</strong>。 <br> 方法二：<strong>运用正规化</strong>，L1、 L2 regularization等等。（神经网络的正规化方法<strong>dropout</strong>——就是在训练的时候, 随机忽略掉一些神经元和神经联结 , 使这个神经网络变得”不完整”，用这个不完整的神经网络训练一次。第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象每次训练的时候, 让每一次预测结果不会依赖于其中某部分特定的神经元。像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数，Dropout 的做法是从根本上让神经网络没机会过度依赖。）</p></li><li><p>Dropout</p><div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># 不加dropout的网络</span></a><a class="sourceLine" id="cb28-2" data-line-number="2">net_overfitting <span class="op">=</span> torch.nn.Sequential(</a><a class="sourceLine" id="cb28-3" data-line-number="3">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a><a class="sourceLine" id="cb28-4" data-line-number="4">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-5" data-line-number="5">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a><a class="sourceLine" id="cb28-6" data-line-number="6">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-7" data-line-number="7">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a><a class="sourceLine" id="cb28-8" data-line-number="8">    )</a><a class="sourceLine" id="cb28-9" data-line-number="9"></a><a class="sourceLine" id="cb28-10" data-line-number="10">    <span class="co"># 加上dropout</span></a><a class="sourceLine" id="cb28-11" data-line-number="11">net_dropped <span class="op">=</span> torch.nn.Sequential(</a><a class="sourceLine" id="cb28-12" data-line-number="12">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a><a class="sourceLine" id="cb28-13" data-line-number="13">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a><a class="sourceLine" id="cb28-14" data-line-number="14">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-15" data-line-number="15">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a><a class="sourceLine" id="cb28-16" data-line-number="16">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a><a class="sourceLine" id="cb28-17" data-line-number="17">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-18" data-line-number="18">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a><a class="sourceLine" id="cb28-19" data-line-number="19">    )＃除了网络构架不同外，其他大同小异。</a></code></pre></div><p>　 　</p></li></ul><h3 id="二批标准化batch-normalization">二、批标准化(Batch Normalization)</h3><ul><li><p>什么是批标准化 <br> 　Batch Normalization(BN), 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法。 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律。数据随着神经网络的传递计算，激活函数的存在会造成网络层对数据的不敏感，比如0.1和2经过Tanh函数后，前者仍0.1，而2变成1，那这样再大的数都会变成1，所以神经层对数据失去了感觉，这样的问题同样存在于隐藏层中，所以BN则是用在这些神经层中优化网络的方法。 <br> 　Batch就是数据分批处理，每一批数据前向传递的过程中，每一层都进行BN处理，添加在层和激励函数之间。反BN：<span class="math inline">\(BN_(\gamma,\beta)(x_i)\)</span>是将 normalize 后的数据再扩展和平移，是为了让神经网络自己去学着使用和修改这个扩展参数 <span class="math inline">\(\gamma\)</span>, 和 平移参数<span class="math inline">\(\beta\)</span>, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 <span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>来抵消一些 normalization 的操作。</p></li><li><p>代码 　莫烦<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py" target="_blank" rel="noopener">BN_code</a></p></li></ul><p>　　　　　　<font color="red">部分内容待学习</font></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>pytorch学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN-循环神经网络入门</title>
    <link href="/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/"/>
    <url>/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="循环神经网络入门pytorch实现">循环神经网络入门（pytorch实现）</h2><p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著  参考视频：<a href="https://www.bilibili.com/video/BV134411w76S" target="_blank" rel="noopener">微软人工智能公开课—循环神经网络RNN</a></p><p><strong>一、回顾前馈神经网络</strong> <br> 　　RNN是在前馈式神经网络基础上的，所以先回顾一下前馈神经网络。 <br> 　　前馈式神经网络（FNN, Feedforward Neural Network）一般的结构如下图： 　　<img src="https://img-blog.csdnimg.cn/20200403141749146.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="前馈式神经网络"> 图中神经元的输出表示为：<span class="math inline">\(O_t=f(\sum_{i=1}^n a_iw_i+b)\)</span>假设某一层的输出为<span class="math inline">\(h_n\)</span>，则下一层的输出为<span class="math inline">\(f_{out}(h_n)\)</span>，这样重复迭代网络的输出为<span class="math inline">\(y=f_{out}(f_n(f_{n-1}(...)))\)</span>，如此一来神经网络就成了巨大的复合函数，但实际我们并不会用到这个函数来计算，而使用计算图来表示，这样方便得出每一个节点的输入输出数据的梯度。对于RNN，就从这样的简单的前馈神经网络的传递方式说起。</p><p><strong>二、RNN网络简介</strong> <br> 　　循环神经网络（ Recurrent Neural Network, RNN ）雏形见于美国物理学家 J.J.Hopfield 于 1982 年提出的可用作联想存储器的互联网络——Hopfield 神经网络模型，如下图，每个节点都有输入，两两节点之间有双相连接： 　　<img src="https://img-blog.csdnimg.cn/20200403144156566.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Hopfiled Network"> 　　基于传统的机器学习算法十分依赖人工特征的提取致使模型一直无法提高正确率，而之前的全连神经网络以及卷积神经网络中，信息只在层与层之间存在运算关系，而没有连接节点之间的信息流动，这样在每一个时间都会有一个单独的参数，<font color="red">因此不能在时间上共享不同序列长度或序列不同位置的统计强度，所以无法对训练时没有见过的序列长度进行泛化</font>。 所以发展RNN对具有时间序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，在模型的不同部分共享参数解决了这个问题，并使得模型能够扩展到对不同形式的样本（这里指不同长度的样本）进行泛化。 <strong>循环神经网络会在几个时间步内共享相同的权重以刻画一个序列当前的输出与之前信息的关系，这体现在结构上是循环神经网络的隐藏层之间存在连接，隐藏层的输入来自于输入层的数据以及上一时刻隐藏层的输出</strong>。这样的结构使得循环神经网络会对之前的信息有所记忆， 同时利用之前的信息影响后面节点的输出。 <br> 　　RNN网络主要用于处理离散序列数据：离散线性、长度可变的序列，例如时域语音信号，金融市场走势等。网络可用于序列数据的分析（市场趋势预测）、序列数据的生产（基于图片的文字描述）、序列数据的转换（语音识别以及机器翻译）。</p><p><strong>三、循环网络结构</strong> <br> 　 　循环神经网络典型结构及其按时间先后展开结构如图所示：　 　 　<img src="https://img-blog.csdnimg.cn/20200403160334980.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="RNN"> 对于主体结构<span class="math inline">\(A\)</span>，一般可认为是循环神经网络的一个隐藏单元。在<span class="math inline">\(t\)</span>时刻，主体结构<span class="math inline">\(A\)</span>会读取输入层的输入<span class="math inline">\(x_t\)</span>以及上一时刻的输出，并输出当前时刻的<span class="math inline">\(o_t\)</span>值（图中未给出）。此后<span class="math inline">\(A\)</span>结构在<span class="math inline">\(t\)</span>时刻的状态值表达式：<span class="math inline">\(h^t=f(h^{t-1},x_t;\theta)\)</span>，其中<span class="math inline">\(\theta\)</span>可以是网络中的其它参数比如权重或偏置等，循环的过程就是<span class="math inline">\(A\)</span>不断被执行的过程。但是循环神经网络目前无法做到无限循环，因为循环过多会出现梯度消失的问题。 <br> 　　假设隐藏单元的激活函数是tanh函数，则循环体结构<span class="math inline">\(A\)</span>如下：　　<img src="https://img-blog.csdnimg.cn/20200403155812724.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="循环体"> 则隐藏单元状态值表示为：<span class="math inline">\(h^t=tanh(Wh^{t-1}+Ux_t+b^h)\)</span>，<span class="math inline">\(b^h\)</span>是由<span class="math inline">\(x_t\)</span>得到<span class="math inline">\(h^t\)</span>的偏置，<span class="math inline">\(W\)</span>是相邻时刻隐藏单元间的权重矩阵，<span class="math inline">\(U\)</span>是从<span class="math inline">\(x_t\)</span>计算得到这个隐藏单元时用到的权重矩阵。从当前的状态值<span class="math inline">\(h^t\)</span>都得到输出还需要一个全连接神经网络来完成这个过程，表达式为：<span class="math inline">\(o^t=b_o+Vh^t\)</span>，<span class="math inline">\(V\)</span>是由<span class="math inline">\(h^t\)</span>得到<span class="math inline">\(o^t\)</span>的权重矩阵。如果输出是离散的，则可以用softmax处理<span class="math inline">\(o^t\)</span>得到标准化的概率向量<span class="math inline">\(y\)</span>：<span class="math inline">\(y^t=softmax(o^t)\)</span>，向量的值对应离散变量可能值的概率。</p><p><strong>四、网络的训练</strong> <br> 　　要对网络进行训练，需要有一个与<span class="math inline">\(x\)</span>序列配对的<span class="math inline">\(o\)</span>的所有时间步内的总loss。 <br> 　　RNN的反向传播算法称为时间反向传播 (Back-Propagation Through Time, BPTT)。基本原理和 BP 算法是一样的，三步走：前向计算每个神经元的输出值；反向计算误差项值；计算每个权重的梯度；最后再用随机梯度下降算法更新权重。详细的计算公式参考<a href="https://zhuanlan.zhihu.com/p/85776566" target="_blank" rel="noopener">知乎</a>。在反向传播过程中，当输入序列过长的时候，在求取一个比较远的时刻的梯度时，需要回溯到前面的所有时刻的信息，由于连乘项的存在，导致前面时刻的信息会缺失，这就是RNN中的梯度消失问题，还有就是当连乘出现大于1时的梯度爆炸，后者采用clip的方式即可，前者将会用到LSTM来缓解。</p><p><strong>五、LSTM</strong> <br> 　　LSTM 结构由 Sepp Hochreiter教授和 Jurgen Schrnidhuber教授于 1997 年 提出，它本身就是一种特殊的循环体结构 。在一个整体的循环神经网络中，除了外部的RNN大循环（<strong>循环体是 LSTM</strong>）外，还需要考虑 LSTM 本身单元“细胞”之间的自循环。LSTM单元结构如下图： 　　<img src="https://img-blog.csdnimg.cn/20200403173928627.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM单元"> LSTM由自身的三个门结构进行控制，门结构使用Sigmoid函数对输入的信息进行控制，让信息有选择性的影响RNN中每个时刻的状态。遗忘门（Forget Gate）是让网络“忘记”之前没用的信息，上一时刻的状态值通过与自循环权重 (该权重就是遗忘门的输出) 进行位乘可得到当前时刻状态值的一个加数。遗忘门输出表达式：</p><p><span class="math display">\[f_i^t=sigmoid(b_i^f+\sum U_{i,j}^fx_j^t+\sum W_{i,j}^fh_j^{t-1})\]</span></p><p>其中，<span class="math inline">\(h^{t-1}\)</span>是包含一个LSTM细胞上一时刻的所有输出，可以被看作是当前隐藏层向量，数量为<span class="math inline">\(j\)</span>，<span class="math inline">\(W\)</span>是循环权重。要保存长期的记忆，还需要输入门，同样是根据<span class="math inline">\(x^t\)</span>、<span class="math inline">\(b^g\)</span>和<span class="math inline">\(h^{t-1}\)</span>来决定哪些部分将进入当前时刻的状态，输入门的值表示为：</p><p><span class="math display">\[g_i^t=sigmoid(b_i^g+\sum U_{i,j}^gx_j^t+\sum W_{i,j}^gh_j^{t-1})\]</span></p><p>进一步计算LSTM结构当前时刻的状态值：</p><p><span class="math display">\[C_i^t=f_i^tC_i^{t-1}+g_i^ttanh(b_i+\sum U_{i,j}x_j^t+\sum W_{i,j}h_j^{t-1})\]</span></p><p>其中，<span class="math inline">\(W\)</span>值为遗忘门的循环权重。然后得出结构当前时刻的输出值：</p><p><span class="math display">\[h^t=tanh(C_i^t)q_i^t\]</span></p><p><span class="math display">\[q_i^t=sigmoid(b_i^q+\sum U_{i,j}^qx_j^t+\sum W_{i,j}^qh_j^{t-1})\]</span></p><p>其中，<span class="math inline">\(W^q\)</span>为遗忘门的循环权重，<span class="math inline">\(q_i^t\)</span>为输出们的输出值。用LSTM作为循环体的RNN网络如下图所示： <img src="https://img-blog.csdnimg.cn/20200403192938761.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM-RNN"></p><p>　　循环神经网络的变种还有：双向循环神经网络和深层循环神经网络。</p><ul><li>至此，关于RNN和LSTM的学习就结束啦~ 用pytorch实现LSTM的RNN对MNIST数据集分类，以及RNN实现一元二次曲线的回归的代码地址：<a href="https://gitee.com/sparklekk/RNN" target="_blank" rel="noopener">sparklekk</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度学习模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>循环神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>总结经典深度学习网络（pytorch、tensorflow实现）</title>
    <link href="/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/"/>
    <url>/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="总结经典的深度学习模型">总结经典的深度学习模型</h2><p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著</p><h3 id="一lenet-5">一、LeNet-5</h3><p><strong>1、LeNet-5网络简介</strong></p><p>　　LeNet-5是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。LeNet-5 模型由Yann LeCun教授于1998年提出， 在MNIST数据集上，LeNet-5模型可以达到大约99.4%的准确率。与近几年的卷积神经网络比较，LeNet-5的网络规模比较小，但却包含了构成现代CNN网络的基本组件——卷积层、 Pooling层、全连接层。 <br> <strong>2、模型结构</strong> <br> 　　网络一共有8层(包含输入和输出在内)，基本网络架构如下： <img src="https://img-blog.csdnimg.cn/20200401163708326.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="ＬｅＮｅｔ－５"> 备注：图中C表示卷积层 ，S表示池化层。　<br></p><ul><li><p><font color="red"><strong>Layer1</strong></font>：Input层，输入图片大小32×32×1，MNIST数据集大小是28×28，所以输入reshape为32×32是希望高层特征监测感受野的中心能够收集更多潜在的明显特征。</p></li><li><p><font color="red"><strong>Layer2</strong></font>：Conv1层——卷积层，有6个Feature Map，即Convolutions操作时的卷积核的个数为6，卷积核大小为5×5，该层每个单元和输入层的25个单元连接，padding没有使用，stride为1。</p></li><li><p><font color="red"><strong>Layer3</strong></font>：S2为下采样层(Subsampling)，6个14×14的特征图，是上一层的每个特征图经过2×2的最大池化操作得到的，且长宽步长都为2，S2层每个特征图的每一个单元与C3对应特征图中2×2大小的区域连接。</p></li><li><p><font color="red"><strong>Layer4</strong></font>：Conv3层，由上一层的特征图经过卷积操作得到，卷积核大小为5×5，卷积核个数为16，即该层有16个特征图，但并不是与上一层的6个特征图一一对应，而是有固定的连接关系，例如第一个特征图只与Layer3的第1、2、3个特征图的有卷积关系。</p></li><li><p><font color="red"><strong>Layer5</strong></font>：S4层，16个5×5的特征图，每个特征图都是由第四层经过一个2×2的最大池化操作得到的，意义同Layer3。</p></li><li><p><font color="red"><strong>Layer6</strong></font>：Conv5，120个特征图，是由上一层输出经过120个大小为5×5的卷积核得到的，没有padding，stride为1，上一层的16个特征图都连接到该层的每一个单元，所以这里相当于一个全连接层。</p></li><li><p><font color="red"><strong>Layer7</strong></font>：F6是全连接层，有84个神经元，与上一层构成全连接的关系，再经由Sigmoid激活函数传到输出层。</p></li><li><p><font color="red"><strong>Layer8</strong></font>：Output层也是一个全连接层，共有10个单元，对应0~9十个数字。本层单元计算的是径向基函数：<span class="math inline">\(y_i =\sum_{j}(x-w_{i,j})^2\)</span>,RBF的计算与第i个数字的比特图编码有关，对于第i个单元，yi的值越接近0，则表示越接近第i个数字的比特编码，即识别当前输入的结果为第i个数字。</p></li></ul><p><strong>3、pytorch和tensorflow实现LeNet-5网络的MNIST手写数字识别</strong> <br> 　　代码地址：<a href="https://github.com/KK-xi/LeNet-5" target="_blank" rel="noopener">GitHub</a> <br> 　　LeNet-5网络规模比较小，所以它无法很好的处理类似ImageNet的比较大的图像数据集。</p><h3 id="二alexnet">二、AlexNet</h3><p>1、<strong>AlexNet网络简介</strong> <br> 　　2012年， Hinton的学生Alex Krizhevsky借助深度学习的相关理论提出了深度卷积神经网络模型AlexNet。卷积层的数量有5个，池化层的数量有3个，也就是说，并不是所有的卷积层后面都连接有池化层。在这些卷积与池化层之后是3个全连层，最后一个全连层的单元数量为1000个，用于完成对ImageNet数据集中的图片完成1000分类（具体分类通过Softmax层实现)。</p><p>2、<strong>模型结构</strong> <br>　　 　　网络结构如下图所示，有两个子网络，可以用<font color="red">GPU分别进行训练</font>(特点1)，两个GPU之间存在通信： 　　<img src="https://img-blog.csdnimg.cn/20200401164504200.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="AlexNet"></p><ul><li><p><font color="red"><strong>第一段卷积</strong></font>: 使用了96个11×11卷积核对输入的224×224×3的RGB图像进行卷积操作，长宽移动步长均为4，得到的结果是96个55x55的特征图；得到基本的卷积数据后，第二个操作是<font color="red">ReLU去线性化</font>(这是AlexNet的特点2)；第三个操作是<font color="red">LRN局部归一化</font>（AlexNet首次提出,特点3)；第四个操作是3×3的max pooling，步长为2。</p></li><li><p><font color="red"><strong>第二段卷积</strong></font>：流程类似第一阶段，用到的核大小不同，256个深度为3(因为是三通道图像)的5×5的卷积核，stride为1×1，然后ReLU去线性化，再是LRN局部归一化，最后是3×3的最大池化操作，步长为2。</p></li><li><p><font color="red"><strong>第三段卷积</strong></font>：输入上一阶段的特征图，首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，这一阶段没有LRN和池化。</p></li><li><p><font color="red"><strong>第四段卷积</strong></font>：首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化。</p></li><li><p><font color="red"><strong>第五段卷积</strong></font>：首先经过256个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，最后是3×3的最大池化操作，步长为2。</p></li><li><p><font color="red"><strong>第六段全连接(FC)</strong></font>：以上过程完了以后经过三层全连接层，前两层有4096个单元，与前一层构成全连接关系，然后都经过一个ReLU函数，左后一层是1000个单元的全连接层(Softmax层)，训练时这几个<font color="red">全连接层使用了Dropout</font>(特点4)。</p></li></ul><p>备注：<font color="red">数据增强的运用</font>(特点5)，在训练的时候模型随机从大小为256×256的原始图像中截取224×224大小的区域，同时还得到图像水平翻转的镜像图，用以增加样本的数量。在测试时，模型会首先截取一张图片的四个角加中间的位置，并进行左右翻转，这样会获得10张图片，将这10张图片作为预测的输入并对得到的10个预测结果求均值，就是这张图片最终的预测结果。</p><p><strong>3、pytorch和tensorflow实现</strong> <br> 　　代码地址：<a href="https://github.com/KK-xi/AlexNet" target="_blank" rel="noopener">Github</a></p><h3 id="三vggnet">三、VGGNet</h3><p><strong>1、VGGNet网络简介</strong> <br> 　　2014年ILSVRC图像分类竞赛的第二名是VGGNet网络模型，其 top-5 错误率为 7.3%，它对卷积神经网络的深度与其性能之间的关系进行了探索。在将网络迁移到其他图片数据上进行应用时， VGGNet比GoogleNet有着更好的泛化性。此外，VGGNet模型是从图像中提取特征的CNN首选算法。<br></p><p><strong>2、模型结构</strong> <br> 　　网络的结构非常简洁，在整个网络中全部使用了大小相同的卷积核 (3×3 )和 最大油化核 (2x2 )。<font color="red">VGGNet模型通过不断地加深网络结构来提升性能</font>，通过重复堆叠的方式，使用这些卷积层和最大池化层成功地搭建了11～19层深的卷积神经网络。下表是这些网络的结构组成层： <img src="https://img-blog.csdnimg.cn/20200401200252309.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGGNet"> 　　表中的conv3表示大小为3×3的卷积核，conv1则是1×1的卷积核，参数量主要集中在全连接层，其他卷积层的参数共享和局部连接降低了参数的数量。表中五个阶段的卷积用于提取特征，每段卷积后面都有最大池化操作，目的是缩小图像尺寸。多个卷积的堆叠可以降低参数量，也有助于学习特征，C级的VGG用到conv1是为了在输入通道数和输出通道数不变(不发生数据降维)的情况下实现线性变换，对非线性提升效果有较好的作用，但是换成conv3效果更好。VGG19的效果只比VGG16好一点点，所以牛津的研究团队就停止在VGG19层了，不再增加更多层数了。下图是VGG-16的网络构架图： 　　<img src="https://img-blog.csdnimg.cn/20200401203149754.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGG-16"></p><p><strong>3、pytorch和tensorflow实现</strong> <br> 　　这里实现的是VGG-16网络(pytorch中只是搭建了网络)，代码地址：<a href="https://github.com/KK-xi/VGGNet-16" target="_blank" rel="noopener">GitHub</a> 　　在通常的网络训练时，<font color="red">VGGNet通过Multi-Scale方法对图像进行数据增强处理</font>，我自己在训练MNIST的时候没有用数据增强。因为卷积神经网络对于图像的缩放有一定的不变性，所以将这种经过 Multi-Scale多尺度缩放裁剪后的图片合在一起输入到卷积神经网络中训练，可以增加网络的这种<font color="red">不变性</font>(不变性的理解：pooling操作时，对局部感受野取其极大值，如果图像在尺度上发生了变化，有一定概率在尺度变化后对应的感受野取到的极大值不变，这样就可以使特征图不变，同样也增加了一定的平移不变性)。在预测时也采用了Multi-Scale的方法，将图像Scale到一个尺寸再裁剪并输入到卷积网络计算。输入到网络中的图片是某一张图片经过缩放裁剪后的多个样本，这样会得到一张图片的多个分类结果，所以紧接着要做的事就是对这些分类结果进行平均以得到最后这张图片的分类结果，这种平均的方式会提高图片数据的利用率并使分类的效果变好。</p><h3 id="四inceptionnet-v3">四、InceptionNet-V3</h3><p><strong>1、InceptionNet-V3网络简介</strong> <br> 　Google的InceptionNet首次亮相是在2014年的ILSVRC比赛中，称为 Inception-V1，后来又开发了三个版本，其中InceptionNet-V3最具代表性。相比VGGNet, Inception-V1增加了深度，达到了22层，但是其参数却只有500万个左右(SM)，这是远低于AlexNet(60M 左右)和VGGNet (140M 左右)的，是因为该网络将全连层和一般的卷积中采用了<font color="red">稀疏连接的方法(Hebbian原理)</font>。根据相关性高的单元应该被聚集在一起的结论，这些在同一空间位置但在不同通道的卷积核的输出结果也是稀疏的，也可以通过类似将稀疏矩阵聚类为较为密集的子矩阵的方式来提高计算性能。沿着这样的一个思路，Google团队提出了Inception Module结构来实现这样的目标。</p><p><strong>2、模型结构</strong> <br> 　　网络中主要用到了稀疏连接的思想提出了<font color="red">Inception Module</font>，其借鉴了论文《Network in Network》的做法，即提出的<font color="red">MLPConv</font>——使用MLP对卷积操作得到的特征图进行进一步的操作，从而得到本层的最终输出特征图，这样可以允许在输出通道之间组合信息，以此提升卷积层的表达能力。在InceptionNet-V3中，Inception Module的基本结构如下图：　　<img src="https://img-blog.csdnimg.cn/20200401233024519.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Inception Module"> 结构中的<font color="red">Filter Concat</font>是将特征图在深度方向(channel维度)进行串联拼接，这样可以构建出符合Hebbian原理的稀疏结构。conv1的作用是把相关性高、同一空间位置不同通道的特征连接在一起，并且计算量小，<strong>可以增加一层特征变换和非线性化</strong>，最大池化是为了增加网络对不同尺度的适应性。在Inception V2中首次用到了<font color="red">.批标准化(Batch Normalization)</font>，V3在V2基础上的Module<font color="red">改进之处就是在分支中使用分支，将二维卷积拆分为两个非对称一维卷积</font>，这种卷积可以在处理更丰富的空间特征以及增加特征多样性等方面做得比普通卷积更好。多个这种Inception Module堆叠起来就形成了InceptionNet-V3，其网络构架如下图，整个网络的主要思想就是找到一个最优的Inception Module结构，更好的实现局部稀疏的稠密化。 <img src="https://img-blog.csdnimg.cn/20200401235114326.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="InceptionNet-V3"></p><p><strong>3、使用InceptionNet-V3完成模型迁移学习</strong></p><ul><li><font color="red">迁移学习</font>：随着卷积神经网络模型层数的加深以及复杂度的逐渐增加，训练这些模型所需要的带有标注的数据也越来越多。比如对于ResNet，其深度有 152 层，使用ImageNet数据集中带有标注的 120 万张图片才能将其训练得到 96.5% 的准确率。尽管这是个比较不错的准确率，但是在真实应用中，几乎很难收集到这么多带有标注的图片数据，而且这些数据训练一个复杂的卷积神经网络也要花费很长的时间 。<strong>迁移学习的出现就是为了解决上述标注数据以及训练时间的问题</strong>。所谓迁移学习，就是将一个问题上训练好的模型通过简单的调整使其适用于 一个新的问题，例如在 Google 提供的基于 ImageNet 数据集训练好的 Inception V3 模型的基础上进行简单的修改，使其能够解决基于其他数据集的图片分类任务。被修改的全连接层之前的一个网络层叫做<font color="red">瓶颈层</font>(Bottleneck，这里是V3中最后一个Dropout层)。</li><li>模型迁移学习tensorflow代码地址：<a href="https://github.com/KK-xi/Inception_V3_transfer_learning" target="_blank" rel="noopener">Github</a></li></ul><h3 id="五resnet">五、ResNet</h3><p><strong>1、ResNet网络简介</strong> <br> 　　ResNet (Residual Neural Network）由微软研究院的何情明等 4 名华人提出，网络深度达到了152 层，top-5 错误率 只有3.57% 。虽然ResNet的深度远远高于 VGGNet，但是参数量却比 VGGNet 低，效果也更好。ResNet 中最创新就是<font color="red">残差学习单元（Residual Unit）</font>的引入，它是参考了瑞士教授 Schmidhuber 的论文《Training Very Deep Networks》中提出的 Highway Network。 <font color="red"> Highway Network</font> 的出现是为了解决较深的神经 网络难以训练的问题，主要思想是启发于LSTM的门(Gate)结构，使得有一定比例的前一层的信息没有经过矩阵乘法和非线性变换而是直接传输到下一层，网络要学习的就是原始信息应该以何种比例保留下来。后来残差网络的学习单元就受益于Highway Network加深网络层数的做法。</p><p><strong>2、模型结构</strong> <br> 　　随着网络加深，由于反向传播过程的叠乘可能出现<font color="red">梯度消失</font>，结果就是准确率下降，为此ReNet中引入残差学习单元(如图所示，2层和3层)，思想就是对于一个达到了准确率饱和的较浅网络，在后面加几个全等映射层允许初始信息直接传到下一层(y=x)时，误差不会因此而增加，并且网络要学习的就是原来输出<span class="math inline">\(H(x)\)</span>与原始输入<span class="math inline">\(x\)</span>的残差<span class="math inline">\(F(x)=H(x)-x\)</span>。 <img src="https://img-blog.csdnimg.cn/20200402093002712.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="残差单元"> 　　将多个残差单元堆叠起来就组成了ResNet网络，如下图所示是一个34层的残差网络： <img src="https://img-blog.csdnimg.cn/20200402095125460.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="ResNet"> 旁边的支线就是将上一层残差单元的输出直接参与到该层的输出，这样的连接方式被称为<font color="red">Shortcut和Skip Connection</font>，这样可以一定程度上保护信息的完整性。最后在改进的ResNet V2中通过该连接使用的激活函数(ReLU)被更换为Identity Mappings (<span class="math inline">\(y=x\)</span>)，残差单元都使用了BN归一化处理，使得训练更加容易并且泛化能力更强。</p><p><strong>3、pytorch和tensorflow实现ResNet</strong> <br> 　代码地址：<a href="https://github.com/KK-xi/ResNet" target="_blank" rel="noopener">GitHub</a> 　这里只是搭建的网络，没有进行任何任务的训练，可以在自己的数据集上训练试一下。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度学习模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>总结SLAM相关知识点资源库以及企业</title>
    <link href="/2020/03/31/%E8%A7%86%E8%A7%89SLAM%E7%9F%A5%E8%AF%86%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E4%BC%81%E4%B8%9A%E6%80%BB%E7%BB%93/"/>
    <url>/2020/03/31/%E8%A7%86%E8%A7%89SLAM%E7%9F%A5%E8%AF%86%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E4%BC%81%E4%B8%9A%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h3 id="视觉slam的知识资源以及相关企业总结">视觉SLAM的知识资源以及相关企业总结</h3><pre><code>先在这儿记录一下，SLAM涉及的太多了，现在接触的只是冰山一角。</code></pre><p>声名：我只是微信公众号<font color="red">计算机视觉life</font>的搬运工。 <bar> <bar> <bar></bar></bar></bar></p><h4 id="一slam知识库">一、SLAM知识库</h4><p><strong>1、SLAM框架/算法流程</strong></p><p><a href="img-Nb7dyKIQ-1586231125936"></a><img src="https://img-blog.csdnimg.cn/20200407114420210.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" width="50%"></p><p><strong>2、应掌握的数学知识</strong></p><ul><li><strong>矩阵</strong>：四则运算、求逆、反对称矩阵；矩阵分解（SVD、QR、Cholesky）。</li><li><strong>李群与李代数</strong>：指数对数映射；李代数求导；扰动模型。</li><li><strong>非线性优化</strong>：梯度下降；牛顿法；高斯牛顿法；LM算法；Bundle Adjustment。</li><li><strong>微积分</strong>：求(偏)导、泰勒展开等。</li></ul><p><strong>3、专业知识</strong> <br> - <strong>计算机视觉</strong> <br> 　<strong>①</strong> 传感器类型： <br> 　　　激光雷达 <br> 　　　视觉传感器：单目相机、双目相机、RGB-D相机、全景相机、Event相机。 <br> 　　　IMU (Inertial measurement unit，测量物体三轴姿态角及加速度的装置) <br> 　<strong>②</strong> 相机： <br> 　　　针孔相机模型、双目相机模型、RGB-D相机模型、相机标定、 去畸变。 <br> 　<strong>③</strong> 特征点 <br> 　　　特征点检测、特征点描述子、特征点匹配、特征点筛选。 <br> 　<strong>④</strong> 多视角几何 <br> 　　　对极约束、本质矩阵、单应矩阵、三角化。 <br>　　　 - <strong>书籍文献</strong> <br> 　《视觉SLAM十四讲》、《Multiple View Geometry》、《机器人状态估计》</p><p><strong>4、编程环境</strong></p><ul><li>Linux系统操作：推荐Ubuntu16.04 <br> 　　　　　　　　书《鸟哥的Linux私房菜》 　　　　　　　　</li><li>开发环境：Clion（JetBrains推出的C/C++跨平台集成开发环境） <br> 　　　　　　Kdevelop（免费的） 　　　　　　</li><li><p>编译工具：学习跨平台编译器cmake、电子书《Cmake Practice》</p></li><li>第三方库：Opencv(计算机视觉)、Eigen(几何变换)、Sophus(李代数)、Ceres(非线性优化)、G2o(图优化)、OpenGL(计算机图形学) 　　　　　　</li><li><p>文档编辑：Gedit、Vim、Nano</p></li></ul><p><strong>5、应用场景</strong></p><ul><li><p>自动驾驶：作为激光传感器的辅助（百度、腾讯、驭势、图森）</p></li><li><p>增强现实：手机、智能眼镜上结合IMU用于定位（微软、三星、华为、虹软、悉见）</p></li><li><p>机器人：无人机定位、建立地图（大疆）；服务机器人室内定位及导航（思岚）；工业机器人定位导航（阿里、京东AGV仓库运输）</p></li><li><p>三维重建：物体重建（3D打印、3D虚拟试衣）；大场景重建（虚拟全景漫游）</p></li></ul><p><strong>6、公开数据集</strong> <br> 　　TUM RGB-D SLAM Dataset、KITTI Vision Benchmark Suite、EuRoC MAV Dataset.</p><p><strong>7、典型开源方案</strong></p><ul><li>稀疏法：ORB-SLAM2（单目、双目、RGB-D）</li><li>半稠密法：LSD-SLAM（单目、双目、RGB-D）、DSO（单目）</li><li>稠密法：Elastic Fusion（RGB-D）、Bundle Fusion（RGB-D）、 　　　　　RGB-D SLAM V2（RGB-D）</li><li>多传感器融合：VINS（单目+IMU）、OKVIS（单目、双目、四目+IMU）</li></ul><h4 id="二slam学习资源总结">二、SLAM学习资源总结</h4><p><strong>1、公众号</strong> <br> 　　泡泡机器人SLAM、计算机视觉life、3D视觉工坊、小白学视觉、计算机视觉之路、AI算法修炼营、PCL点云。 　</p><p><strong>2、视频公开课</strong> <br> 　　① <a href="https://space.bilibili.com/38737757" target="_blank" rel="noopener">泡泡机器人公开课</a> <br> 　　② <a href="https://space.bilibili.com/45189691" target="_blank" rel="noopener">计算机视觉life SLAM研习社直播公开课</a> <br> 　　③ <a href="https://www.dis.uniroma1.it/~labrococo/tutorial_icra_2016/" target="_blank" rel="noopener">SLAM Totorial@ICRA 2016</a> <br> 　　④ <a href="https://www.coursera.org/specializations/robotics" target="_blank" rel="noopener">Robotics-UPenn on Coursera by Vijay Kumar(2016)</a> <br> 　　⑤ <a href="https://vision.in.tum.de/teaching/ss2016/mvg2016" target="_blank" rel="noopener">Computer Vision Ⅱ：Multiple View Geometry-TUM by Daniel Cremers (Spring 2016)</a> <br> 　　⑥ <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa15/" target="_blank" rel="noopener">Advanced Robotics-UCBerkeley by Pieter Abbeel (Fall 2015)</a> <br> 　　⑦ <a href="https://ylatif.github.io/movingsensors/" target="_blank" rel="noopener">The Problem of Mobile Sensors</a> <br></p><p><strong>3、主要研究方向</strong> <br> 　　视觉里程计：线特征；点线特征融合；点面特征融合；点线面多特征融合；多鱼眼VO；特征点发直接法结合；抗光照变化。 <br></p><p>　　语义SLAM：3D语义地图重定位；语义建图互相促进。 <br></p><p>　　多传感器融合：VIO（松耦合、紧耦合）；多传感器在线标定；camera，lidar融合定位；lidar，天花板摄像头融合；lidar，imu融合；TOF辅助双目密集匹配；事件相机SLAM；多机协同SLAM。 <br></p><p>优化方法：非线性增量优化；BA改进；离散连续图模型优化；边缘的约束图优化。 <br></p><p>　　三维重建：大尺度场景下的实时重建；RGBD室内稠密重建；地图数据关联；MVS；3D物体分类，提升重建质量；多子图融合。 <br></p><p>　　结合深度学习：基于深度学习的特征点；结合深度学习的深度估计；深度学习做闭环检测；深度学习估计关键帧深度。</p><p><strong>4、会议期刊</strong></p><ul><li><p>英文期刊 <br> ICRA（IEEE International Conference on Robotics and Automation） <br> IROS（IEEE International Conference on Intelligent Robots and Systems） <br> CVPR（IEEE International Conference on Computer Vision and Pattern Recognition） <br> RSS（Robotics：Science and Systems） <br> ECCV（European Conference on Computer Vision） <br> ISMAR（IEEE and ACM International Symposium on Mixed and Augmented Reality.IEEE） <br> JFR（Journal of Field Robotics） <br> IEEE Transactions on Robotics <br> ACRA（Australian Conference on Robotics and Automation） <br> ICARCV（International Conference on Cotrol，Automation，Robotics and Vision） <br> ISR（International Symposium on Robotics） <br> IEEE Transactions On Pattern Analysis And achine Intelligence <br></p></li><li><p>中文期刊 <br> 计算机辅助设计与图形学学报 <br> 机器人 <br> 计算机应用研究 <br> 中国科学：信息科学 <br></p></li></ul><p><strong>5、知名研究实验室</strong></p><ul><li><p>欧洲 <br> 苏黎世联邦理工学院的Autonomous System Lab <br> 苏黎世大学Robotics and Perception Group <br> 慕尼黑工业大学The Computer Vision Group <br> 英国伦敦大学帝国理工学院 Dyson 机器人实验室 <br> 英国牛津大学Active Vision Laboratory <br> 德国弗莱堡大学Autonomous Intelligent Systems <br> 西班牙萨拉戈萨大学SLAM实验室 <br></p></li><li><p>北美 <br> 麻省理工计算机科学与人工智能实验室（CSAIL）海洋机器人组 <br> 明尼苏达大学Multiple Autonomous Robotics Systems Laboratory <br> 宾夕法尼亚大学GRASP实验室 <br> 华盛顿大学UW Robotics and Estimation Lab <br> 哥伦比亚大学计算机视觉与机器人组 <br> 加拿大谢布鲁克大学IntRoLab <br> 斯坦福大学人工智能实验室自动驾驶团队 <br> 卡内基梅隆大学Robot Perception Lab <br> 特拉华大学Robot Perception and Navigation Group</p></li><li><p>亚洲 <br> 香港科技大学Aerial Robotics Group <br> 浙江大学CAD&amp;CG国家重点实验室Computer Vision Group <br> 清华大学自动化系宽带网络与数字媒体实验室BBNC <br> 中科院自动化研究所国家模式识别实验室Robot Vision Group <br> 上海交通大学感知与导航研究所 <br> 武汉大学Computer Vision &amp; Remote Sensing Lab <br> 日本先进工业科技研究所 <br> 筑波大学智能机器人研究室 <br> 新加坡南洋理工大学HESL实验室 <br> 韩国科学技术研究院 <br></p></li><li><p>澳洲 <br> 澳大利亚悉尼科技大数学CAS实验室 <br> 澳大利亚机器学习研究所机器人视觉中心 <br></p></li></ul><h4 id="三slam相关企业">三、SLAM相关企业</h4><p><strong>1、移动机器人</strong></p><ul><li><p>扫地机器人 <br> 科沃斯（苏州）、石头（北京）、追觅（上海）、银星智能（深圳）</p></li><li><p>服务机器人 <br> 优必选（深圳）、达闼（北京）、思岚（上海）、高仙（上海）、猎户星空（北京）、速感（北京）、普渡（深圳）、美团（北京）</p></li><li><p>仓储机器人 <br> 旷视（北京、上海）、京东（北京）、顺丰（深圳）、海康威视（杭州）、极智嘉（北京）</p></li></ul><p><strong>2、相机传感器</strong></p><ul><li><p>双目相机 <br> 小觅（无锡）、Indemind（北京）、爱观（上海）、中科慧眼（北京）</p></li><li><p>RGBD相机 <br> Intel（北京、上海）、奥比中光（深圳）、Pico（青岛、北京）、图漾（上海）、云从（重庆）</p></li><li><p>激光雷达 <br> 禾赛（上海）、镭神智能（深圳）、速腾聚创（深圳）</p></li><li><p>事件相机 <br> 芯仑（上海）</p></li></ul><p><strong>3、无人机</strong> <br> 　　大疆（深圳、上海）、亿航（广州）、臻迪（北京）</p><p><font color="red"><strong>4、智能驾驶</strong></font></p><ul><li><p>无人驾驶 <br> 百度（北京）、阿里菜鸟/达摩院（杭州）、腾讯（北京）、驭势（北京、上海）、momenta（北京）、滴滴（北京）、图森（北京）、飞步科技（杭州）、纽励科技（上海）、小马智行（广州）、Aptiv（上海）、文远知行（广州）</p></li><li><p>辅助驾驶 <br> 纵目科技（上海）、魔视智能（上海）、极目智能（武汉）、虹软（杭州）、商汤（北京）</p></li><li><p>芯片 <br> NVIDIA（上海）、地平线（北京、南京）</p></li><li><p>高精地图 <br> 高德（北京）、四维图新（北京）</p></li><li><p>汽车厂商 <br> 上汽研究院（上海）、蔚来汽车（上海）、小鹏汽车（广州）、宇通客车（郑州）</p></li></ul><p><strong>5、增强现实</strong></p><ul><li><p>智能眼镜 <br> 联想（上海）、视辰（上海）、肇观电子（上海）、悉见（北京）</p></li><li><p>手机 <br> 华为（上海）、虹软（杭州、上海）、商汤（杭州）、亮风台（上海）、今日头条（北京）</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>SLAM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像处理的注意力机制</title>
    <link href="/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="图像处理中的attention-mechanism">图像处理中的Attention Mechanism</h3><pre><code>摘要：关于在图像处理任务(图像分类、超分辨率、图像描述、图像分割等)中添加注意力机制的问题，计算机视觉（computer vision）中的注意力机制（attention）就是想让系统学会注意重点信息。</code></pre><p>参考博客：<a href="https://blog.csdn.net/xys430381_1/article/details/89323444" target="_blank" rel="noopener">blog1</a>总结的很好，<a href="https://blog.csdn.net/bvl10101111/article/details/78470716" target="_blank" rel="noopener">blog2</a>给了添加注意力的基本套路，<a href="https://blog.csdn.net/Wayne2019/article/details/78488142" target="_blank" rel="noopener">blog3</a>也可以看看。</p><p>参考文献：《Spatial transformer networks》、《Squeeze and Excitation Networks》、《CBAM: Convolutional Block Attention Module》</p><h4 id="一图像处理的注意力机制基本概念">一、图像处理的注意力机制基本概念</h4><p>　　通常举的例子来讲，有一幅图：一只鸟儿翱翔在天空。如果要对这幅图进行鸟类的分类识别任务，人眼去看可能就关注这只鸟是什么鸟，神经网络去做这个任务就要提取特征然后再分类，他对于这幅图中像素点的处理都是一样的，不会像人眼一样专门关注鸟儿，所以神经网络需要我们告知它重点关注哪部分，这就是注意力机制。 <br> 　　现在的深度学习与视觉注意力机制结合的研究工作，大多数是集中于使用<font color="red">掩码(mask)</font>来形成注意力机制。(<strong>掩码的原理</strong>就是：根据掩码矩阵（也称作核）重新计算图像中每个像素的值。掩码矩阵中的值表示一层新的权重，将图片数据中关键的特征标识出来，通过学习训练，让深度神经网络学到每一张新图片中需要关注的区域，也就形成了注意力。)</p><h4 id="二注意力机制分类">二、注意力机制分类</h4><p><font color="navy">▲ <strong>按注意力的可微性来分</strong></font> <font color="navy"><strong>1、软注意力 (soft attention)</strong></font> <br> 　　每个区域被关注的程度高低，用0~1的score表示。软注意力的关键点在于，这种注意力更关注区域或者通道，而且软注意力是确定性的注意力，学习完成后直接可以通过网络生成，最关键的地方是软注意力是可微的。可以微分的注意力就可以通过神经网络算出梯度并且前向传播和后向反馈来学习得到注意力的权重。软注意力的注意力域可以分为：空间域 (spatial domain)、通道域(channel domain)、层域(layer domain)、混合域(mixed domain)。</p><p><strong>① 空间域</strong>： <br> 　　从文章<font color="maroon"><strong>《Spatial transformer networks》</strong></font>来了解空间注意力机制，其中的spatial transformer就起到了空间注意力机制的作用，不仅可以选择空间中最关注（注意力集中）的区域，还可以将这些区域转换为规范的、预期的姿态（Spatial transformer具有旋转、缩放变换的功能，这样图片局部的重要信息能够通过变换而被框盒提取出来），从而简化以下层的识别。下面说说这篇文章：</p><ul><li><p><font color="teal"><strong>文章的思路及其好处</strong></font>：在CNNs中只有有限的（小空间的）、预先定义好的池化机制（max pooling 或者average pooling 的方法）来处理数据空间排列的变化，将图片信息压缩以减少运算量提升准确率，但这样导致了空间不变性只在较深的网络层中存在，而在输入数据变换较大时实际上不存在这种不变性。这篇文章中的空间转换模块优点：一个spatial transformer可以将适当的区域裁剪和缩放标准化，这样可以简化后续的分类任务，并获得更好的分类性能；给定一组包含相同（但未知）类的不同实例的图像，可以使用spatial transformer在每个图像中框选它们；利用spatial transformer（注意力）的好处在于，低分辨率的输入可以转换为高分辨率的原始输入，从而提高计算效率。</p></li><li><p><font color="teal"><strong>模型构架</strong></font>：spatial transformer可以放在任意网络中，并且计算速度快，其构架如下图： <img src="https://img-blog.csdnimg.cn/20200411231633386.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="100%"> 空间转换模块(注意力机制模块)有三个组成部分：</p><p><strong>Localisation Network</strong> —— 为了便于计算，首先用一个定位网络(Localisation Network) 获取输入的特征图<span class="math inline">\(U∈R^{H×W×C}\)</span>，并通过若干隐藏层输出应用于特征图的空间变换参数<span class="math inline">\(θ=floc(U)\)</span>。本地化网络函数<span class="math inline">\(floc()\)</span>可以采用任何形式，例如完全连接的网络或卷积网络，但应包括最终回归层，以生成转换参数<span class="math inline">\(θ\)</span>。</p><p><strong>Grid generator</strong>—— 使用变换参数<span class="math inline">\(θ\)</span>生成一个采样网格 (即变换矩阵<span class="math inline">\(T_θ(G)\)</span>)，该网格是一组对输入映射进行采样从而产生变换后的输出点，这是由Grid generator完成的。下图是文章的一个例子： 　　　　 <img src="https://img-blog.csdnimg.cn/20200412091624329.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" width="80%"> 其中，(a)中的采样矩阵是单位矩阵，不做任何变换，其中I是恒等变换参数。(b)中的矩阵是可以产生缩放旋转变换<span class="math inline">\(T_θ(G)\)</span>的采样矩阵。</p><p><strong>Sampler</strong>—— 最后，将原始特征图 U 和变换器作为采样器的输入，生成最后的特征图V，其中 V 的像素点是输入 U 经由上一阶段的采样网格作用的结果。现假设这个变换是2D的情况 (假设<span class="math inline">\(T_θ=A_θ\)</span>)，变换公式如下： <span class="math display">\[ \left(\begin{array}{c}x_i^s \\y_i^s\end{array}\right) =T_θ(G_i)=A_θ\left(\begin{array}{c}x_i^t \\y_i^t\\1\end{array}\right) =\left[\begin{array}{ccc}θ_{11}&amp;θ_{12}&amp;θ_{13}\\θ_{21}&amp;θ_{22}&amp;θ_{23}\end{array}\right] \left(\begin{array}{c}x_i^t \\y_i^t\\1\end{array}\right)\]</span> 公式中<span class="math inline">\((x_i^t ，y_i^t)\)</span>是输出特征图的常规网格的目标坐标，<span class="math inline">\((x_i^s，y_i^s)\)</span>是输入特征图中定义采样点的源坐标，<span class="math inline">\(A_θ\)</span>是仿射变换矩阵。宽度和高度坐标均为归一化的。当然，也有定义3D情况下的变换，允许裁剪、平移、旋转和缩放。这个模块加进去最大的好处就是能够对上一层信号的关键信息进行识别(即添加attention)。</p></li><li><p><font color="teal"><strong>结果讨论</strong></font>：由于文章的方法对通道信息没有特别的处理变换，所以这种变换更适合用在添加卷积层之前的变换，因为每一个卷积核(filter)在卷积操作以后都会产生的通道信息，而且它们含有的信息以及信息的关注程度是不一样的。所以有了第二种注意域的机制——通道域。</p></li></ul><p><strong>② 通道域</strong>： <br> 　　从文章<font color="maroon"><strong>《Squeeze and Excitation Networks》</strong></font>来了解通道注意力机制。计算机视觉任务中，捕获特征的空间相关性可以提升CNNs的性能，以上有了空间注意力，这篇文章就研究了网络设计的不同方面，即添加通道注意力。</p><ul><li><p><font color="teal"><strong>文章的思路及其好处</strong></font>：文章引入了一种新的架构单元 Squeeze-and-Excitation (SE) block，其目的是通过利用卷积特征之间的相互依赖关系来提高网络的质量。文中提出了一种机制，允许网络执行特征重新校准，通过这种机制，网络可以学习使用全局信息来选择性地强调信息特征和抑制不太有用的特征。</p></li><li><p><font color="teal"><strong>模型构架</strong></font>：多个这种SE block聚合在一起就构成了SENet。文章所提到的SE blok构架如下图所示： <img src="https://img-blog.csdnimg.cn/20200412131746928.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="-"> 对于任何给定的变换<span class="math inline">\(F_{tr}\)</span> (例后边将其当作如卷积)，将输入X变换到<span class="math inline">\(U∈R^{H×W×C}\)</span>，可以构造相应的SE block来执行特征重新校准。重先对一些符号做提前声明：<span class="math inline">\(V=[v_1,v_2,...,v_c]\)</span>表示变换<span class="math inline">\(F_{tr}\)</span>的一系列卷积核，<span class="math inline">\(v_c\)</span>表示第 c 个卷积的参数；卷积之后的输出<span class="math inline">\(U=[u_1,u_2,...,u_c]\)</span>，其中<span class="math inline">\(u_c=v_c*X\)</span>，需注意具体过程是vc的一个单通道 (二维空间核<span class="math inline">\(v_c^s\)</span>) 作用于x的对应通道 (<span class="math inline">\(x^s\)</span>) 上，为了简化表示省略了偏置项。校准的三个过程过程就是通道注意力机制的过程：</p><p><strong>挤压(squeeze， Global Information Embedding)</strong> —— 由于卷积操作只是局部感受野，最终输出的特征图的每个小单元不能用来探索除开这个小区域以外的联系特征，所以该方法首先将特征 U 通过挤压操作传递，挤压操作通过压缩其空间维度（H×W）上的特征来生成通道描述符。基于通道的特征量<span class="math inline">\(z∈R^c\)</span>，其第 c 个量计算方式如下： 　　　　　　　　　<img src="https://img-blog.csdnimg.cn/20200412151403621.png" srcset="/img/loading.gif" width="50%"> 其实这里就是全局平均池化操作。</p><p><strong>激励(excitation )</strong> —— 聚合之后是一个激励操作，其目的是捕获通道依赖关系，为满足条件，文章用到的机制函数如下：<span class="math display">\[s=F_{ex}(z,W)=\sigma(g(z,W))=\sigma(W_2\delta(W_1z))\]</span>其中的<span class="math inline">\(\sigma\)</span>表示ReLU函数，<span class="math inline">\(\delta\)</span>是sigmoid函数，用于生成channel 间0~1的 attention weights（注意力权重用于激励每一层通道，权重通过学习生成）。为简化模型，在非线性前接了一个两层全连接FC的botteneck (一个降维层压缩 channel 数)，然后再是ReLU和升维层重构回 channel 数。</p><strong>缩放(scale)</strong> —— block的最终输出是通过激活s重新缩放 U 来获得的，其公式如下：<span class="math display">\[\tilde{x}=F_{scale}(u_c,s_c)=s_cu_c\]</span>其中<span class="math inline">\(\tilde{X}=[\tilde{x_1},\tilde{x_2},...,\tilde{x_c}]\)</span>，<span class="math inline">\(F_{scale}(u_c,s_c)\)</span>表<span class="math inline">\(s_c\)</span>与H×W维度上的特征图<span class="math inline">\(u_c\)</span>的乘积，不同通道的值乘上不同的权重，从而可以增强对关键通道域的注意力。 　　</li><li><p><font color="teal"><strong>结果讨论</strong></font>：通道域的注意力是对一个通道内的信息直接全局平均池化，而忽略每一个通道内的局部信息，这种做法其实也是比较简单粗暴。所以结合两种思路，就可以设计出混合域的注意力机制模型。</p></li></ul><p><strong>③混合域</strong>： <br> 　　从2018年的文章<font color="maroon"><strong>《CBAM: Convolutional Block Attention Module》</strong></font>来了解混合域注意力机制。这篇文章是基于 SE-Net 的 Squeeze-and-Excitation block进行进一步拓展，可以理解为文中把 channel attention看成是教网络关注什么；而spatial attention 是教网络关注何处。</p><ul><li><p><font color="teal"><strong>文章的思路及好处</strong></font>：提出了一个新模块“卷积块注意模块”。由于卷积运算通过将跨通道和空间信息融合在一起来提取信息特征，因此文中用到的模块是用来注意沿这两个维度（通道和空间轴）的重要特征。文章的贡献之处有：注意力模块（CBAM）简单而有效，该模块可应用于提高CNN提取描述符的能力。</p></li><li><p><font color="teal"><strong>模型构架</strong></font>：在SENet的基础上进行改进，得到CBAM模型基本构架如下图所示：<img src="https://img-blog.csdnimg.cn/2020041223143241.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="CBAM"> 给定一个中间特征图<span class="math inline">\(F∈R^{C×H×W}\)</span>作为输入，CBAM依次得到1D的通道注意图<span class="math inline">\(M_c∈R^{C×1×1}\)</span>和2D空间注意图$M_s∈R^{1×H×W}。 整个注意力过程可以总结为： <span class="math display">\[F&#39; = M_c(F)⊗F\]</span><span class="math display">\[F&#39;&#39;= M_s(F_0)⊗F_0\]</span>其中⊗表示逐元素乘法。 在乘法过程中，注意值会相应地传递：通道注意值会沿空间维度传递，反之亦然。 <span class="math inline">\(F&#39;&#39;\)</span>是最终输出。下面介绍每个注意模块：</p><p><strong>Channel attention module</strong> —— 该子模块构架如下图： <img src="https://img-blog.csdnimg.cn/20200412235136512.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif"> 为计算通道注意力，我们压缩输入特征图的空间尺寸，过使用平均池化操作和最大池化操作来收集特征图的部分信息，生成两个不同的空间上下文描述符：<span class="math inline">\(F^c_{avg}\)</span>和<span class="math inline">\(F^c_{max}\)</span>，分别表示平均池化特征和最大池化特征。然后将两个描述符传送到共享网络(多层感知器MLP组成)，以生成我们的频道注意图<span class="math inline">\(M_c\)</span>。为了减少参数，隐藏层的激活输出大小设置为<span class="math inline">\(R^{C / r×1×1}\)</span>，其中 r 是缩小率。通道注意力的计算公式为：<span class="math display">\[M_c(F)=σ(MLP(AvgPool(F))+ MLP(MaxPool(F)))=σ(W_1(W_0(F^c_{avg}))+ W_1(W_0(F^c_{max})))\]</span> 其中 σ 表示Sigmoid函数，<span class="math inline">\(W_0∈R^{C/ r×C}\)</span>，<span class="math inline">\(W_1∈R^{C×C / r}\)</span>。两个输入均共享MLP权重<span class="math inline">\(W_0\)</span>和<span class="math inline">\(W_1\)</span>，并且ReLU激活后才乘<span class="math inline">\(W_0\)</span>。</p><p><strong>Spatial attention module</strong> —— 空间注意力子模块的构架如下图： <img src="https://img-blog.csdnimg.cn/20200412235258831.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif"> 利用特征之间的空间关系来生成空间注​​意图，空间注意力集中在“哪里”是一个信息部分，与通道注意力是互补的。为了计算空间注意力，首先沿通道轴应用平均池化和最大池化操作，来聚合特征图的通道信息，生成两个二维图：<span class="math inline">\(F^s_{avg}∈R^{1×H×W}\)</span>和<span class="math inline">\(F^s_{max}∈R^{1×H×W}\)</span>。在级联的特征描述符上，应用卷积层以生成空间注​​意图<span class="math inline">\(M_s(F)∈R^{H×W}\)</span>，该图对强调或抑制的位置进行编码。，空间注意力的计算公式为： 　　　　　　　　<img src="https://img-blog.csdnimg.cn/20200413000142165.png" srcset="/img/loading.gif" width="60%"></p>其中σ表示S型函数，<span class="math inline">\(f^{7×7}\)</span>表示大小为7×7的卷积核。下图是一个ResNet中嵌入这个CBAM块的Resblock构架图： <img src="https://img-blog.csdnimg.cn/20200413000728407.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif"></li><li><p><font color="teal"><strong>结果讨论</strong></font>：这里的CBAM模块是轻量级的。所以放在网络中也比较方便，而且包含了通道信息和空间信息的处理，总体上比SENet要好。</p></li></ul><p><font color="blue"><strong>2、硬注意力</strong> (hard attention)</font> <br> 　　首先强注意力是更加关注点，也就是图像中的每个点都有可能延伸出注意力，同时强注意力是一个随机的预测过程，更强调动态变化。当然，最关键是强注意力是一个不可微的注意力，训练过程往往是通过增强学习(reinforcement learning)来完成的。Hard-attention，就是0/1问题，哪些区域是被 attentioned，哪些区域不关注。在这里不做介绍。</p><p><font color="blue"><strong>3、自意力</strong> (self-attention)</font> <br> 　　自注意力机制是注意力机制的改进，其减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性。自注意力机制 (self-attention)在序列模型中取得了很大的进步；另外一方面，上下文信息（context information）对于很多视觉任务都很关键，如语义分割，目标检测。自注意力机制通过（key, query, value）的三元组提供了一种有效的捕捉全局上下文信息的建模方式。</p><p>　　<strong>自注意力的缺点和相应的改进策略</strong>：由于每一个点都要捕捉全局的上下文信息，这就导致了自注意力机制模块会有很大的计算复杂度和显存容量。如果我们能知道一些先验信息，比如上述的特征对其通常是一定的邻域内，我们可以通过限制在一定的邻域内来做，另外还有如何进行高效的稀疏化，以及自注意力机制和图卷积的联系。同时，这种建模方式的缺点还没有考虑channel上信息，相应的改进策略是如何进行spatial和channel上信息的有效结合。 　　不对自注意力机制做过多的介绍。</p><h4 id="三注意力机制的实现">三、注意力机制的实现</h4><p>主要参考别人复现的CBAM，即通道注意力和空间注意力结合的模型：pytorch实现，地址：<a href="https://github.com/KK-xi/CBAM-attention_mechanism-_pytorch" target="_blank" rel="noopener">github</a>。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>attention mechanism</category>
      
    </categories>
    
    
    <tags>
      
      <tag>attention mechanism</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title> 视觉SLAM十四讲 第一章</title>
    <link href="/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%20%E7%AC%AC%E4%B8%80%E7%AB%A0/"/>
    <url>/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2%20%E7%AC%AC%E4%B8%80%E7%AB%A0/</url>
    
    <content type="html"><![CDATA[<h2 id="视觉slam十四讲">视觉SLAM十四讲</h2><p>作者：高翔 | 张涛 <br> 最后更新于：2017.03.31</p><h3 id="第一-章-前言">第一 章 前言</h3><h4 id="本书讲什么">1.1本书讲什么</h4><p>　　本书介绍视觉SLAM。</p><p>　　SLAM 是 Simultaneous Localization and Mapping 的缩写，中文译作“同时定位与地图构建”。它是指搭载特定传感器的主体，在没有环境先验信息的情况下，于运动过程中建立环境的模型，同时估计自己的运动。如果这里的传感器主要为相机，那就称为 “视觉 SLAM”，此时我们要做的，就是根据一张张连续运动的图像（它们形成一段视频），从中推断相机的运动，以及周围环境的情况。</p><p>　　如今视觉SLAM的应用点有许多，在许多地方我们都想知道自己的位置：室内的扫地机和移动机器人需要定位，野外的自动驾驶汽车需要定位，空中的无人机需要 定位，虚拟现实和增强现实的设备也需要定位。</p><p>　　本书提及了 SLAM 的历史、理论、算法、现状，并且把完整的 SLAM 系统分成几个模块：视觉里程计、后端优化、建图以及回环检测。一些必要的数学理论和许多编程知识，会用到 <strong>Eigen、 OpenCV、PCL、g2o、Ceres </strong>等库。</p><h4 id="如何使用本书">1.2如何使用本书</h4><p><font color="red"><strong>理论+习题+编程</strong></font>： <br></p><p>１. 第一部分为<font color="maroon">数学基础篇</font>，铺垫与视觉 SLAM 相关的数学知识，包括： <br></p><p>　• 第二讲为 SLAM 系统概述，介绍一个 SLAM 系统由哪些模块组成，各模块的具体工作是什么。实践部分介绍编程环境的搭建过程以及 IDE 的使用。 <br></p><p>　• 第三讲介绍三维空间运动，将接触旋转矩阵、四元数、欧拉角的相关知识，并且在 Eigen 当中使用它们。 <br></p><p>　• 第四讲为李群和李代数。将学习李代数的定义和使用方式，然后通过 Sophus 操作它们。 <br></p><p>　 • 第五讲介绍针孔相机模型以及图像在计算机中的表达。将用 OpenCV 来调取相机的内外参数。 <br></p><p>　• 第六讲介绍非线性优化，包括状态估计理论基础、最小二乘问题、梯度下降方 法。完成一个使用 Ceres 和 g2o 进行曲线拟合的实验。 <br></p><p>２. 第二部分为 <font color="maroon">SLAM 技术篇</font>。使用第一部分所介绍的理论，讲述视觉 SLAM 中各个模块的工作原理： <br></p><p>　• 第七讲为特征点法的视觉里程计。该讲内容比较多，包括特征点的提取与匹配、 对极几何约束的计算、PnP 和 ICP 等。实践中用这些方法去估计两个图像之间的运动。 <br></p><p>　• 第八讲为直接法的视觉里程计。将学习光流和直接法的原理，然后利用 g2o 实现一个简单的 RGB-D 直接法。 <br></p><p>　• 第九讲为视觉里程计的实践章，将搭建一个视觉里程计框架，综合应用先学过的知识，实现它的基本功能。 <br></p><p>　• 第十讲为后端优化，主要为 Bundle Adjustment 的深入讨论，包括基本的 BA 以及如何利用稀疏性加速求解过程。 <br></p><p>　• 第十一讲主要讲后端优化中的位姿图。位姿图是表达关键帧之间约束的一种更紧凑的形式。将用 g2o 和 gtsam 对一个位姿球进行优化。 <br></p><p>　• 第十二讲为回环检测，我们主要介绍以词袋方法为主的回环检测。你将使用 dbow3 书写字典训练程序和回环检测程序。 <br></p><p>　• 第十三讲为地图构建。会讨论如何使用单目进行稠密深度图的估计（以及这是多么不可靠），然后讨论 RGB-D 的稠密地图构建过程。写极线搜索与块匹配的程序，然后在 RGB-D 中遇到点云地图和八叉树地图的构建问题。 <br></p><p>　• 第十四讲主要介绍当前的开源 SLAM 项目以及未来的发展方向。 <br></p><p><font color="red"><strong>需要具备的：Linux &amp; C++ &amp; 基础数学知识</strong></font>， 装一个Ubuntu子系统去，参考<a href="https://blog.csdn.net/daybreak222/article/details/87968078" target="_blank" rel="noopener">博客</a>。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>视觉SLAM十四讲</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>seu</title>
    <link href="/2020/03/30/eu/"/>
    <url>/2020/03/30/eu/</url>
    
    <content type="html"><![CDATA[<ul><li>2019.10.06 东大一角</li></ul><figure><img src="https://img-blog.csdnimg.cn/20200330115513856.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="seu"><figcaption>seu</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>单目深度估计总结</title>
    <link href="/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    <url>/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<p><bar> <bar></bar></bar></p><p><bar> <bar></bar></bar></p><p>《High Quality Monocular Depth Estimation via Transfer Learning》 作者：Ibraheem Alhashim and Peter Wonka</p><p>备注：只是一篇总结，不是解读哒~</p><h2 id="一为什么要看这篇文章">一、为什么要看这篇文章？</h2><p>　1、因为最近萌生了一个想法，觉得可以用用深度图；</p><p>　2、这篇文章相比其他的文章，模型结构更简单，深度图分辨率更高。</p><h2 id="二文章提出的出发点">二、文章提出的出发点</h2><p>　 1、首先，一张图片2D到3D的深度估计是很多场景理解或者是重建工作中的基础；</p><p>　2、其次，这篇文章希望提出的深度估计方法能获得高分辨率的深度估计结果。</p><p>　3、一些任务（例如图像增强的或用到3D重建等的）要求更快的计算速度。</p><p>　所以设计出一个更简单的模块化的网络构架，使得获得的深度图分辨率更高、质量更高，训练更容易，并且更方便的将以后在其他任务中表现较好的模型迁移到深度估计问题。</p><h2 id="三framework">三、Framework</h2><p>文章中给出的简化架构：<img src="https://img-blog.csdnimg.cn/20200314154230721.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="看似很简单的网络架构"> <strong>1、Encoder</strong></p><p>　　这里用到的编码器是另一篇文章《Densely Connected Convolutional Networks》里的DenseNet-169。下图是生长率k=4的5层的Dense-block结构图，对这里的k的一种解释是，每个层都可以访问这个块中的所有前面的特征映射，因此也可以访问网络的集体特征。可以将特征图视为网络的全局状态，每层将其自身的k个特征映射添加到该状态，增长率决定了每一层对全局状态贡献多少新信息。一旦写入全局状态，就可以从网络中的任何地方访问全局状态，并且与传统的网络架构不同，不需要从一层复制到另一层。 <img src="https://img-blog.csdnimg.cn/20200314160111493.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="5-layer dense block with growth rate of k=4"> 然后，为了在网络中进行下采样（改变图像大小），将多个Dense-block连接起来，就形成了下图的样子： <img src="https://img-blog.csdnimg.cn/20200314161447623.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="Dense Net with three dense blocks"> 两个相邻块之间的层称为过渡层，并通过卷积和池化来改变特征图的大小，但是本文中是删除了最后的top-layer，因为文章是做depth estimation而不是Classification task。但是呢，深度估计这篇文章用的DensNet-169有4个blocks，网络结构参数如下： <img src="https://img-blog.csdnimg.cn/20200314162029675.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="DenseNet architectures for ImageNet"></p><p><strong>2、Decoder</strong></p><p>　　对于decoder，从一个1×1的卷积层开始，其输出通道的数量与去掉top-layer的编码器的输出相同。然后依次添加2×2的双线性upsampling块和串接到encoder的池化层POOL，其后跟两个并列的3×3卷积层，这样的构造重复3次（不同之处在于串接到的池化层分别是Pool3，Pool2，Pool1），然后就是2×2的双线性upsampling块和串接到encoder的卷积层CONV1，然后是两个并列的3×3的卷积层（为了使该块的输出为输入通道数的一半），最后经过一个3×3卷积层最终输出通道数为1的图像。</p><p>　　这个encoder-decoder带有跳过连接，encoder构架不太紧凑，大概的结构就这样啦~~</p><h2 id="四文章的亮点">四、文章的亮点</h2><p>1、第一点大概就是用了这么一个简单的网络来做深度估计，网络复杂不等于结果好；　<br> 2、定义了一个损失函数，通过最小化深度值的差异来平衡重建深度图像之间的关系，同时惩罚深度图的图像域中高频细节的失真： <img src="https://img-blog.csdnimg.cn/20200314165259760.png" srcset="/img/loading.gif" alt="损失函数L"><br>3、运用数据增强策略，文章只用了镜像翻转，以及改变颜色通道排列，后者还可以做一个further work； <br> 4、提出了一个新的test dataset。</p><h2 id="五结果以及改进空间">五、结果以及改进空间</h2><p>　　1、在室外场景中没别人的方法表现好，猜想是因为提供的深度图的性质（由于损失函数不仅要考虑逐点差异，而且还要通过查看每个点周围的区域来优化边缘和外观保留，因此对于非常稀疏的深度图像，学习过程不能很好地收敛）；　<br> 　　2、文章用到的方法仍然需要label； <br> 　　3、文章所提的改进空间挺多的，比如用在嵌入式设备上，这个网络存在局限性，以及更清楚地确定不同编码器、数据增强和学习策略对性能和贡献的影响，都是未来工作中值得关注的内容。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度估计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper conclusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
