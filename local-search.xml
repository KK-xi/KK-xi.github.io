<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>pytorch学习记录</title>
    <link href="/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="课程名pytorch-动态神经网络">课程名：《Pytorch 动态神经网络》</h2><p>课程来源：<a href="https://www.bilibili.com/video/av15997678?p=35" target="_blank" rel="noopener">here</a><br>作者：莫烦</p><h2 id="day01-安装pytorch">day01 安装Pytorch</h2><p>前提：安装Anaconda参考别人的安装教程<a href="https://www.jianshu.com/p/742dc4d8f4c5" target="_blank" rel="noopener" class="uri">https://www.jianshu.com/p/742dc4d8f4c5</a> - 在开始菜单找到Anaconda的命令提示行(Anaconda Prompt)，并输入conda create -n pytorch python=3.7(我自己的是3.7版本)，建立一个Pytorch的环境： <img src="https://img-blog.csdnimg.cn/20200318180532330.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 然后，出现以下情况，问是否安装等等工具包，选择[y]开始安装： <img src="https://img-blog.csdnimg.cn/20200318180734340.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 安装成功以后，会出现如下，就是激活环境的语句： <img src="https://img-blog.csdnimg.cn/20200318180919400.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入conda activate pytorch进入pytorch环境： <img src="https://img-blog.csdnimg.cn/20200318181124374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 输入pip list可以查看这个环境下的工具包，可以看到没有需要的pytorch，所以需要安装： <img src="https://img-blog.csdnimg.cn/2020031818131941.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_12,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> - 去官网查看自己适合哪个版本，比如我的是CPU，习惯用pip，如下图：<img src="https://img-blog.csdnimg.cn/20200322091319185.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1">pip install torch<span class="op">==</span><span class="dv">1</span>.<span class="fl">4.0</span><span class="op">+</span>cpu torchvision<span class="op">==</span><span class="dv">0</span>.<span class="fl">5.0</span><span class="op">+</span>cpu <span class="op">-</span>f https:<span class="op">//</span>download.pytorch.org<span class="op">/</span>whl<span class="op">/</span>torch_stable.html</a></code></pre></div><ul><li>完了以后，在pytorch环境中进入&gt;&gt;python，测试一下是否安装成功，输入import torch即可。</li></ul><h2 id="day02">day02</h2><h3 id="一神经网络简介">一、神经网络简介</h3><p>1、 机器学习—梯度下降机制(optimization) <br> 2、神经网络黑盒：输入端-黑盒-输出端； <br> 　　　　　　黑盒：特征代表输入数据。</p><h3 id="二why-pytorch">二、why Pytorch？</h3><p>1、与tensorflow的区别</p><ul><li><p>tensorflow是静态的框架，构建好tensorflow的计算图之后，这个计算图是不能改变的，计算流程是固定的，类似C++，写代码时要用他自己的一些API。缺点之一例如训练的时候loss一直将不下来，模型很难得到优化，debug就很困难。</p></li><li><p>pytorch是动态的框架，和python一样，直接计算，不用开启会话。 <br></p></li></ul><h3 id="三variable变量">三、Variable变量</h3><ul><li>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置，里面的值会不停的变化，就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那里面的鸡蛋就是 Torch 的 Tensor 。</li><li>PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</li><li><p>from torch.autograd import Variable</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">ten<span class="op">=</span>torch.FloatTensor([[<span class="dv">1</span>,<span class="dv">2</span>],[<span class="dv">3</span>,<span class="dv">4</span>]]) <span class="co"># tensor的类型</span></a><a class="sourceLine" id="cb2-2" data-line-number="2">variable<span class="op">=</span>Variable(tensor,requires_grad<span class="op">=</span><span class="va">True</span>) </a><a class="sourceLine" id="cb2-3" data-line-number="3"><span class="co"># 将tensor传给variable，需要Variable来建立一个计算图纸，把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度，如果要就会计算Variable节点的梯度</span></a><a class="sourceLine" id="cb2-4" data-line-number="4">t_out <span class="op">=</span> torch.mean(ten<span class="op">*</span>ten) <span class="co"># 计算x^2</span></a><a class="sourceLine" id="cb2-5" data-line-number="5">v_out <span class="op">=</span> torch.mean(variable<span class="op">*</span>variable)</a><a class="sourceLine" id="cb2-6" data-line-number="6"></a><a class="sourceLine" id="cb2-7" data-line-number="7">v_out.backward() <span class="co"># v_outbackward时，variable也会变化，因为是一体的</span></a><a class="sourceLine" id="cb2-8" data-line-number="8"><span class="bu">print</span>(variable)</a><a class="sourceLine" id="cb2-9" data-line-number="9"><span class="co">#直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图),所以我们要转换一下, 将它变成 tensor 形式</span></a><a class="sourceLine" id="cb2-10" data-line-number="10"><span class="bu">print</span>(variable.data)</a><a class="sourceLine" id="cb2-11" data-line-number="11"><span class="bu">print</span>(variable.data.numpy())<span class="co"># variable.data为tensor的形式，tensor才能转换为numpy形式</span></a></code></pre></div></li><li>autograd根据用户对Variable的操作构建其计算图，这个图将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候， 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力。</li><li>variable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li><li>多次反向传播时，<font color="red">梯度是累加的</font>。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li><li><p>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</p></li></ul><h2 id="day-03">day 03</h2><h3 id="一激励函数activation">一、激励函数（Activation）</h3><ul><li><p>什么是Activation 非线性的函数激活网络的输出：ReLU、Sigmoid、Tanh、Softplus</p></li><li><p>Torch中的激励函数</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"></a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</a><a class="sourceLine" id="cb3-3" data-line-number="3"><span class="im">from</span> torch.autograd <span class="im">import</span> Variable</a><a class="sourceLine" id="cb3-4" data-line-number="4"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a><a class="sourceLine" id="cb3-5" data-line-number="5"></a><a class="sourceLine" id="cb3-6" data-line-number="6">x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">200</span>)  <span class="co"># x data (tensor), shape=(100, 1)</span></a><a class="sourceLine" id="cb3-7" data-line-number="7">x <span class="op">=</span> Variable(x)</a><a class="sourceLine" id="cb3-8" data-line-number="8">x_np <span class="op">=</span> x.data.numpy()   <span class="co"># numpy 数据才能用来画图</span></a><a class="sourceLine" id="cb3-9" data-line-number="9"></a><a class="sourceLine" id="cb3-10" data-line-number="10">y_relu <span class="op">=</span> torch.relu(x).data.numpy()</a><a class="sourceLine" id="cb3-11" data-line-number="11">y_sigmoid <span class="op">=</span> torch.sigmoid(x).data.numpy()</a><a class="sourceLine" id="cb3-12" data-line-number="12">y_tanh <span class="op">=</span> torch.tanh(x).data.numpy() <span class="co"># 计算出非线性函数输出后也要转化为numpy数据</span></a><a class="sourceLine" id="cb3-13" data-line-number="13"><span class="co"># y_softplus = F.softplus(x).data.numpy()   </span></a><a class="sourceLine" id="cb3-14" data-line-number="14"></a><a class="sourceLine" id="cb3-15" data-line-number="15"><span class="co">#画图</span></a><a class="sourceLine" id="cb3-16" data-line-number="16">plt.figure(<span class="dv">1</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</a><a class="sourceLine" id="cb3-17" data-line-number="17">plt.subplot(<span class="dv">221</span>)</a><a class="sourceLine" id="cb3-18" data-line-number="18">plt.plot(x_np, y_relu, c<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;relu&#39;</span>)</a><a class="sourceLine" id="cb3-19" data-line-number="19">plt.ylim((<span class="op">-</span><span class="dv">1</span>, <span class="dv">5</span>))</a><a class="sourceLine" id="cb3-20" data-line-number="20">plt.legend(loc<span class="op">=</span><span class="st">&#39;best&#39;</span>)</a></code></pre></div></li><li><p>结果 <img src="https://img-blog.csdnimg.cn/2020032211361126.png?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li></ul><h3 id="二regression回归">二、Regression回归</h3><p>直接放莫老师教的代码过来：</p><ul><li><p>Layer图搭建以及计算流程</p><p>```python class Net(torch.nn.Module): # torch.nn.Module是Net的主模块 def <strong>init</strong>(self, n_feature, n_hidden, n_output): # 搭建层所需要的信息 super(Net, self).__init__() # 继承Net的模块功能 self.hidden = torch.nn.Linear(n_feature, n_hidden) # hidden layer self.predict = torch.nn.Linear(n_hidden, n_output) # output layer</p><pre><code>  def forward(self, x): # 前向传递的过程，搭流程图         x = F.relu(self.hidden(x))      # activation function for hidden layer         x = self.predict(x)             # linear output   return x  ```</code></pre></li><li><p>定义Net</p><div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">net <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">1</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">1</span>)     <span class="co"># define</span></a></code></pre></div></li><li><p>优化神经网络(torch.optim.),以及loss function定义</p><div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">optimizer <span class="op">=</span> torch.optim.SGD(net.parameters(), lr<span class="op">=</span><span class="fl">0.2</span>)</a><a class="sourceLine" id="cb6-2" data-line-number="2">loss_func <span class="op">=</span> torch.nn.MSELoss() <span class="co"># 均方差作为loss</span></a></code></pre></div></li><li><p>开始训练</p><div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">200</span>):</a><a class="sourceLine" id="cb7-2" data-line-number="2">    prediction <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a><a class="sourceLine" id="cb7-3" data-line-number="3"></a><a class="sourceLine" id="cb7-4" data-line-number="4">    loss <span class="op">=</span> loss_func(prediction, y)     <span class="co"># must be (1. nn output, 2. target)</span></a><a class="sourceLine" id="cb7-5" data-line-number="5"></a><a class="sourceLine" id="cb7-6" data-line-number="6">    optimizer.zero_grad()   <span class="co"># clear gradients for next train</span></a><a class="sourceLine" id="cb7-7" data-line-number="7">    loss.backward()         <span class="co"># backpropagation, compute gradients</span></a><a class="sourceLine" id="cb7-8" data-line-number="8">    optimizer.step()        <span class="co"># apply gradients</span></a><a class="sourceLine" id="cb7-9" data-line-number="9">    <span class="co"># 以上三步为优化步骤</span></a></code></pre></div></li></ul><h3 id="三-classification-分类">三、 Classification 分类</h3><p>　与上面不同的是：</p><ul><li>构造的伪数据不一样，是包含有对应标签的数据；(数据不能是一维)</li><li>网络输入输出不同，有两个输入两个输出；</li><li>loss用到的是交叉熵cross entropy loss，out与标签y</li></ul><div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">loss_func <span class="op">=</span> torch.nn.CrossEntropyLoss() </a><a class="sourceLine" id="cb8-2" data-line-number="2">loss <span class="op">=</span> loss_func(out, y)</a></code></pre></div><ul><li><p>output是取值，转换成概率值需要加softmax(out)</p><div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">out <span class="op">=</span> net(x)     <span class="co"># input x and predict based on x</span></a><a class="sourceLine" id="cb9-2" data-line-number="2">prediction <span class="op">=</span> F.softmax(out)  <span class="co">#将输出对应值转化成概率</span></a></code></pre></div></li></ul><h3 id="四快速搭建网络">四、快速搭建网络</h3><ul><li><p>method1—搭建网络、流程图，定义网络</p><div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="kw">class</span> Net(torch.nn.Module):</a><a class="sourceLine" id="cb10-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_feature, n_hidden, n_output):</a><a class="sourceLine" id="cb10-3" data-line-number="3">        <span class="bu">super</span>(Net, <span class="va">self</span>).<span class="fu">__init__</span>()</a><a class="sourceLine" id="cb10-4" data-line-number="4">        <span class="va">self</span>.hidden <span class="op">=</span> torch.nn.Linear(n_feature, n_hidden)   <span class="co"># hidden layer</span></a><a class="sourceLine" id="cb10-5" data-line-number="5">        <span class="va">self</span>.predict <span class="op">=</span> torch.nn.Linear(n_hidden, n_output)   <span class="co"># output layer</span></a><a class="sourceLine" id="cb10-6" data-line-number="6"></a><a class="sourceLine" id="cb10-7" data-line-number="7">    <span class="kw">def</span> forward(<span class="va">self</span>, x):</a><a class="sourceLine" id="cb10-8" data-line-number="8">        x <span class="op">=</span> F.relu(<span class="va">self</span>.hidden(x))      <span class="co"># activation function for hidden layer</span></a><a class="sourceLine" id="cb10-9" data-line-number="9">        x <span class="op">=</span> <span class="va">self</span>.predict(x)             <span class="co"># linear output</span></a><a class="sourceLine" id="cb10-10" data-line-number="10">        <span class="cf">return</span> x</a><a class="sourceLine" id="cb10-11" data-line-number="11"></a><a class="sourceLine" id="cb10-12" data-line-number="12">net1 <span class="op">=</span> Net(n_feature<span class="op">=</span><span class="dv">2</span>, n_hidden<span class="op">=</span><span class="dv">10</span>, n_output<span class="op">=</span><span class="dv">2</span>)  </a></code></pre></div></li><li><p>method2—利用torch.nn.Sequential直接定义网络</p><div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1">net2<span class="op">=</span>torch.nn.Sequential(</a><a class="sourceLine" id="cb11-2" data-line-number="2">    torch.nn.Linear(<span class="dv">2</span>,<span class="dv">10</span>),</a><a class="sourceLine" id="cb11-3" data-line-number="3">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb11-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>,<span class="dv">2</span>)</a><a class="sourceLine" id="cb11-5" data-line-number="5">)</a></code></pre></div></li></ul><h3 id="五网络的保存和提取">五、网络的保存和提取</h3><ul><li><p>方法1：保存—提取</p><div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1">torch.save(net1, <span class="st">&#39;net.pkl&#39;</span>) <span class="co"># 保存整个网络，以pkl形式保存</span></a></code></pre></div><div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1">net2 <span class="op">=</span> torch.load(<span class="st">&#39;net.pkl&#39;</span>)</a><a class="sourceLine" id="cb13-2" data-line-number="2">prediction <span class="op">=</span> net2(x)</a></code></pre></div></li><li><p>方法2：保存—提取</p><div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1">torch.save(net1.state_dict(), <span class="st">&#39;net_params.pkl&#39;</span>) <span class="co"># 只保存网络中节点的参数 (速度快, 占内存少)</span></a></code></pre></div><div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1">net3 <span class="op">=</span> torch.nn.Sequential(</a><a class="sourceLine" id="cb15-2" data-line-number="2">    torch.nn.Linear(<span class="dv">1</span>, <span class="dv">10</span>),</a><a class="sourceLine" id="cb15-3" data-line-number="3">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb15-4" data-line-number="4">    torch.nn.Linear(<span class="dv">10</span>, <span class="dv">1</span>)</a><a class="sourceLine" id="cb15-5" data-line-number="5">)</a><a class="sourceLine" id="cb15-6" data-line-number="6">net3.load_state_dict(torch.load(<span class="st">&#39;net_params.pkl&#39;</span>))</a><a class="sourceLine" id="cb15-7" data-line-number="7">prediction <span class="op">=</span> net3(x)</a></code></pre></div></li></ul><h3 id="六批数据训练mini_batch-training">六、批数据训练(mini_batch training)</h3><ul><li><p>将数据分批训练，一个epoch训练所有批次的数据：</p><div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="im">import</span> torch.utils.data <span class="im">as</span> Data</a><a class="sourceLine" id="cb16-2" data-line-number="2">BATCH_SIZE <span class="op">=</span> <span class="dv">5</span> <span class="co"># 抽取训练的数据</span></a><a class="sourceLine" id="cb16-3" data-line-number="3"><span class="co"># BATCH_SIZE = 8</span></a><a class="sourceLine" id="cb16-4" data-line-number="4"></a><a class="sourceLine" id="cb16-5" data-line-number="5">x <span class="op">=</span> torch.linspace(<span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">10</span>)       <span class="co"># this is x data (torch tensor)</span></a><a class="sourceLine" id="cb16-6" data-line-number="6">y <span class="op">=</span> torch.linspace(<span class="dv">10</span>, <span class="dv">1</span>, <span class="dv">10</span>)       <span class="co"># this is y data (torch tensor)</span></a><a class="sourceLine" id="cb16-7" data-line-number="7"></a><a class="sourceLine" id="cb16-8" data-line-number="8">torch_dataset <span class="op">=</span> Data.TensorDataset(data_tensor <span class="op">=</span> x, target_tensor <span class="op">=</span> y)</a><a class="sourceLine" id="cb16-9" data-line-number="9">loader <span class="op">=</span> Data.DataLoader(</a><a class="sourceLine" id="cb16-10" data-line-number="10">    dataset<span class="op">=</span>torch_dataset,      <span class="co"># torch TensorDataset format</span></a><a class="sourceLine" id="cb16-11" data-line-number="11">    batch_size<span class="op">=</span>BATCH_SIZE,      <span class="co"># mini batch size</span></a><a class="sourceLine" id="cb16-12" data-line-number="12">    shuffle<span class="op">=</span><span class="va">True</span>,               <span class="co"># random shuffle for training</span></a><a class="sourceLine" id="cb16-13" data-line-number="13">    num_workers<span class="op">=</span><span class="dv">2</span>,              <span class="co"># 多线程来读数据</span></a><a class="sourceLine" id="cb16-14" data-line-number="14">)</a><a class="sourceLine" id="cb16-15" data-line-number="15"></a><a class="sourceLine" id="cb16-16" data-line-number="16"><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):   <span class="co"># 训练所有!整套!数据 3 次</span></a><a class="sourceLine" id="cb16-17" data-line-number="17">    <span class="cf">for</span> step, (batch_x, batch_y) <span class="kw">in</span> <span class="bu">enumerate</span>(loader):  <span class="co"># 每一步 loader 释放一小批数据用来学习</span></a><a class="sourceLine" id="cb16-18" data-line-number="18">        <span class="co"># 假设这里就是你训练的地方...</span></a><a class="sourceLine" id="cb16-19" data-line-number="19">        <span class="co"># 打出来一些数据</span></a><a class="sourceLine" id="cb16-20" data-line-number="20">        <span class="bu">print</span>(<span class="st">&#39;Epoch: &#39;</span>, epoch, <span class="st">&#39;| Step: &#39;</span>, step, <span class="st">&#39;| batch x: &#39;</span>,</a><a class="sourceLine" id="cb16-21" data-line-number="21">              batch_x.numpy(), <span class="st">&#39;| batch y: &#39;</span>, batch_y.numpy())</a></code></pre></div></li><li><p>DataLoader 是PyTorch中数据读取的接口，PyTorch训练模型基本都会用到该接口，其目的：将dataset根据batch_size大小、shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。</p></li><li><p>enumerate()函数 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。在这里就是把是个数据分成size为5的两份数据后，将每一份数据对应的下标给step，数据给(batch_x, batch_y)。</p></li></ul><h2 id="day-04">day 04</h2><h3 id="一优化器optimizer加速神经网络训练深度学习">一、优化器Optimizer加速神经网络训练（深度学习）</h3><ul><li>数据分批送入网络，进行SGD优化；</li><li>Momentum更新参数方法：<span class="math inline">\(m=b1*m-Learningrate*dx,W+=m\)</span></li><li>AdaGrad：<span class="math inline">\(v+=dx^2，W+=-Learning rate*dx/\sqrt v\)</span></li><li>RMSProp方法(上述两种的合并)：<span class="math inline">\(v=b1*v+(1-b1)*dx^2,W+=-Learning_rate*dx/\sqrt v\)</span></li><li>Adam:<span class="math inline">\(m = b1*m+(1-b1)*dx\)</span>——&gt;Momentum 　　　 <span class="math inline">\(v = b2*v+(1-b2)*dx^2\)</span>——&gt;AdaGrad 　　　 <span class="math inline">\(W+=-Learning_rate*m/\sqrt v\)</span></li></ul><h3 id="二opttimizer优化器">二、Opttimizer优化器 　</h3><ul><li><p>几种常见优化器： 　</p><div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="co"># different optimizers</span></a><a class="sourceLine" id="cb17-2" data-line-number="2">opt_SGD <span class="op">=</span> torch.optim.SGD(net_SGD.parameters(), lr<span class="op">=</span>LR)</a><a class="sourceLine" id="cb17-3" data-line-number="3">opt_Momentum <span class="op">=</span> torch.optim.SGD(net_Momentum.parameters(), lr<span class="op">=</span>LR, momentum<span class="op">=</span><span class="fl">0.8</span>)</a><a class="sourceLine" id="cb17-4" data-line-number="4">opt_RMSprop <span class="op">=</span> torch.optim.RMSprop(net_RMSprop.parameters(), lr<span class="op">=</span>LR, alpha<span class="op">=</span><span class="fl">0.9</span>)</a><a class="sourceLine" id="cb17-5" data-line-number="5">opt_Adam <span class="op">=</span> torch.optim.Adam(net_Adam.parameters(), lr<span class="op">=</span>LR, betas<span class="op">=</span>(<span class="fl">0.9</span>, <span class="fl">0.99</span>))</a><a class="sourceLine" id="cb17-6" data-line-number="6">optimizers <span class="op">=</span> [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</a></code></pre></div></li></ul><p><img src="https://img-blog.csdnimg.cn/20200323135943525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"> 　　从图中可以看出，目前性能最优的应该是Adam。</p><h3 id="三-卷积神经网络cnn">三、 卷积神经网络(CNN)</h3><p>　　图像处理中，不是对每个像素点卷积处理，而是对一小块区域进行计算，这样加强了图像信息的连续性，使得神经网络能看到图片信息而非一个点，同时加深了神经网络对图片的理解。批量过滤器每次对图像收集一小块信息，最后将这些整理出来得到边缘信息，再对这些信息进行类似的处理，得到更高层的信息结构(例如眼睛、鼻子等)，最后把总结出来的信息套入几层full connection进行分类等操作。卷积操作时，神经层会丢失一些信息，池化层可以将Layer中有用的信息筛选出来给下一层，因此图片的长宽不断压缩，压缩的工作是池化层进行的。</p><div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1">1、import需要的工具包和库：torch、torchvision、torch.nn、torch.utils.data</a><a class="sourceLine" id="cb18-2" data-line-number="2">2、超参数：EPOCH、BATCH_SIZE、LR</a><a class="sourceLine" id="cb18-3" data-line-number="3">3、下载mnist数据集：torchvision.datasets.MNIST(root<span class="op">=</span><span class="st">&#39;./mnist/&#39;</span>,train<span class="op">=</span><span class="va">True</span>,transform<span class="op">=</span>torchvision.transform.ToTensor(),download<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb18-4" data-line-number="4"> <span class="co">#root是保存或提取的位置，transform是将数据集PIL.Image or numpy.ndarray转换成torch.FloatTensor(C×H×W)，训练的时候normalize成[0,1]间的值</span></a><a class="sourceLine" id="cb18-5" data-line-number="5"> test数据集处理：test—_x,test_y</a><a class="sourceLine" id="cb18-6" data-line-number="6"> 4、批训练train_loader定义：Data.DataLoader(dataset<span class="op">=</span>train_data,batch_size<span class="op">=</span>BATCH_SIZE,shuffle<span class="op">=</span><span class="va">True</span>)</a><a class="sourceLine" id="cb18-7" data-line-number="7"> 5、定义网络构架CNN(nn.Module):conv1—conv2—RELU—pooling—conv2—ReLU—pooling—output</a><a class="sourceLine" id="cb18-8" data-line-number="8"> 网络计算流程：conv1(x)——conv2(x)——展平多维卷积图——计算输出</a><a class="sourceLine" id="cb18-9" data-line-number="9"> 6、定义optimizer和loss function</a><a class="sourceLine" id="cb18-10" data-line-number="10"> 7、训练和测试</a></code></pre></div><h3 id="四什么是lstm循环卷积网络rnn">四、什么是LSTM循环卷积网络(RNN)</h3><ul><li>LSTM(Long Short-Term Memory)——长短期记忆</li><li>RNN是在有序的数据上进行学习 <img src="https://img-blog.csdnimg.cn/20200323202037102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>分类问题(mnist数据集) 　我们将图片数据看成一个时间上的连续数据, 每一行的像素点都是这个时刻的输入, 读完整张图片就是从上而下的读完了每行的像素点。然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了。</li><li>回归问题 这部分内容参考<a href="https://blog.csdn.net/qq_41639077/article/details/105218095" target="_blank" rel="noopener">KiKi的另一篇blog</a>，包括分类问题和回归问题的pytorch实现。</li></ul><h3 id="五自编码非监督学习autoencoder">五、自编码/非监督学习（Autoencoder）　</h3><p>　　原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作。 所以, 何不<strong>压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了。</strong> 训练好的自编码中间这一部分就是能总结原数据的精髓，我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。(摘自莫烦python) * 代码</p><pre><code>```pythonself.encoder = nn.Sequential(    nn.Linear(28*28, 128),    nn.Tanh(),    nn.Linear(128, 64),    nn.Tanh(),    nn.Linear(64, 12),    nn.Tanh(),    nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt)self.decoder = nn.Sequential(    nn.Linear(3, 12),    nn.Tanh(),    nn.Linear(12, 64),    nn.Tanh(),    nn.Linear(64, 128),    nn.Tanh(),    nn.Linear(128, 28*28),    nn.Sigmoid(),       # compress to a range (0, 1))def forward(self, x):    encoded = self.encoder(x)    decoded = self.decoder(encoded)    return encoded, decodedautoencoder = AutoEncoder()```</code></pre><h3 id="六gan生成对抗网络">六、GAN—生成对抗网络</h3><p>　（原理已经学习过了，直接上代码）</p><ul><li><p>pytorch中实现：(代码中的对象不是图像，用到的是二次曲线) 　<br></p></li><li><p>超参数</p><pre><code>  ```python  BATCH_SIZE = 64  LR_G = 0.0001 # 生成器的学习率  LR_D = 0.0001 # 判别器的学习率  N_IDEAS = 5 # random_noise的个数  ART_COMPONENTS = 15  # 定义规格，一条曲线上有多少个点  PAINT_POINTS = np.vstack([np.linspace(-1,1,ART_COMPONENTS)for _ in range(BATCH_SIZE)]) # 规定整批画的点，从-1到1共15个点  ``` </code></pre></li><li><p>没有train data，自己伪造一些real data</p><pre><code>  ```python  def artist_works():     # painting from the famous artist (real target)      a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis] # 二次曲线的系数      paintings = a * np.power(PAINT_POINTS, 2) + (a-1)  # 二次曲线的参数，区间表示upper和      paintings = torch.from_numpy(paintings).float()      return paintings  ```     </code></pre></li><li><p>定义生成器和判别器</p><pre><code>  ```python  G = nn.Sequential(                      # Generator      nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)      nn.ReLU(),      nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas  )  D = nn.Sequential(                      # Discriminator      nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G      nn.ReLU(),      nn.Linear(128, 1),      nn.Sigmoid(),                       # tell the probability that the art work is made by artist  )  ```</code></pre></li><li><p>优化器</p><pre><code>  ```python      opt_D = torch.optim.Adam(D.parameters(), lr=LR_D)  opt_G = torch.optim.Adam(G.parameters(), lr=LR_G)  ```</code></pre></li><li><p>训练啦</p><pre><code>  ```python  for step in range(10000):      artist_paintings = artist_works()           # real painting from artist      G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas      G_paintings = G(G_ideas)                    # fake painting from G (random ideas)      prob_artist0 = D(artist_paintings)          # D try to increase this prob      prob_artist1 = D(G_paintings)               # D try to reduce this prob      D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))      G_loss = torch.mean(torch.log(1. - prob_artist1))      opt_D.zero_grad()      D_loss.backward(retain_graph=True)      # reusing computational graph      opt_D.step()      opt_G.zero_grad()      G_loss.backward()      opt_G.step()  ```</code></pre></li></ul><p><strong>补充：</strong> cGAN与GAN的区别在于多了一个类别标签，这个label会跟随noise一起输入到生成器中，并且也要跟随fake和real一起输入到判别其中，最终计算各自的loss。</p><h3 id="七为什么torch是动态的待补充">七、为什么Torch是动态的<font color="red">(待补充)</font></h3><p>　例子：RNN网络 　Tensorflow就是预先定义好要做的task的框架、步骤，然后开启会话之后喂数据一步到位的计算出结果，开启会话后便不能修改网络构架了，只能是照着计算流图跟着计算，所以是静态的；Torch也可以先定义好框架然后套进去，但计算的时候无论网络怎么变化每一个叶子节点的梯度都能给出，tensorflow就做不到这一点，并且torch是边给出计算图纸一边进行训练。torch就像是散装的一样，可以一块一块的制作好并进行计算，比较灵活，所以是动态的。</p><h3 id="八gpu加速">八、GPU加速</h3><p>　以之前CNN为例，对其代码进行修改</p><ul><li><p>dataset部分</p><div class="sourceCode" id="cb25"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb25-1" data-line-number="1">test_x <span class="op">=</span> torch.unsqueeze(test_data.test_data, dim<span class="op">=</span><span class="dv">1</span>).<span class="bu">type</span>(torch.FloatTensor).cuda()<span class="op">/</span><span class="fl">255.</span>   <span class="co"># Tensor on GPU</span></a><a class="sourceLine" id="cb25-2" data-line-number="2">test_y <span class="op">=</span> test_data.test_labels.cuda()</a></code></pre></div></li><li><p>CNN网络的参数改为GPU兼容形式</p><div class="sourceCode" id="cb26"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">class</span> CNN(nn.Module):</a><a class="sourceLine" id="cb26-2" data-line-number="2">    ...</a><a class="sourceLine" id="cb26-3" data-line-number="3">cnn <span class="op">=</span> CNN()</a><a class="sourceLine" id="cb26-4" data-line-number="4"><span class="co">##########转换cnn到CUDA#########</span></a><a class="sourceLine" id="cb26-5" data-line-number="5">cnn.cuda()   <span class="co"># Moves all model parameters and buffers to the GPU.</span></a></code></pre></div></li><li><p>training data变成GPU形式</p><div class="sourceCode" id="cb27"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb27-1" data-line-number="1"><span class="cf">for</span> epoch ..:</a><a class="sourceLine" id="cb27-2" data-line-number="2">    <span class="cf">for</span> step, ...:</a><a class="sourceLine" id="cb27-3" data-line-number="3">        <span class="co">##########修改1###########</span></a><a class="sourceLine" id="cb27-4" data-line-number="4">        b_x <span class="op">=</span> x.cuda()    <span class="co"># Tensor on GPU</span></a><a class="sourceLine" id="cb27-5" data-line-number="5">        b_y <span class="op">=</span> y.cuda()    <span class="co"># Tensor on GPU</span></a><a class="sourceLine" id="cb27-6" data-line-number="6">        ...</a><a class="sourceLine" id="cb27-7" data-line-number="7"></a><a class="sourceLine" id="cb27-8" data-line-number="8">        <span class="cf">if</span> step <span class="op">%</span> <span class="dv">50</span> <span class="op">==</span> <span class="dv">0</span>:</a><a class="sourceLine" id="cb27-9" data-line-number="9">            test_output <span class="op">=</span> cnn(test_x)</a><a class="sourceLine" id="cb27-10" data-line-number="10"></a><a class="sourceLine" id="cb27-11" data-line-number="11">            <span class="co"># !!!!!!!! 修改2  !!!!!!!!! #</span></a><a class="sourceLine" id="cb27-12" data-line-number="12">            pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a><a class="sourceLine" id="cb27-13" data-line-number="13"></a><a class="sourceLine" id="cb27-14" data-line-number="14">            accuracy <span class="op">=</span> torch.<span class="bu">sum</span>(pred_y <span class="op">==</span> test_y) <span class="op">/</span> test_y.size(<span class="dv">0</span>)</a><a class="sourceLine" id="cb27-15" data-line-number="15">            ...</a><a class="sourceLine" id="cb27-16" data-line-number="16"></a><a class="sourceLine" id="cb27-17" data-line-number="17">test_output <span class="op">=</span> cnn(test_x[:<span class="dv">10</span>])</a><a class="sourceLine" id="cb27-18" data-line-number="18"></a><a class="sourceLine" id="cb27-19" data-line-number="19"><span class="co"># !!!!!!!! 修改3 !!!!!!!!! #</span></a><a class="sourceLine" id="cb27-20" data-line-number="20">pred_y <span class="op">=</span> torch.<span class="bu">max</span>(test_output, <span class="dv">1</span>)[<span class="dv">1</span>].cuda().data.squeeze()  <span class="co"># 将操作放去 GPU</span></a><a class="sourceLine" id="cb27-21" data-line-number="21">...</a><a class="sourceLine" id="cb27-22" data-line-number="22"><span class="bu">print</span>(test_y[:<span class="dv">10</span>], <span class="st">&#39;real number&#39;</span>)</a></code></pre></div></li></ul><h2 id="day-05">day 05</h2><h3 id="一过拟合overfitting">一、过拟合(Overfitting)</h3><ul><li><p>过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。模型在训练集上效果好，然而在测试集上效果差，模型泛化能力差。</p></li><li><p>原因 <br> 　1）在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候； <br> 　2）权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。 　</p></li><li><p>解决方法</p><p>方法一： <strong>增加数据量</strong>。 <br> 方法二：<strong>运用正规化</strong>，L1、 L2 regularization等等。（神经网络的正规化方法<strong>dropout</strong>——就是在训练的时候, 随机忽略掉一些神经元和神经联结 , 使这个神经网络变得”不完整”，用这个不完整的神经网络训练一次。第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象每次训练的时候, 让每一次预测结果不会依赖于其中某部分特定的神经元。像l1, l2正规化一样, 过度依赖的 W , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数，Dropout 的做法是从根本上让神经网络没机会过度依赖。）</p></li><li><p>Dropout</p><div class="sourceCode" id="cb28"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb28-1" data-line-number="1"><span class="co"># 不加dropout的网络</span></a><a class="sourceLine" id="cb28-2" data-line-number="2">net_overfitting <span class="op">=</span> torch.nn.Sequential(</a><a class="sourceLine" id="cb28-3" data-line-number="3">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a><a class="sourceLine" id="cb28-4" data-line-number="4">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-5" data-line-number="5">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a><a class="sourceLine" id="cb28-6" data-line-number="6">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-7" data-line-number="7">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a><a class="sourceLine" id="cb28-8" data-line-number="8">    )</a><a class="sourceLine" id="cb28-9" data-line-number="9"></a><a class="sourceLine" id="cb28-10" data-line-number="10">    <span class="co"># 加上dropout</span></a><a class="sourceLine" id="cb28-11" data-line-number="11">net_dropped <span class="op">=</span> torch.nn.Sequential(</a><a class="sourceLine" id="cb28-12" data-line-number="12">    torch.nn.Linear(<span class="dv">1</span>, N_HIDDEN),</a><a class="sourceLine" id="cb28-13" data-line-number="13">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a><a class="sourceLine" id="cb28-14" data-line-number="14">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-15" data-line-number="15">    torch.nn.Linear(N_HIDDEN, N_HIDDEN),</a><a class="sourceLine" id="cb28-16" data-line-number="16">    torch.nn.Dropout(<span class="fl">0.5</span>),  <span class="co"># drop 50% of the neuron</span></a><a class="sourceLine" id="cb28-17" data-line-number="17">    torch.nn.ReLU(),</a><a class="sourceLine" id="cb28-18" data-line-number="18">    torch.nn.Linear(N_HIDDEN, <span class="dv">1</span>),</a><a class="sourceLine" id="cb28-19" data-line-number="19">    )＃除了网络构架不同外，其他大同小异。</a></code></pre></div><p>　 　</p></li></ul><h3 id="二批标准化batch-normalization">二、批标准化(Batch Normalization)</h3><ul><li><p>什么是批标准化 <br> 　Batch Normalization(BN), 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法。 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律。数据随着神经网络的传递计算，激活函数的存在会造成网络层对数据的不敏感，比如0.1和2经过Tanh函数后，前者仍0.1，而2变成1，那这样再大的数都会变成1，所以神经层对数据失去了感觉，这样的问题同样存在于隐藏层中，所以BN则是用在这些神经层中优化网络的方法。 <br> 　Batch就是数据分批处理，每一批数据前向传递的过程中，每一层都进行BN处理，添加在层和激励函数之间。反BN：<span class="math inline">\(BN_(\gamma,\beta)(x_i)\)</span>是将 normalize 后的数据再扩展和平移，是为了让神经网络自己去学着使用和修改这个扩展参数 <span class="math inline">\(\gamma\)</span>, 和 平移参数<span class="math inline">\(\beta\)</span>, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 <span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>来抵消一些 normalization 的操作。</p></li><li><p>代码 　莫烦<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py" target="_blank" rel="noopener">BN_code</a></p></li></ul><p>　　　　　　<font color="red">部分内容待学习</font></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>pytorch学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN-循环神经网络入门</title>
    <link href="/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/"/>
    <url>/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="循环神经网络入门pytorch实现">循环神经网络入门（pytorch实现）</h2><p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著  参考视频：<a href="https://www.bilibili.com/video/BV134411w76S" target="_blank" rel="noopener">微软人工智能公开课—循环神经网络RNN</a></p><p><strong>一、回顾前馈神经网络</strong> <br> 　　RNN是在前馈式神经网络基础上的，所以先回顾一下前馈神经网络。 <br> 　　前馈式神经网络（FNN, Feedforward Neural Network）一般的结构如下图： 　　<img src="https://img-blog.csdnimg.cn/20200403141749146.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="前馈式神经网络"> 图中神经元的输出表示为：<span class="math inline">\(O_t=f(\sum_{i=1}^n a_iw_i+b)\)</span>假设某一层的输出为<span class="math inline">\(h_n\)</span>，则下一层的输出为<span class="math inline">\(f_{out}(h_n)\)</span>，这样重复迭代网络的输出为<span class="math inline">\(y=f_{out}(f_n(f_{n-1}(...)))\)</span>，如此一来神经网络就成了巨大的复合函数，但实际我们并不会用到这个函数来计算，而使用计算图来表示，这样方便得出每一个节点的输入输出数据的梯度。对于RNN，就从这样的简单的前馈神经网络的传递方式说起。</p><p><strong>二、RNN网络简介</strong> <br> 　　循环神经网络（ Recurrent Neural Network, RNN ）雏形见于美国物理学家 J.J.Hopfield 于 1982 年提出的可用作联想存储器的互联网络——Hopfield 神经网络模型，如下图，每个节点都有输入，两两节点之间有双相连接： 　　<img src="https://img-blog.csdnimg.cn/20200403144156566.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Hopfiled Network"> 　　基于传统的机器学习算法十分依赖人工特征的提取致使模型一直无法提高正确率，而之前的全连神经网络以及卷积神经网络中，信息只在层与层之间存在运算关系，而没有连接节点之间的信息流动，这样在每一个时间都会有一个单独的参数，<font color="red">因此不能在时间上共享不同序列长度或序列不同位置的统计强度，所以无法对训练时没有见过的序列长度进行泛化</font>。 所以发展RNN对具有时间序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，在模型的不同部分共享参数解决了这个问题，并使得模型能够扩展到对不同形式的样本（这里指不同长度的样本）进行泛化。 <strong>循环神经网络会在几个时间步内共享相同的权重以刻画一个序列当前的输出与之前信息的关系，这体现在结构上是循环神经网络的隐藏层之间存在连接，隐藏层的输入来自于输入层的数据以及上一时刻隐藏层的输出</strong>。这样的结构使得循环神经网络会对之前的信息有所记忆， 同时利用之前的信息影响后面节点的输出。 <br> 　　RNN网络主要用于处理离散序列数据：离散线性、长度可变的序列，例如时域语音信号，金融市场走势等。网络可用于序列数据的分析（市场趋势预测）、序列数据的生产（基于图片的文字描述）、序列数据的转换（语音识别以及机器翻译）。</p><p><strong>三、循环网络结构</strong> <br> 　 　循环神经网络典型结构及其按时间先后展开结构如图所示：　 　 　<img src="https://img-blog.csdnimg.cn/20200403160334980.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="RNN"> 对于主体结构<span class="math inline">\(A\)</span>，一般可认为是循环神经网络的一个隐藏单元。在<span class="math inline">\(t\)</span>时刻，主体结构<span class="math inline">\(A\)</span>会读取输入层的输入<span class="math inline">\(x_t\)</span>以及上一时刻的输出，并输出当前时刻的<span class="math inline">\(o_t\)</span>值（图中未给出）。此后<span class="math inline">\(A\)</span>结构在<span class="math inline">\(t\)</span>时刻的状态值表达式：<span class="math inline">\(h^t=f(h^{t-1},x_t;\theta)\)</span>，其中<span class="math inline">\(\theta\)</span>可以是网络中的其它参数比如权重或偏置等，循环的过程就是<span class="math inline">\(A\)</span>不断被执行的过程。但是循环神经网络目前无法做到无限循环，因为循环过多会出现梯度消失的问题。 <br> 　　假设隐藏单元的激活函数是tanh函数，则循环体结构<span class="math inline">\(A\)</span>如下：　　<img src="https://img-blog.csdnimg.cn/20200403155812724.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="循环体"> 则隐藏单元状态值表示为：<span class="math inline">\(h^t=tanh(Wh^{t-1}+Ux_t+b^h)\)</span>，<span class="math inline">\(b^h\)</span>是由<span class="math inline">\(x_t\)</span>得到<span class="math inline">\(h^t\)</span>的偏置，<span class="math inline">\(W\)</span>是相邻时刻隐藏单元间的权重矩阵，<span class="math inline">\(U\)</span>是从<span class="math inline">\(x_t\)</span>计算得到这个隐藏单元时用到的权重矩阵。从当前的状态值<span class="math inline">\(h^t\)</span>都得到输出还需要一个全连接神经网络来完成这个过程，表达式为：<span class="math inline">\(o^t=b_o+Vh^t\)</span>，<span class="math inline">\(V\)</span>是由<span class="math inline">\(h^t\)</span>得到<span class="math inline">\(o^t\)</span>的权重矩阵。如果输出是离散的，则可以用softmax处理<span class="math inline">\(o^t\)</span>得到标准化的概率向量<span class="math inline">\(y\)</span>：<span class="math inline">\(y^t=softmax(o^t)\)</span>，向量的值对应离散变量可能值的概率。</p><p><strong>四、网络的训练</strong> <br> 　　要对网络进行训练，需要有一个与<span class="math inline">\(x\)</span>序列配对的<span class="math inline">\(o\)</span>的所有时间步内的总loss。 <br> 　　RNN的反向传播算法称为时间反向传播 (Back-Propagation Through Time, BPTT)。基本原理和 BP 算法是一样的，三步走：前向计算每个神经元的输出值；反向计算误差项值；计算每个权重的梯度；最后再用随机梯度下降算法更新权重。详细的计算公式参考<a href="https://zhuanlan.zhihu.com/p/85776566" target="_blank" rel="noopener">知乎</a>。在反向传播过程中，当输入序列过长的时候，在求取一个比较远的时刻的梯度时，需要回溯到前面的所有时刻的信息，由于连乘项的存在，导致前面时刻的信息会缺失，这就是RNN中的梯度消失问题，还有就是当连乘出现大于1时的梯度爆炸，后者采用clip的方式即可，前者将会用到LSTM来缓解。</p><p><strong>五、LSTM</strong> <br> 　　LSTM 结构由 Sepp Hochreiter教授和 Jurgen Schrnidhuber教授于 1997 年 提出，它本身就是一种特殊的循环体结构 。在一个整体的循环神经网络中，除了外部的RNN大循环（<strong>循环体是 LSTM</strong>）外，还需要考虑 LSTM 本身单元“细胞”之间的自循环。LSTM单元结构如下图： 　　<img src="https://img-blog.csdnimg.cn/20200403173928627.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM单元"> LSTM由自身的三个门结构进行控制，门结构使用Sigmoid函数对输入的信息进行控制，让信息有选择性的影响RNN中每个时刻的状态。遗忘门（Forget Gate）是让网络“忘记”之前没用的信息，上一时刻的状态值通过与自循环权重 (该权重就是遗忘门的输出) 进行位乘可得到当前时刻状态值的一个加数。遗忘门输出表达式：</p><p><span class="math display">\[f_i^t=sigmoid(b_i^f+\sum U_{i,j}^fx_j^t+\sum W_{i,j}^fh_j^{t-1})\]</span></p><p>其中，<span class="math inline">\(h^{t-1}\)</span>是包含一个LSTM细胞上一时刻的所有输出，可以被看作是当前隐藏层向量，数量为<span class="math inline">\(j\)</span>，<span class="math inline">\(W\)</span>是循环权重。要保存长期的记忆，还需要输入门，同样是根据<span class="math inline">\(x^t\)</span>、<span class="math inline">\(b^g\)</span>和<span class="math inline">\(h^{t-1}\)</span>来决定哪些部分将进入当前时刻的状态，输入门的值表示为：</p><p><span class="math display">\[g_i^t=sigmoid(b_i^g+\sum U_{i,j}^gx_j^t+\sum W_{i,j}^gh_j^{t-1})\]</span></p><p>进一步计算LSTM结构当前时刻的状态值：</p><p><span class="math display">\[C_i^t=f_i^tC_i^{t-1}+g_i^ttanh(b_i+\sum U_{i,j}x_j^t+\sum W_{i,j}h_j^{t-1})\]</span></p><p>其中，<span class="math inline">\(W\)</span>值为遗忘门的循环权重。然后得出结构当前时刻的输出值：</p><p><span class="math display">\[h^t=tanh(C_i^t)q_i^t\]</span></p><p><span class="math display">\[q_i^t=sigmoid(b_i^q+\sum U_{i,j}^qx_j^t+\sum W_{i,j}^qh_j^{t-1})\]</span></p><p>其中，<span class="math inline">\(W^q\)</span>为遗忘门的循环权重，<span class="math inline">\(q_i^t\)</span>为输出们的输出值。用LSTM作为循环体的RNN网络如下图所示： <img src="https://img-blog.csdnimg.cn/20200403192938761.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM-RNN"></p><p>　　循环神经网络的变种还有：双向循环神经网络和深层循环神经网络。</p><ul><li>至此，关于RNN和LSTM的学习就结束啦~ 用pytorch实现LSTM的RNN对MNIST数据集分类，以及RNN实现一元二次曲线的回归的代码地址：<a href="https://gitee.com/sparklekk/RNN" target="_blank" rel="noopener">sparklekk</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度学习模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>循环神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>总结经典深度学习网络（pytorch、tensorflow实现）</title>
    <link href="/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/"/>
    <url>/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="总结经典的深度学习模型">总结经典的深度学习模型</h2><p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著</p><h3 id="一lenet-5">一、LeNet-5</h3><p><strong>1、LeNet-5网络简介</strong></p><p>　　LeNet-5是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。LeNet-5 模型由Yann LeCun教授于1998年提出， 在MNIST数据集上，LeNet-5模型可以达到大约99.4%的准确率。与近几年的卷积神经网络比较，LeNet-5的网络规模比较小，但却包含了构成现代CNN网络的基本组件——卷积层、 Pooling层、全连接层。 <br> <strong>2、模型结构</strong> <br> 　　网络一共有8层(包含输入和输出在内)，基本网络架构如下： <img src="https://img-blog.csdnimg.cn/20200401163708326.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="ＬｅＮｅｔ－５"> 备注：图中C表示卷积层 ，S表示池化层。 　 - <font color="red"><strong>Layer1</strong></font>：Input层，输入图片大小32×32×1，MNIST数据集大小是28×28，所以输入reshape为32×32是希望高层特征监测感受野的中心能够收集更多潜在的明显特征。</p><ul><li><p><font color="red"><strong>Layer2</strong></font>：Conv1层——卷积层，有6个Feature Map，即Convolutions操作时的卷积核的个数为6，卷积核大小为5×5，该层每个单元和输入层的25个单元连接，padding没有使用，stride为1。</p></li><li><p><font color="red"><strong>Layer3</strong></font>：S2为下采样层(Subsampling)，6个14×14的特征图，是上一层的每个特征图经过2×2的最大池化操作得到的，且长宽步长都为2，S2层每个特征图的每一个单元与C3对应特征图中2×2大小的区域连接。</p></li><li><p><font color="red"><strong>Layer4</strong></font>：Conv3层，由上一层的特征图经过卷积操作得到，卷积核大小为5×5，卷积核个数为16，即该层有16个特征图，但并不是与上一层的6个特征图一一对应，而是有固定的连接关系，例如第一个特征图只与Layer3的第1、2、3个特征图的有卷积关系。</p></li><li><p><font color="red"><strong>Layer5</strong></font>：S4层，16个5×5的特征图，每个特征图都是由第四层经过一个2×2的最大池化操作得到的，意义同Layer3。</p></li><li><p><font color="red"><strong>Layer6</strong></font>：Conv5，120个特征图，是由上一层输出经过120个大小为5×5的卷积核得到的，没有padding，stride为1，上一层的16个特征图都连接到该层的每一个单元，所以这里相当于一个全连接层。</p></li><li><p><font color="red"><strong>Layer7</strong></font>：F6是全连接层，有84个神经元，与上一层构成全连接的关系，再经由Sigmoid激活函数传到输出层。</p></li><li><p><font color="red"><strong>Layer8</strong></font>：Output层也是一个全连接层，共有10个单元，对应0~9十个数字。本层单元计算的是径向基函数：<span class="math inline">\(y_i =\sum_{j}(x-w_{i,j})^2\)</span>,RBF的计算与第i个数字的比特图编码有关，对于第i个单元，yi的值越接近0，则表示越接近第i个数字的比特编码，即识别当前输入的结果为第i个数字。</p></li></ul><p><strong>3、pytorch和tensorflow实现LeNet-5网络的MNIST手写数字识别</strong> <br> 　　代码地址：<a href="https://github.com/KK-xi/LeNet-5" target="_blank" rel="noopener">GitHub</a> <br> 　　LeNet-5网络规模比较小，所以它无法很好的处理类似ImageNet的比较大的图像数据集。</p><h3 id="二alexnet">二、AlexNet</h3><p>1、<strong>AlexNet网络简介</strong> <br> 　　2012年， Hinton的学生Alex Krizhevsky借助深度学习的相关理论提出了深度卷积神经网络模型AlexNet。卷积层的数量有5个，池化层的数量有3个，也就是说，并不是所有的卷积层后面都连接有池化层。在这些卷积与池化层之后是3个全连层，最后一个全连层的单元数量为1000个，用于完成对ImageNet数据集中的图片完成1000分类（具体分类通过Softmax层实现)。</p><p>2、<strong>模型结构</strong> <br>　　 　　网络结构如下图所示，有两个子网络，可以用<font color="red">GPU分别进行训练</font>(特点1)，两个GPU之间存在通信： 　　<img src="https://img-blog.csdnimg.cn/20200401164504200.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="AlexNet"></p><ul><li><p><font color="red"><strong>第一段卷积</strong></font>: 使用了96个11×11卷积核对输入的224×224×3的RGB图像进行卷积操作，长宽移动步长均为4，得到的结果是96个55x55的特征图；得到基本的卷积数据后，第二个操作是<font color="red">ReLU去线性化</font>(这是AlexNet的特点2)；第三个操作是<font color="red">LRN局部归一化</font>（AlexNet首次提出,特点3)；第四个操作是3×3的max pooling，步长为2。</p></li><li><p><font color="red"><strong>第二段卷积</strong></font>：流程类似第一阶段，用到的核大小不同，256个深度为3(因为是三通道图像)的5×5的卷积核，stride为1×1，然后ReLU去线性化，再是LRN局部归一化，最后是3×3的最大池化操作，步长为2。</p></li><li><p><font color="red"><strong>第三段卷积</strong></font>：输入上一阶段的特征图，首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，这一阶段没有LRN和池化。</p></li><li><p><font color="red"><strong>第四段卷积</strong></font>：首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化。</p></li><li><p><font color="red"><strong>第五段卷积</strong></font>：首先经过256个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，最后是3×3的最大池化操作，步长为2。</p></li><li><p><font color="red"><strong>第六段全连接(FC)</strong></font>：以上过程完了以后经过三层全连接层，前两层有4096个单元，与前一层构成全连接关系，然后都经过一个ReLU函数，左后一层是1000个单元的全连接层(Softmax层)，训练时这几个<font color="red">全连接层使用了Dropout</font>(特点4)。</p></li></ul><p>备注：<font color="red">数据增强的运用</font>(特点5)，在训练的时候模型随机从大小为256×256的原始图像中截取224×224大小的区域，同时还得到图像水平翻转的镜像图，用以增加样本的数量。在测试时，模型会首先截取一张图片的四个角加中间的位置，并进行左右翻转，这样会获得10张图片，将这10张图片作为预测的输入并对得到的10个预测结果求均值，就是这张图片最终的预测结果。</p><p><strong>3、pytorch和tensorflow实现</strong> <br> 　　代码地址：<a href="https://github.com/KK-xi/AlexNet" target="_blank" rel="noopener">Github</a></p><h3 id="三vggnet">三、VGGNet</h3><p><strong>1、VGGNet网络简介</strong> <br> 　　2014年ILSVRC图像分类竞赛的第二名是VGGNet网络模型，其 top-5 错误率为 7.3%，它对卷积神经网络的深度与其性能之间的关系进行了探索。在将网络迁移到其他图片数据上进行应用时， VGGNet比GoogleNet有着更好的泛化性。此外，VGGNet模型是从图像中提取特征的CNN首选算法。<br></p><p><strong>2、模型结构</strong> <br> 　　网络的结构非常简洁，在整个网络中全部使用了大小相同的卷积核 (3×3 )和 最大油化核 (2x2 )。<font color="red">VGGNet模型通过不断地加深网络结构来提升性能</font>，通过重复堆叠的方式，使用这些卷积层和最大池化层成功地搭建了11～19层深的卷积神经网络。下表是这些网络的结构组成层： <img src="https://img-blog.csdnimg.cn/20200401200252309.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGGNet"> 　　表中的conv3表示大小为3×3的卷积核，conv1则是1×1的卷积核，参数量主要集中在全连接层，其他卷积层的参数共享和局部连接降低了参数的数量。表中五个阶段的卷积用于提取特征，每段卷积后面都有最大池化操作，目的是缩小图像尺寸。多个卷积的堆叠可以降低参数量，也有助于学习特征，C级的VGG用到conv1是为了在输入通道数和输出通道数不变(不发生数据降维)的情况下实现线性变换，对非线性提升效果有较好的作用，但是换成conv3效果更好。VGG19的效果只比VGG16好一点点，所以牛津的研究团队就停止在VGG19层了，不再增加更多层数了。下图是VGG-16的网络构架图： 　　<img src="https://img-blog.csdnimg.cn/20200401203149754.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGG-16"></p><p><strong>3、pytorch和tensorflow实现</strong> <br> 　　这里实现的是VGG-16网络(pytorch中只是搭建了网络)，代码地址：<a href="https://github.com/KK-xi/VGGNet-16" target="_blank" rel="noopener">GitHub</a> 　　在通常的网络训练时，<font color="red">VGGNet通过Multi-Scale方法对图像进行数据增强处理</font>，我自己在训练MNIST的时候没有用数据增强。因为卷积神经网络对于图像的缩放有一定的不变性，所以将这种经过 Multi-Scale多尺度缩放裁剪后的图片合在一起输入到卷积神经网络中训练，可以增加网络的这种<font color="red">不变性</font>(不变性的理解：pooling操作时，对局部感受野取其极大值，如果图像在尺度上发生了变化，有一定概率在尺度变化后对应的感受野取到的极大值不变，这样就可以使特征图不变，同样也增加了一定的平移不变性)。在预测时也采用了Multi-Scale的方法，将图像Scale到一个尺寸再裁剪并输入到卷积网络计算。输入到网络中的图片是某一张图片经过缩放裁剪后的多个样本，这样会得到一张图片的多个分类结果，所以紧接着要做的事就是对这些分类结果进行平均以得到最后这张图片的分类结果，这种平均的方式会提高图片数据的利用率并使分类的效果变好。</p><h3 id="四inceptionnet-v3">四、InceptionNet-V3</h3><p><strong>1、InceptionNet-V3网络简介</strong> <br> 　Google的InceptionNet首次亮相是在2014年的ILSVRC比赛中，称为 Inception-V1，后来又开发了三个版本，其中InceptionNet-V3最具代表性。相比VGGNet, Inception-V1增加了深度，达到了22层，但是其参数却只有500万个左右(SM)，这是远低于AlexNet(60M 左右)和VGGNet (140M 左右)的，是因为该网络将全连层和一般的卷积中采用了<font color="red">稀疏连接的方法(Hebbian原理)</font>。根据相关性高的单元应该被聚集在一起的结论，这些在同一空间位置但在不同通道的卷积核的输出结果也是稀疏的，也可以通过类似将稀疏矩阵聚类为较为密集的子矩阵的方式来提高计算性能。沿着这样的一个思路，Google团队提出了Inception Module结构来实现这样的目标。</p><p><strong>2、模型结构</strong> <br> 　　网络中主要用到了稀疏连接的思想提出了<font color="red">Inception Module</font>，其借鉴了论文《Network in Network》的做法，即提出的<font color="red">MLPConv</font>——使用MLP对卷积操作得到的特征图进行进一步的操作，从而得到本层的最终输出特征图，这样可以允许在输出通道之间组合信息，以此提升卷积层的表达能力。在InceptionNet-V3中，Inception Module的基本结构如下图：　　<img src="https://img-blog.csdnimg.cn/20200401233024519.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Inception Module"> 结构中的<font color="red">Filter Concat</font>是将特征图在深度方向(channel维度)进行串联拼接，这样可以构建出符合Hebbian原理的稀疏结构。conv1的作用是把相关性高、同一空间位置不同通道的特征连接在一起，并且计算量小，<strong>可以增加一层特征变换和非线性化</strong>，最大池化是为了增加网络对不同尺度的适应性。在Inception V2中首次用到了<font color="red">.批标准化(Batch Normalization)</font>，V3在V2基础上的Module<font color="red">改进之处就是在分支中使用分支，将二维卷积拆分为两个非对称一维卷积</font>，这种卷积可以在处理更丰富的空间特征以及增加特征多样性等方面做得比普通卷积更好。多个这种Inception Module堆叠起来就形成了InceptionNet-V3，其网络构架如下图，整个网络的主要思想就是找到一个最优的Inception Module结构，更好的实现局部稀疏的稠密化。 <img src="https://img-blog.csdnimg.cn/20200401235114326.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="InceptionNet-V3"></p><p><strong>3、使用InceptionNet-V3完成模型迁移学习</strong></p><ul><li><font color="red">迁移学习</font>：随着卷积神经网络模型层数的加深以及复杂度的逐渐增加，训练这些模型所需要的带有标注的数据也越来越多。比如对于ResNet，其深度有 152 层，使用ImageNet数据集中带有标注的 120 万张图片才能将其训练得到 96.5% 的准确率。尽管这是个比较不错的准确率，但是在真实应用中，几乎很难收集到这么多带有标注的图片数据，而且这些数据训练一个复杂的卷积神经网络也要花费很长的时间 。<strong>迁移学习的出现就是为了解决上述标注数据以及训练时间的问题</strong>。所谓迁移学习，就是将一个问题上训练好的模型通过简单的调整使其适用于 一个新的问题，例如在 Google 提供的基于 ImageNet 数据集训练好的 Inception V3 模型的基础上进行简单的修改，使其能够解决基于其他数据集的图片分类任务。被修改的全连接层之前的一个网络层叫做<font color="red">瓶颈层</font>(Bottleneck，这里是V3中最后一个Dropout层)。</li><li>模型迁移学习tensorflow代码地址：<a href="https://github.com/KK-xi/Inception_V3_transfer_learning" target="_blank" rel="noopener">Github</a></li></ul><h3 id="五resnet">五、ResNet</h3><p><strong>1、ResNet网络简介</strong> <br> 　　ResNet (Residual Neural Network）由微软研究院的何情明等 4 名华人提出，网络深度达到了152 层，top-5 错误率 只有3.57% 。虽然ResNet的深度远远高于 VGGNet，但是参数量却比 VGGNet 低，效果也更好。ResNet 中最创新就是<font color="red">残差学习单元（Residual Unit）</font>的引入，它是参考了瑞士教授 Schmidhuber 的论文《Training Very Deep Networks》中提出的 Highway Network。 <font color="red"> Highway Network</font> 的出现是为了解决较深的神经 网络难以训练的问题，主要思想是启发于LSTM的门(Gate)结构，使得有一定比例的前一层的信息没有经过矩阵乘法和非线性变换而是直接传输到下一层，网络要学习的就是原始信息应该以何种比例保留下来。后来残差网络的学习单元就受益于Highway Network加深网络层数的做法。</p><p><strong>2、模型结构</strong> <br> 　　随着网络加深，由于反向传播过程的叠乘可能出现<font color="red">梯度消失</font>，结果就是准确率下降，为此ReNet中引入残差学习单元(如图所示，2层和3层)，思想就是对于一个达到了准确率饱和的较浅网络，在后面加几个全等映射层允许初始信息直接传到下一层(y=x)时，误差不会因此而增加，并且网络要学习的就是原来输出<span class="math inline">\(H(x)\)</span>与原始输入<span class="math inline">\(x\)</span>的残差<span class="math inline">\(F(x)=H(x)-x\)</span>。 <img src="https://img-blog.csdnimg.cn/20200402093002712.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="残差单元"> 　　将多个残差单元堆叠起来就组成了ResNet网络，如下图所示是一个34层的残差网络： <img src="https://img-blog.csdnimg.cn/20200402095125460.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="ResNet"> 旁边的支线就是将上一层残差单元的输出直接参与到该层的输出，这样的连接方式被称为<font color="red">Shortcut和Skip Connection</font>，这样可以一定程度上保护信息的完整性。最后在改进的ResNet V2中通过该连接使用的激活函数(ReLU)被更换为Identity Mappings (<span class="math inline">\(y=x\)</span>)，残差单元都使用了BN归一化处理，使得训练更加容易并且泛化能力更强。</p><p><strong>3、pytorch和tensorflow实现ResNet</strong> <br> 　代码地址：<a href="https://github.com/KK-xi/ResNet" target="_blank" rel="noopener">GitHub</a> 　这里只是搭建的网络，没有进行任何任务的训练，可以在自己的数据集上训练试一下。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度学习模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像处理注意力机制</title>
    <link href="/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="记录最近所看的注意力机制">记录最近所看的注意力机制</h3>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>attention mechanism</category>
      
    </categories>
    
    
    <tags>
      
      <tag>attention mechanism</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲</title>
    <link href="/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/"/>
    <url>/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/</url>
    
    <content type="html"><![CDATA[<p>第一章</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>SLAM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>seu</title>
    <link href="/2020/03/30/eu/"/>
    <url>/2020/03/30/eu/</url>
    
    <content type="html"><![CDATA[<ul><li>2019.10.06 东大一角</li></ul><figure><img src="https://img-blog.csdnimg.cn/20200330115513856.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="seu"><figcaption>seu</figcaption></figure>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>单目深度估计总结</title>
    <link href="/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    <url>/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<p><bar> <bar></bar></bar></p><p><bar> <bar></bar></bar></p><p>《High Quality Monocular Depth Estimation via Transfer Learning》 作者：Ibraheem Alhashim and Peter Wonka</p><p>备注：只是一篇总结，不是解读哒~ ## 一、为什么要看这篇文章？ 1、因为最近萌生了一个想法，觉得可以用用深度图； 2、多了解一样是一样。</p><h2 id="二文章提出的出发点">二、文章提出的出发点</h2><p>1、首先，一张图片2D到3D的深度估计是很多场景理解或者是重建工作中的基础； 2、其次，这篇文章希望提出的深度估计方法能获得高分辨率的深度估计结果。</p><h2 id="三framework">三、Framework</h2><p>文章中给出的简化架构：<img src="https://img-blog.csdnimg.cn/20200314154230721.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="看似很简单的网络架构"> <strong>1、Encoder</strong> 这里用到的编码器是另一篇文章《Densely Connected Convolutional Networks》里的DenseNet-169（这篇文章就看了一下架构，还没细看，有时间去仔细看看，看了回来补充）。下图是生长率k=4的5层的Dense-block结构图，对这里的k的一种解释是，每个层都可以访问这个块中的所有前面的特征映射，因此也可以访问网络的集体特征。可以将特征图视为网络的全局状态，每层将其自身的k个特征映射添加到该状态，增长率决定了每一层对全局状态贡献多少新信息。一旦写入全局状态，就可以从网络中的任何地方访问全局状态，并且与传统的网络架构不同，不需要从一层复制到另一层。 <img src="https://img-blog.csdnimg.cn/20200314160111493.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="5-layer dense block with growth rate of k=4"> 然后，为了在网络中进行下采样（改变图像大小），将多个Dense-block连接起来，就形成了下图的样子： <img src="https://img-blog.csdnimg.cn/20200314161447623.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="Dense Net with three dense blocks"> 两个相邻块之间的层称为过渡层，并通过卷积和池化来改变特征图的大小，但是本文中是删除了最后的top-layer，因为文章是做depth estimation而不是Classification task。但是呢，深度估计这篇文章用的DensNet-169有4个blocks，网络结构参数如下： <img src="https://img-blog.csdnimg.cn/20200314162029675.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="DenseNet architectures for ImageNet"></p><p><strong>2、Decoder</strong> 对于decoder，从一个1×1的卷积层开始，其输出通道的数量与去掉top-layer的编码器的输出相同。然后依次添加2×2的双线性upsampling块和串接池化层POOL（框架图中并未画出），其后跟两个并列的3×3卷积层，这样的构造重复3次，然后就是2×2的双线性upsampling块和串接卷积层CONV，然后是两个并列的3×3的卷积层，最后经过一个3×3卷积层最终输出通道数为1的图像。</p><p>这个encoder-decoder带有跳过连接，感觉就是把两部分硬生生连在一起了，整个网络构架不太紧凑，大概的结构就这样啦~~</p><h2 id="四文章的亮点">四、文章的亮点</h2><p>1、第一点大概就是用了这么一个简单的网络来做深度估计，网络复杂不等于结果好； 2、定义了一个损失函数，通过最小化深度值的差异来平衡重建深度图像之间的关系，同时惩罚深度图的图像域中高频细节的失真： <img src="https://img-blog.csdnimg.cn/20200314165259760.png" srcset="/img/loading.gif" alt="损失函数L"><br>3、运用数据增强策略，文章只用了镜像翻转，以及改变颜色通道排列，后者还可以做一个further work； 4、提出了一个新的test dataset（用不上，所以没看）。</p><h2 id="五结果以及改进空间">五、结果以及改进空间</h2><p>1、在室外场景中没别人的方法表现好，猜想是因为提供的深度图的性质； 2、文章所提的改进空间挺多的，比如用在嵌入式设备上，这个网络存在局限性，以及更清楚地确定不同编码器、数据增强和学习策略对性能和贡献的影响，都是未来工作中值得关注的内容。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度估计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper conclusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
