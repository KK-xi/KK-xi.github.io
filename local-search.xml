<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>论文《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》</title>
    <link href="/2020/04/07/%E6%96%87%E3%80%8AGround-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss%E3%80%8B/"/>
    <url>/2020/04/07/%E6%96%87%E3%80%8AGround-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss%E3%80%8B/</url>
    
    <content type="html"><![CDATA[<h3 id="《Ground-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss》（2019-ICCV）"><a href="#《Ground-to-Aeria-lImage-Geo-Localization-With-a-Hard-Exemplar-Reweighting-Triplet-Loss》（2019-ICCV）" class="headerlink" title="《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》（2019-ICCV）"></a>《Ground-to-Aeria lImage Geo-Localization With a Hard Exemplar Reweighting Triplet Loss》（2019-ICCV）</h3><p>Author：Sudong Cai， Yulan Guo，Salman Khan Jiwei Hu， Gongjian Wen</p><pre><code>中心思想：基于Siamese network提出了FCANet，在basic ResNet中加入空间注意力和通道注意力机制(FCAM)，且给出一种新的在线困难样本重新分配权值三元组损失，并在其他文献的成就基础上，加入方向回归损失作为辅助损失。</code></pre><p><font color="red"><strong>补充</strong></font><strong>:</strong>　&lt;/br&gt;<br><strong>1、什么是Hard  example (困难样本)?</strong> &lt;/br&gt;<br>　　困难负样本是指哪些容易被网络预测为正样本的proposal，即假阳性(false positive,FP)，训练hard negative对提升网络的分类性能具有极大帮助。Hard example mining的核心思想就是用分类器对样本进行分类，把其中错误分类的样本(hard negative)放入负样本集合再继续训练分类器。</p><p> <strong>2、Triplet loss原理</strong> &lt;/br&gt;<br>　　Triplet Loss是深度学习中的一种损失函数，用于训练差异性较小的样本，通过优化锚示例(a)与正示例(p)的距离小于锚示例(a)与负示例(n)的距离，实现样本的相似性计算。</p><ul><li><p>输入一个三元组 <a, p, n><br> a： anchor<br> p： positive, 与 a 是同一类别的样本<br> n： negative, 与 a 是不同类别的样本</a,></p></li><li><p>公式</p><script type="math/tex; mode=display">L= max(d(a,p)-d(a,n)+margin,0)</script><p> 优化目标就是拉近 a, p 的距离， 拉远 a, n 的距离。其中hard triplets是:  $d(a,n)&lt;d(a,p)$,  即a, p的距离远；easy triplets:  $L=0$ 即 $d(a,p)+margin&lt;d(a,n)$，这种情况不需要优化，a, p的距离本来就很近， a, n的距离远。</p></li></ul><h4 id="一、Background＆Highlight"><a href="#一、Background＆Highlight" class="headerlink" title="一、Background＆Highlight"></a>一、Background＆Highlight</h4><ul><li><p><strong>Background</strong> &lt;/br&gt;<br> 1、ground-to-ground image geo-localization没有可用的参考图像数据集； &lt;/br&gt;<br> 2、cross-view图像匹配的弊端：视点的剧烈变化、方向信息的不定性、光照变化等； &lt;/br&gt;<br> 3、 handcrafted features用于计算相似度，导致结果准确率低； &lt;/br&gt;<br> 4、已有的hard example mining仍然很难找出困难样本。</p></li><li><p><strong>Highlight</strong> &lt;/br&gt;<br>1、提出了一个新的triplet loss(<font color="red">online exemplar reweighting triplet loss </font>)提高训练，可以实现online hard example mining； &lt;/br&gt;<br>2、提出了lightweight attention module (FCAM)。</p></li></ul><h4 id="二、Framework"><a href="#二、Framework" class="headerlink" title="二、Framework"></a>二、Framework</h4><p> <strong>1、 Overview</strong> &lt;/br&gt;<br>      文章提出的构架概览如图，网络是基于Siamese network设计的:<br><img src="https://img-blog.csdnimg.cn/20200318172040954.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述">  　　　　 </p><p>主要组成部分：① baseline是两个CNN，用于提取图像的特征，网络分支之间没有共享权重(因为在其他文献中已经说明共享权重的效果并没它好，有一个说法就是这两个分支本来就是各自训练的)。② Dual Attention，就是文中提出的FCAM模型，是在两个CNN网络中都加入了包括通道注意力和空间维度注意力的机制，用于加大特征的区别。③ Main Loss，即文章所提的困难样本重加权triplet loss，把大的权重分配给有用的hard triplets，而把小的权重分配给信息较少的easy triplets。④ Auxiliary Loss，即Orientation regression (OR) 学习分支的损失。</p><p><strong>2、Feature Context-Based Attention Module</strong>     </p><ul><li><strong>Channel attention submodule</strong> &lt;/br&gt;<br>　　通道注意力机制是用来强调信息相对丰富的通道。网络框架如下图所示，采用了卷积块注意模块(<a href="https://arxiv.org/pdf/1807.06521.pdf" target="_blank" rel="noopener">CBAM</a>)，输入特征图U(W×H×C)，应用max-pooling $f<em>{max}$和 average-pooling $f</em>{avg}$ 作用于U产生一维的全局通道描述符$v^1$和$v^2$。然后，两个描述符经由 Multi-Layer Perceptron (MLP)  $f_{ext}(^.,^.)$ 激活，分析通道之间的依赖关系。最后，用Sigmoid函数激活两个描述符的总和得到$Z^C(U)$。输出channel attention map $U’$是通过$Z^C(U)$和U之间的元素级乘法生成的。<br>  <img src="https://img-blog.csdnimg.cn/20200318172206762.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li></ul><ul><li><strong>Spatial attention submodule</strong> &lt;/br&gt;<br> 空间注意力用于突出有意义的特征单元，网络构架如图所示。受 <a href="https://www.researchgate.net/publication/319535111_Learned_Contextual_Feature_Reweighting_for_Image_Geo-Localization" target="_blank" rel="noopener">Contextual Reweighting Network (CRN)</a> 启发，文章将特征上下文感知学习(特征reweighting strategy)集成到CBAM的基本控件注意子模块中。来自通道注意力的特征图$U’$作为输入，然后连接两个特征掩码$f<em>{max}^c$和$f</em>{avg}^c$(沿着通道轴线的最大池化和平均池化)生成S(W×H×2)。然后，为了利用特征元的上下文信息，使用了不同感受野的卷积（3×3，5×5和7×7）来生成中间特征掩码，再将masks连接起来生成特征掩码P(W×H×3)，再用1×1的卷积学习和积累权重生成$Z^S(U’)$。最后，spatial attention map $U’’$是通过$Z^S(U)$和$U’$之间的元素级乘法生成的。<br> <img src="https://img-blog.csdnimg.cn/20200318172347324.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li></ul><ul><li><strong>Building block</strong> &lt;/br&gt;<br>   集合FCAM到基本ResNet的每一个building block中就形成了CNN特征提取网络，如下图：<br><img src="https://img-blog.csdnimg.cn/2020031817270364.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li></ul><ul><li><strong>Hard Exemplar Reweighting Triplet Loss</strong> &lt;/br&gt;<br> 将基于triplet reweighting的在线困难样本挖掘策略集合到soft-margin triplet loss中，Loss定义如下：<script type="math/tex; mode=display">L_{hard}(A_i,P_i,N_{i,k})= w_{hard}(A_i,P_i,N_{i,k})*log(1+exp(d_p(i)-d_n(i,k)))</script> 其中$A<em>i$，$P_i$和$N</em>{i,k}$分别表示anchor，positive exemplar和 k-th negative exemplar。运用距离修正逻辑回归估计当前三元组的难度，根据$gap(i,k)=d<em>n(i,k)-d_p(i)$的大小计算$w</em>{hard}(A<em>i,P_i,N</em>{i,k})$，最终定义式如下：<br><img src="https://img-blog.csdnimg.cn/20200318160521251.png" srcset="/img/loading.gif" alt="困难样本权重"></li><li><p><strong>Orientation regression</strong>  &lt;/br&gt;                         在现有的跨视图地理定位基准数据集中，锚点的方向及其对应的正样本在训练集中固定，而在测试集中打乱。所以文章随机旋转航空图像得到的不同角度可以作为训练的标签。为了解决方向未知的问题，在框架中增加一个方向回归，本文将重新分配权值的方向回归损失定义为：</p><script type="math/tex; mode=display">L_{OR}(A_i,P_i,N_{i,k})= w_{hard}(A_i,P_i,N_{i,k})*(d^1_R(i)+d^2_R(i))</script><p>   其中$d^1_R(i)$和$d^2_R(i)$ 分别表示sin和cosine值的回归误差。</p><p><font color="red"><strong>总的误差定义式如下：</strong></font></p><script type="math/tex; mode=display">L_{HER}(A_i,P_i,N_{i,k})= \lambda_1*L_{hard}(A_i,P_i,N_{i,k})+\lambda_2*L_{OR}(A_i,P_i,N_{i,k})</script></li></ul><h4 id="三、Dataset"><a href="#三、Dataset" class="headerlink" title="三、Dataset"></a>三、Dataset</h4><ul><li><strong>CVUSA dataset</strong>  &lt;/br&gt;<br>包含35532个用于训练的地空图像对和8884个用于测试的图像对。地面图像为全景图像，并且地空图均为高分辨率。</li><li><strong>Vo and Hays’ (VH) dataset</strong>  &lt;/br&gt;<br>包含超过100万张由美国11个不同城市的谷歌地图收集的交叉视图图像对。使用8个子集作为训练集，并使用从Denver, Detroit, 和 Seattle捕获的其余3个子集进行评估，所有街道视图查询图像裁剪为固定大小230×230。另外，训练集中的方位角是固定的，测试集中是未知的。<br><img src="https://img-blog.csdnimg.cn/20200318165049147.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="datasets">    </li></ul><h4 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h4><ul><li><p><strong>Results</strong> <br><br> 1、匹配结果中top1%的recall达到了98.3%，但是top10左右的只有70%~80%；</p><p> 2、性能排序：this approach&gt;add soft-margin ranking loss&gt; add additional OR&gt;pasitive pairs Euclidean loss;</p><p> 3、CVUSA dataset&gt;VH dataset，因为前者是全景图像且分辨率高，包含的信息多；</p><p> 4、直接学习方向的不变性作为辅助，比把所有图像特征聚集起来要好。</p></li></ul><ul><li><p><strong>Problems</strong> &lt;/br&gt;<br>1、数据集的问题，放在正视图像上效果不太好；</p><p>2、网络有点复杂；</p><p>3、总的损失中权重$\lambda$如何取值，对结果的影响。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>即使定位论文</category>
      
      <category>添加注意力机制</category>
      
    </categories>
    
    
    <tags>
      
      <tag>即使定位</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch学习记录</title>
    <link href="/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/"/>
    <url>/2020/04/03/ytorch%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/</url>
    
    <content type="html"><![CDATA[<h2 id="课程名：《Pytorch-动态神经网络》"><a href="#课程名：《Pytorch-动态神经网络》" class="headerlink" title="课程名：《Pytorch 动态神经网络》"></a>课程名：《Pytorch 动态神经网络》</h2><p>课程来源：<a href="https://www.bilibili.com/video/av15997678?p=35" target="_blank" rel="noopener">here</a><br>作者：莫烦</p><h2 id="day01-安装Pytorch"><a href="#day01-安装Pytorch" class="headerlink" title="day01 安装Pytorch"></a>day01 安装Pytorch</h2><p>前提：安装Anaconda参考别人的安装教程<a href="https://www.jianshu.com/p/742dc4d8f4c5" target="_blank" rel="noopener">https://www.jianshu.com/p/742dc4d8f4c5</a></p><ul><li>在开始菜单找到Anaconda的命令提示行(Anaconda Prompt)，并输入conda create -n pytorch python=3.7(我自己的是3.7版本)，建立一个Pytorch的环境：<br><img src="https://img-blog.csdnimg.cn/20200318180532330.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>然后，出现以下情况，问是否安装等等工具包，选择[y]开始安装：<br><img src="https://img-blog.csdnimg.cn/20200318180734340.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>安装成功以后，会出现如下，就是激活环境的语句：<br><img src="https://img-blog.csdnimg.cn/20200318180919400.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_10,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>输入conda activate pytorch进入pytorch环境：<br><img src="https://img-blog.csdnimg.cn/20200318181124374.png" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>输入pip list可以查看这个环境下的工具包，可以看到没有需要的pytorch，所以需要安装：<br><img src="https://img-blog.csdnimg.cn/2020031818131941.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_12,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li><p>去官网查看自己适合哪个版本，比如我的是CPU，习惯用pip，如下图：<img src="https://img-blog.csdnimg.cn/20200322091319185.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="在这里插入图片描述"></p><pre><code class="lang-python">pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html</code></pre></li><li><p>完了以后，在pytorch环境中进入&gt;&gt;python，测试一下是否安装成功，输入import torch即可。</p></li></ul><h2 id="day02"><a href="#day02" class="headerlink" title="day02"></a>day02</h2><h3 id="一、神经网络简介"><a href="#一、神经网络简介" class="headerlink" title="一、神经网络简介"></a>一、神经网络简介</h3><p> 1、 机器学习—梯度下降机制(optimization) &lt;/br&gt;<br> 2、神经网络黑盒：输入端-黑盒-输出端； &lt;/br&gt;<br>　　　　　　黑盒：特征代表输入数据。</p><h3 id="二、why-Pytorch？"><a href="#二、why-Pytorch？" class="headerlink" title="二、why Pytorch？"></a>二、why Pytorch？</h3><p>   1、与tensorflow的区别</p><ul><li>tensorflow是静态的框架，构建好tensorflow的计算图之后，这个计算图是不能改变的，计算流程是固定的，类似C++，写代码时要用他自己的一些API。缺点之一例如训练的时候loss一直将不下来，模型很难得到优化，debug就很困难。</li></ul><ul><li>pytorch是动态的框架，和python一样，直接计算，不用开启会话。 &lt;/br&gt;</li></ul><h3 id="三、Variable变量"><a href="#三、Variable变量" class="headerlink" title="三、Variable变量"></a>三、Variable变量</h3><ul><li>在 Torch 中的 Variable 就是一个存放会变化的值的地理位置，里面的值会不停的变化，就像一个裝鸡蛋的篮子，鸡蛋数会不停变动。那里面的鸡蛋就是 Torch 的 Tensor 。 </li><li>PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构。</li><li><p>from torch.autograd import Variable</p><pre><code class="lang-python">  ten=torch.FloatTensor([[1,2],[3,4]]) # tensor的类型  variable=Variable(tensor,requires_grad=True)   # 将tensor传给variable，需要Variable来建立一个计算图纸，把鸡蛋放到篮子里, requires_grad是参不参与误差反向传播, 要不要计算梯度，如果要就会计算Variable节点的梯度  t_out = torch.mean(ten*ten) # 计算x^2  v_out = torch.mean(variable*variable)  v_out.backward() # v_outbackward时，variable也会变化，因为是一体的  print(variable)  #直接print(variable)只会输出 Variable 形式的数据, 在很多时候是用不了的(比如想要用 plt 画图),所以我们要转换一下, 将它变成 tensor 形式  print(variable.data)  print(variable.data.numpy())# variable.data为tensor的形式，tensor才能转换为numpy形式</code></pre></li><li><p>autograd根据用户对Variable的操作构建其计算图，这个图将所有的计算步骤 (节点) 都连接起来，最后进行误差反向传递的时候， 一次性将所有 variable 里面的修改幅度 (梯度) 都计算出来, 而 tensor 就没有这个能力。</p></li><li>variable默认是不需要求导的，即requires_grad属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点requires_grad都为True。</li><li>多次反向传播时，<font color="red">梯度是累加的</font>。反向传播的中间缓存会被清空，为进行多次反向传播需指定retain_graph=True来保存这些缓存。</li><li>variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播。</li></ul><h2 id="day-03"><a href="#day-03" class="headerlink" title="day 03"></a>day 03</h2><h3 id="一、激励函数（Activation）"><a href="#一、激励函数（Activation）" class="headerlink" title="一、激励函数（Activation）"></a>一、激励函数（Activation）</h3><ul><li><p>什么是Activation<br>非线性的函数激活网络的输出：ReLU、Sigmoid、Tanh、Softplus</p></li><li><p>Torch中的激励函数</p><pre><code class="lang-python"> import torch.nn.functional as F from torch.autograd import Variable import matplotlib.pyplot as plt x = torch.linspace(-5, 5, 200)  # x data (tensor), shape=(100, 1) x = Variable(x) x_np = x.data.numpy()   # numpy 数据才能用来画图y_relu = torch.relu(x).data.numpy()y_sigmoid = torch.sigmoid(x).data.numpy()y_tanh = torch.tanh(x).data.numpy() # 计算出非线性函数输出后也要转化为numpy数据# y_softplus = F.softplus(x).data.numpy()   #画图plt.figure(1, figsize=(8, 6))plt.subplot(221)plt.plot(x_np, y_relu, c=&#39;red&#39;, label=&#39;relu&#39;)plt.ylim((-1, 5))plt.legend(loc=&#39;best&#39;)</code></pre></li><li><p>结果<br><img src="https://img-blog.csdnimg.cn/2020032211361126.png?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></p></li></ul><h3 id="二、Regression回归"><a href="#二、Regression回归" class="headerlink" title="二、Regression回归"></a>二、Regression回归</h3><p>直接放莫老师教的代码过来：</p><ul><li><p>Layer图搭建以及计算流程</p><pre><code class="lang-python">  class Net(torch.nn.Module):   # torch.nn.Module是Net的主模块      def __init__(self, n_feature, n_hidden, n_output): # 搭建层所需要的信息          super(Net, self).__init__()  # 继承Net的模块功能          self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer          self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer      def forward(self, x): # 前向传递的过程，搭流程图             x = F.relu(self.hidden(x))      # activation function for hidden layer             x = self.predict(x)             # linear output       return x</code></pre></li></ul><ul><li><p>定义Net</p><pre><code class="lang-python"> net = Net(n_feature=1, n_hidden=10, n_output=1)     # define</code></pre></li></ul><ul><li><p>优化神经网络(torch.optim.),以及loss function定义</p><pre><code class="lang-python"> optimizer = torch.optim.SGD(net.parameters(), lr=0.2) loss_func = torch.nn.MSELoss() # 均方差作为loss</code></pre></li><li><p>开始训练 </p><pre><code class="lang-python"> for t in range(200):     prediction = net(x)     # input x and predict based on x     loss = loss_func(prediction, y)     # must be (1. nn output, 2. target)     optimizer.zero_grad()   # clear gradients for next train     loss.backward()         # backpropagation, compute gradients     optimizer.step()        # apply gradients     # 以上三步为优化步骤</code></pre></li></ul><h3 id="三、-Classification-分类"><a href="#三、-Classification-分类" class="headerlink" title="三、    Classification 分类"></a>三、    Classification 分类</h3><p>　与上面不同的是：</p><ul><li>构造的伪数据不一样，是包含有对应标签的数据；(数据不能是一维)</li><li>网络输入输出不同，有两个输入两个输出；</li><li><p>loss用到的是交叉熵cross entropy loss，out与标签y</p><pre><code class="lang-python">loss_func = torch.nn.CrossEntropyLoss() loss = loss_func(out, y)</code></pre></li><li><p>output是取值，转换成概率值需要加softmax(out)</p><pre><code class="lang-python"> out = net(x)     # input x and predict based on x prediction = F.softmax(out)  #将输出对应值转化成概率</code></pre></li></ul><h3 id="四、快速搭建网络"><a href="#四、快速搭建网络" class="headerlink" title="四、快速搭建网络"></a>四、快速搭建网络</h3><ul><li><p>method1—搭建网络、流程图，定义网络</p><pre><code class="lang-python"> class Net(torch.nn.Module):     def __init__(self, n_feature, n_hidden, n_output):         super(Net, self).__init__()         self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer         self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer     def forward(self, x):         x = F.relu(self.hidden(x))      # activation function for hidden layer         x = self.predict(x)             # linear output         return x net1 = Net(n_feature=2, n_hidden=10, n_output=2)</code></pre></li><li><p>method2—利用torch.nn.Sequential直接定义网络</p><pre><code class="lang-python"> net2=torch.nn.Sequential(     torch.nn.Linear(2,10),     torch.nn.ReLU(),     torch.nn.Linear(10,2) )</code></pre></li></ul><h3 id="五、网络的保存和提取"><a href="#五、网络的保存和提取" class="headerlink" title="五、网络的保存和提取"></a>五、网络的保存和提取</h3><ul><li><p>方法1：保存—提取</p><pre><code class="lang-python"> torch.save(net1, &#39;net.pkl&#39;) # 保存整个网络，以pkl形式保存</code></pre><pre><code class="lang-python"> net2 = torch.load(&#39;net.pkl&#39;) prediction = net2(x)</code></pre></li><li><p>方法2：保存—提取</p><pre><code class="lang-python"> torch.save(net1.state_dict(), &#39;net_params.pkl&#39;) # 只保存网络中节点的参数 (速度快, 占内存少)</code></pre><pre><code class="lang-python"> net3 = torch.nn.Sequential(     torch.nn.Linear(1, 10),     torch.nn.ReLU(),     torch.nn.Linear(10, 1) ) net3.load_state_dict(torch.load(&#39;net_params.pkl&#39;)) prediction = net3(x)</code></pre></li></ul><h3 id="六、批数据训练-mini-batch-training"><a href="#六、批数据训练-mini-batch-training" class="headerlink" title="六、批数据训练(mini_batch training)"></a>六、批数据训练(mini_batch training)</h3><ul><li><p>将数据分批训练，一个epoch训练所有批次的数据：</p><pre><code class="lang-python">  import torch.utils.data as Data  BATCH_SIZE = 5 # 抽取训练的数据  # BATCH_SIZE = 8  x = torch.linspace(1, 10, 10)       # this is x data (torch tensor)  y = torch.linspace(10, 1, 10)       # this is y data (torch tensor)  torch_dataset = Data.TensorDataset(data_tensor = x, target_tensor = y)  loader = Data.DataLoader(      dataset=torch_dataset,      # torch TensorDataset format      batch_size=BATCH_SIZE,      # mini batch size      shuffle=True,               # random shuffle for training      num_workers=2,              # 多线程来读数据  )  for epoch in range(3):   # 训练所有!整套!数据 3 次      for step, (batch_x, batch_y) in enumerate(loader):  # 每一步 loader 释放一小批数据用来学习          # 假设这里就是你训练的地方...          # 打出来一些数据          print(&#39;Epoch: &#39;, epoch, &#39;| Step: &#39;, step, &#39;| batch x: &#39;,                batch_x.numpy(), &#39;| batch y: &#39;, batch_y.numpy())</code></pre></li><li><p>DataLoader<br>是PyTorch中数据读取的接口，PyTorch训练模型基本都会用到该接口，其目的：将dataset根据batch_size大小、shuffle等封装成一个Batch Size大小的Tensor，用于后面的训练。</p></li><li><p>enumerate()函数<br>用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。在这里就是把是个数据分成size为5的两份数据后，将每一份数据对应的下标给step，数据给(batch_x, batch_y)。</p></li></ul><h2 id="day-04"><a href="#day-04" class="headerlink" title="day 04"></a>day 04</h2><h3 id="一、优化器Optimizer加速神经网络训练（深度学习）"><a href="#一、优化器Optimizer加速神经网络训练（深度学习）" class="headerlink" title="一、优化器Optimizer加速神经网络训练（深度学习）"></a>一、优化器Optimizer加速神经网络训练（深度学习）</h3><ul><li>数据分批送入网络，进行SGD优化；</li><li>Momentum更新参数方法：$m=b1<em>m-Learningrate</em>dx,W+=m$</li><li>AdaGrad：$v+=dx^2，W+=-Learning rate*dx/\sqrt v$</li><li>RMSProp方法(上述两种的合并)：$v=b1<em>v+(1-b1)</em>dx^2,W+=-Learning_rate*dx/\sqrt v$</li><li>Adam:$m = b1<em>m+(1-b1)</em>dx$——&gt;Momentum<br>　　　 $v = b2<em>v+(1-b2)</em>dx^2$——&gt;AdaGrad<br>　　　 $W+=-Learning_rate*m/\sqrt v$</li></ul><h3 id="二、Opttimizer优化器"><a href="#二、Opttimizer优化器" class="headerlink" title="二、Opttimizer优化器 　"></a>二、Opttimizer优化器 　</h3><ul><li><p>几种常见优化器：　</p><pre><code class="lang-python"> # different optimizers opt_SGD = torch.optim.SGD(net_SGD.parameters(), lr=LR) opt_Momentum = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8) opt_RMSprop = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9) opt_Adam = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99)) optimizers = [opt_SGD, opt_Momentum, opt_RMSprop, opt_Adam]</code></pre></li></ul><p><img src="https://img-blog.csdnimg.cn/20200323135943525.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"><br>　　从图中可以看出，目前性能最优的应该是Adam。</p><h3 id="三、-卷积神经网络-CNN"><a href="#三、-卷积神经网络-CNN" class="headerlink" title="三、    卷积神经网络(CNN)"></a>三、    卷积神经网络(CNN)</h3><p>　　图像处理中，不是对每个像素点卷积处理，而是对一小块区域进行计算，这样加强了图像信息的连续性，使得神经网络能看到图片信息而非一个点，同时加深了神经网络对图片的理解。批量过滤器每次对图像收集一小块信息，最后将这些整理出来得到边缘信息，再对这些信息进行类似的处理，得到更高层的信息结构(例如眼睛、鼻子等)，最后把总结出来的信息套入几层full connection进行分类等操作。卷积操作时，神经层会丢失一些信息，池化层可以将Layer中有用的信息筛选出来给下一层，因此图片的长宽不断压缩，压缩的工作是池化层进行的。</p><pre><code class="lang-python">1、import需要的工具包和库：torch、torchvision、torch.nn、torch.utils.data2、超参数：EPOCH、BATCH_SIZE、LR3、下载mnist数据集：torchvision.datasets.MNIST(root=&#39;./mnist/&#39;,train=True,transform=torchvision.transform.ToTensor(),download=True) #root是保存或提取的位置，transform是将数据集PIL.Image or numpy.ndarray转换成torch.FloatTensor(C×H×W)，训练的时候normalize成[0,1]间的值 test数据集处理：test—_x,test_y 4、批训练train_loader定义：Data.DataLoader(dataset=train_data,batch_size=BATCH_SIZE,shuffle=True) 5、定义网络构架CNN(nn.Module):conv1—conv2—RELU—pooling—conv2—ReLU—pooling—output 网络计算流程：conv1(x)——conv2(x)——展平多维卷积图——计算输出 6、定义optimizer和loss function 7、训练和测试</code></pre><h3 id="四、什么是LSTM循环卷积网络-RNN"><a href="#四、什么是LSTM循环卷积网络-RNN" class="headerlink" title="四、什么是LSTM循环卷积网络(RNN)"></a>四、什么是LSTM循环卷积网络(RNN)</h3><ul><li>LSTM(Long Short-Term Memory)——长短期记忆</li><li>RNN是在有序的数据上进行学习<br><img src="https://img-blog.csdnimg.cn/20200323202037102.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="在这里插入图片描述"></li><li>分类问题(mnist数据集)<br>　我们将图片数据看成一个时间上的连续数据, 每一行的像素点都是这个时刻的输入, 读完整张图片就是从上而下的读完了每行的像素点。然后我们就可以拿出 RNN 在最后一步的分析值判断图片是哪一类了。</li><li>回归问题<br>这部分内容参考<a href="https://blog.csdn.net/qq_41639077/article/details/105218095" target="_blank" rel="noopener">KiKi的另一篇blog</a>，包括分类问题和回归问题的pytorch实现。</li></ul><h3 id="五、自编码-非监督学习（Autoencoder）"><a href="#五、自编码-非监督学习（Autoencoder）" class="headerlink" title="五、自编码/非监督学习（Autoencoder）　"></a>五、自编码/非监督学习（Autoencoder）　</h3><p>　　原来有时神经网络要接受大量的输入信息, 比如输入信息是高清图片时, 输入信息量可能达到上千万, 让神经网络直接从上千万个信息源中学习是一件很吃力的工作。 所以, 何不<strong>压缩一下, 提取出原图片中的最具代表性的信息, 缩减输入信息量, 再把缩减过后的信息放进神经网络学习，这样学习起来就简单轻松了。</strong> 训练好的自编码中间这一部分就是能总结原数据的精髓，我们只用到了输入数据 X, 并没有用到 X 对应的数据标签, 所以也可以说自编码是一种非监督学习。到了真正使用自编码的时候，通常只会用到自编码前半部分。(摘自莫烦python)</p><ul><li><p>代码</p><pre><code class="lang-python">  self.encoder = nn.Sequential(      nn.Linear(28*28, 128),      nn.Tanh(),      nn.Linear(128, 64),      nn.Tanh(),      nn.Linear(64, 12),      nn.Tanh(),      nn.Linear(12, 3),   # compress to 3 features which can be visualized in plt  )  self.decoder = nn.Sequential(      nn.Linear(3, 12),      nn.Tanh(),      nn.Linear(12, 64),      nn.Tanh(),      nn.Linear(64, 128),      nn.Tanh(),      nn.Linear(128, 28*28),      nn.Sigmoid(),       # compress to a range (0, 1)  )  def forward(self, x):      encoded = self.encoder(x)      decoded = self.decoder(encoded)      return encoded, decoded  autoencoder = AutoEncoder()</code></pre></li></ul><h3 id="六、GAN—生成对抗网络"><a href="#六、GAN—生成对抗网络" class="headerlink" title="六、GAN—生成对抗网络"></a>六、GAN—生成对抗网络</h3><p>　（原理已经学习过了，直接上代码）</p><ul><li>pytorch中实现：(代码中的对象不是图像，用到的是二次曲线) 　&lt;/br&gt;</li></ul><ul><li><p>超参数</p><pre><code>  ```python  BATCH_SIZE = 64  LR_G = 0.0001 # 生成器的学习率  LR_D = 0.0001 # 判别器的学习率  N_IDEAS = 5 # random_noise的个数  ART_COMPONENTS = 15  # 定义规格，一条曲线上有多少个点  PAINT_POINTS = np.vstack([np.linspace(-1,1,ART_COMPONENTS)for _ in range(BATCH_SIZE)]) # 规定整批画的点，从-1到1共15个点  ```    </code></pre></li><li><p>没有train data，自己伪造一些real data</p><pre><code>  ```python  def artist_works():     # painting from the famous artist (real target)      a = np.random.uniform(1, 2, size=BATCH_SIZE)[:, np.newaxis] # 二次曲线的系数      paintings = a * np.power(PAINT_POINTS, 2) + (a-1)  # 二次曲线的参数，区间表示upper和      paintings = torch.from_numpy(paintings).float()      return paintings  ```        </code></pre></li><li><p>定义生成器和判别器</p><pre><code>  ```python  G = nn.Sequential(                      # Generator      nn.Linear(N_IDEAS, 128),            # random ideas (could from normal distribution)      nn.ReLU(),      nn.Linear(128, ART_COMPONENTS),     # making a painting from these random ideas  )  D = nn.Sequential(                      # Discriminator      nn.Linear(ART_COMPONENTS, 128),     # receive art work either from the famous artist or a newbie like G      nn.ReLU(),      nn.Linear(128, 1),      nn.Sigmoid(),                       # tell the probability that the art work is made by artist  )  ```</code></pre></li><li><p>优化器</p><pre><code>  ```python      opt_D = torch.optim.Adam(D.parameters(), lr=LR_D)  opt_G = torch.optim.Adam(G.parameters(), lr=LR_G)  ```</code></pre></li><li><p>训练啦</p><pre><code>  ```python  for step in range(10000):      artist_paintings = artist_works()           # real painting from artist      G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas      G_paintings = G(G_ideas)                    # fake painting from G (random ideas)      prob_artist0 = D(artist_paintings)          # D try to increase this prob      prob_artist1 = D(G_paintings)               # D try to reduce this prob      D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))      G_loss = torch.mean(torch.log(1. - prob_artist1))      opt_D.zero_grad()      D_loss.backward(retain_graph=True)      # reusing computational graph      opt_D.step()      opt_G.zero_grad()      G_loss.backward()      opt_G.step()  ```</code></pre></li></ul><p><strong>补充：</strong> cGAN与GAN的区别在于多了一个类别标签，这个label会跟随noise一起输入到生成器中，并且也要跟随fake和real一起输入到判别其中，最终计算各自的loss。</p><h3 id="七、为什么Torch是动态的-待补充"><a href="#七、为什么Torch是动态的-待补充" class="headerlink" title="七、为什么Torch是动态的(待补充)"></a>七、为什么Torch是动态的<font color="red">(待补充)</font></h3><p>   　例子：RNN网络<br>   　Tensorflow就是预先定义好要做的task的框架、步骤，然后开启会话之后喂数据一步到位的计算出结果，开启会话后便不能修改网络构架了，只能是照着计算流图跟着计算，所以是静态的；Torch也可以先定义好框架然后套进去，但计算的时候无论网络怎么变化每一个叶子节点的梯度都能给出，tensorflow就做不到这一点，并且torch是边给出计算图纸一边进行训练。torch就像是散装的一样，可以一块一块的制作好并进行计算，比较灵活，所以是动态的。</p><h3 id="八、GPU加速"><a href="#八、GPU加速" class="headerlink" title="八、GPU加速"></a>八、GPU加速</h3><p>　以之前CNN为例，对其代码进行修改</p><ul><li><p>dataset部分</p><pre><code class="lang-python"> test_x = torch.unsqueeze(test_data.test_data, dim=1).type(torch.FloatTensor).cuda()/255.   # Tensor on GPU test_y = test_data.test_labels.cuda()</code></pre></li><li><p>CNN网络的参数改为GPU兼容形式</p><pre><code class="lang-python"> class CNN(nn.Module):     ... cnn = CNN() ##########转换cnn到CUDA######### cnn.cuda()   # Moves all model parameters and buffers to the GPU.</code></pre></li><li><p>training data变成GPU形式</p><pre><code class="lang-python"> for epoch ..:     for step, ...:         ##########修改1###########         b_x = x.cuda()    # Tensor on GPU         b_y = y.cuda()    # Tensor on GPU         ...         if step % 50 == 0:             test_output = cnn(test_x)             # !!!!!!!! 修改2  !!!!!!!!! #             pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()  # 将操作放去 GPU             accuracy = torch.sum(pred_y == test_y) / test_y.size(0)             ... test_output = cnn(test_x[:10]) # !!!!!!!! 修改3 !!!!!!!!! # pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze()  # 将操作放去 GPU ... print(test_y[:10], &#39;real number&#39;)</code></pre></li></ul><h2 id="day-05"><a href="#day-05" class="headerlink" title="day 05"></a>day 05</h2><h3 id="一、过拟合-Overfitting"><a href="#一、过拟合-Overfitting" class="headerlink" title="一、过拟合(Overfitting)"></a>一、过拟合(Overfitting)</h3><ul><li>过拟合（overfitting）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。模型在训练集上效果好，然而在测试集上效果差，模型泛化能力差。</li></ul><ul><li><p>原因 &lt;/br&gt;<br>　1）在对模型进行训练时，有可能遇到训练数据不够，即训练数据无法对整个数据的分布进行估计的时候； &lt;/br&gt;<br>　2）权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。　</p></li><li><p>解决方法</p><p>方法一： <strong>增加数据量</strong>。 &lt;/br&gt;<br>方法二：<strong>运用正规化</strong>，L1、 L2 regularization等等。（神经网络的正规化方法<strong>dropout</strong>——就是在训练的时候, 随机忽略掉一些神经元和神经联结 , 使这个神经网络变得”不完整”，用这个不完整的神经网络训练一次。第二次再随机忽略另一些, 变成另一个不完整的神经网络。有了这些随机 drop 掉的规则, 我们可以想象每次训练的时候, 让每一次预测结果不会依赖于其中某部分特定的神经元。像l1, l2正规化一样, 过度依赖的 W  , 也就是训练参数的数值会很大, l1, l2会惩罚这些大的 参数，Dropout 的做法是从根本上让神经网络没机会过度依赖。）</p></li></ul><ul><li><p>Dropout</p><pre><code class="lang-python"> # 不加dropout的网络 net_overfitting = torch.nn.Sequential(     torch.nn.Linear(1, N_HIDDEN),     torch.nn.ReLU(),     torch.nn.Linear(N_HIDDEN, N_HIDDEN),     torch.nn.ReLU(),     torch.nn.Linear(N_HIDDEN, 1),     )     # 加上dropout net_dropped = torch.nn.Sequential(     torch.nn.Linear(1, N_HIDDEN),     torch.nn.Dropout(0.5),  # drop 50% of the neuron     torch.nn.ReLU(),     torch.nn.Linear(N_HIDDEN, N_HIDDEN),     torch.nn.Dropout(0.5),  # drop 50% of the neuron     torch.nn.ReLU(),     torch.nn.Linear(N_HIDDEN, 1),     )＃除了网络构架不同外，其他大同小异。</code></pre><p>　　</p></li></ul><h3 id="二、批标准化-Batch-Normalization"><a href="#二、批标准化-Batch-Normalization" class="headerlink" title="二、批标准化(Batch Normalization)"></a>二、批标准化(Batch Normalization)</h3><ul><li><p>什么是批标准化 &lt;/br&gt;<br>　Batch Normalization(BN), 批标准化, 和普通的数据标准化类似, 是将分散的数据统一的一种做法, 也是优化神经网络的一种方法。 具有统一规格的数据, 能让机器学习更容易学习到数据之中的规律。数据随着神经网络的传递计算，激活函数的存在会造成网络层对数据的不敏感，比如0.1和2经过Tanh函数后，前者仍0.1，而2变成1，那这样再大的数都会变成1，所以神经层对数据失去了感觉，这样的问题同样存在于隐藏层中，所以BN则是用在这些神经层中优化网络的方法。 &lt;/br&gt;<br>　Batch就是数据分批处理，每一批数据前向传递的过程中，每一层都进行BN处理，添加在层和激励函数之间。反BN：$BN_(\gamma,\beta)(x_i)$是将 normalize 后的数据再扩展和平移，是为了让神经网络自己去学着使用和修改这个扩展参数 $\gamma$, 和 平移参数$\beta$, 这样神经网络就能自己慢慢琢磨出前面的 normalization 操作到底有没有起到优化的作用, 如果没有起到作用, 我就使用 $\gamma$和$\beta$来抵消一些 normalization 的操作。</p></li><li><p>代码<br>　莫烦<a href="https://github.com/MorvanZhou/PyTorch-Tutorial/blob/master/tutorial-contents/504_batch_normalization.py" target="_blank" rel="noopener">BN_code</a></p></li></ul><p>　　　　　　<font color="red">部分内容待学习</font></p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>pytorch学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RNN-循环神经网络入门</title>
    <link href="/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/"/>
    <url>/2020/04/03/N-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%85%A5%E9%97%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="循环神经网络入门（pytorch实现）"><a href="#循环神经网络入门（pytorch实现）" class="headerlink" title="循环神经网络入门（pytorch实现）"></a>循环神经网络入门（pytorch实现）</h2><p>参考书目：《深度学习算法原理与编程实战》 |   蒋子阳著<br>&lt;/bar&gt;<br>参考视频：<a href="https://www.bilibili.com/video/BV134411w76S" target="_blank" rel="noopener">微软人工智能公开课—循环神经网络RNN</a></p><p><strong>一、回顾前馈神经网络</strong> &lt;/br&gt;<br>　　RNN是在前馈式神经网络基础上的，所以先回顾一下前馈神经网络。 &lt;/br&gt;<br>　　前馈式神经网络（FNN, Feedforward Neural Network）一般的结构如下图：<br>　　<img src="https://img-blog.csdnimg.cn/20200403141749146.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="前馈式神经网络"><br>图中神经元的输出表示为：$O<em>t=f(\sum</em>{i=1}^n a<em>iw_i+b)$假设某一层的输出为$h_n$，则下一层的输出为$f</em>{out}(h<em>n)$，这样重复迭代网络的输出为$y=f</em>{out}(f<em>n(f</em>{n-1}(…)))$，如此一来神经网络就成了巨大的复合函数，但实际我们并不会用到这个函数来计算，而使用计算图来表示，这样方便得出每一个节点的输入输出数据的梯度。对于RNN，就从这样的简单的前馈神经网络的传递方式说起。</p><p><strong>二、RNN网络简介</strong> &lt;/br&gt;<br>　　循环神经网络（ Recurrent Neural Network, RNN ）雏形见于美国物理学家 J.J.Hopfield 于 1982 年提出的可用作联想存储器的互联网络——Hopfield 神经网络模型，如下图，每个节点都有输入，两两节点之间有双相连接：<br>　　<img src="https://img-blog.csdnimg.cn/20200403144156566.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Hopfiled Network"><br>　　基于传统的机器学习算法十分依赖人工特征的提取致使模型一直无法提高正确率，而之前的全连神经网络以及卷积神经网络中，信息只在层与层之间存在运算关系，而没有连接节点之间的信息流动，这样在每一个时间都会有一个单独的参数，<font color="red">因此不能在时间上共享不同序列长度或序列不同位置的统计强度，所以无法对训练时没有见过的序列长度进行泛化</font>。 所以发展RNN对具有时间序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，在模型的不同部分共享参数解决了这个问题，并使得模型能够扩展到对不同形式的样本（这里指不同长度的样本）进行泛化。 <strong>循环神经网络会在几个时间步内共享相同的权重以刻画一个序列当前的输出与之前信息的关系，这体现在结构上是循环神经网络的隐藏层之间存在连接，隐藏层的输入来自于输入层的数据以及上一时刻隐藏层的输出</strong>。这样的结构使得循环神经网络会对之前的信息有所记忆， 同时利用之前的信息影响后面节点的输出。 &lt;/br&gt;<br>　　RNN网络主要用于处理离散序列数据：离散线性、长度可变的序列，例如时域语音信号，金融市场走势等。网络可用于序列数据的分析（市场趋势预测）、序列数据的生产（基于图片的文字描述）、序列数据的转换（语音识别以及机器翻译）。</p><p><strong>三、循环网络结构</strong> &lt;/br&gt;<br>　 　循环神经网络典型结构及其按时间先后展开结构如图所示：　<br>　 　<img src="https://img-blog.csdnimg.cn/20200403160334980.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="RNN"><br>对于主体结构$A$，一般可认为是循环神经网络的一个隐藏单元。在$t$时刻，主体结构$A$会读取输入层的输入$x_t$以及上一时刻的输出，并输出当前时刻的$o_t$值（图中未给出）。此后$A$结构在$t$时刻的状态值表达式：$h^t=f(h^{t-1},x_t;\theta)$，其中$\theta$可以是网络中的其它参数比如权重或偏置等，循环的过程就是$A$不断被执行的过程。但是循环神经网络目前无法做到无限循环，因为循环过多会出现梯度消失的问题。 &lt;/br&gt;<br>　　假设隐藏单元的激活函数是tanh函数，则循环体结构$A$如下：　　<img src="https://img-blog.csdnimg.cn/20200403155812724.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="循环体"><br>则隐藏单元状态值表示为：$h^t=tanh(Wh^{t-1}+Ux_t+b^h)$，$b^h$是由$x_t$得到$h^t$的偏置，$W$是相邻时刻隐藏单元间的权重矩阵，$U$是从$x_t$计算得到这个隐藏单元时用到的权重矩阵。从当前的状态值$h^t$都得到输出还需要一个全连接神经网络来完成这个过程，表达式为：$o^t=b_o+Vh^t$，$V$是由$h^t$得到$o^t$的权重矩阵。如果输出是离散的，则可以用softmax处理$o^t$得到标准化的概率向量$y$：$y^t=softmax(o^t)$，向量的值对应离散变量可能值的概率。</p><p><strong>四、网络的训练</strong> &lt;/br&gt;<br>　　要对网络进行训练，需要有一个与$x$序列配对的$o$的所有时间步内的总loss。 &lt;/br&gt;<br>　　RNN的反向传播算法称为时间反向传播 (Back-Propagation Through Time, BPTT)。基本原理和 BP 算法是一样的，三步走：前向计算每个神经元的输出值；反向计算误差项值；计算每个权重的梯度；最后再用随机梯度下降算法更新权重。详细的计算公式参考<a href="https://zhuanlan.zhihu.com/p/85776566" target="_blank" rel="noopener">知乎</a>。在反向传播过程中，当输入序列过长的时候，在求取一个比较远的时刻的梯度时，需要回溯到前面的所有时刻的信息，由于连乘项的存在，导致前面时刻的信息会缺失，这就是RNN中的梯度消失问题，还有就是当连乘出现大于1时的梯度爆炸，后者采用clip的方式即可，前者将会用到LSTM来缓解。</p><p><strong>五、LSTM</strong> &lt;/br&gt;<br>　　LSTM 结构由 Sepp Hochreiter教授和 Jurgen Schrnidhuber教授于 1997 年 提出，它本身就是一种特殊的循环体结构 。在一个整体的循环神经网络中，除了外部的RNN大循环（<strong>循环体是 LSTM</strong>）外，还需要考虑 LSTM 本身单元“细胞”之间的自循环。LSTM单元结构如下图：<br>　　<img src="https://img-blog.csdnimg.cn/20200403173928627.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM单元"><br>LSTM由自身的三个门结构进行控制，门结构使用Sigmoid函数对输入的信息进行控制，让信息有选择性的影响RNN中每个时刻的状态。遗忘门（Forget Gate）是让网络“忘记”之前没用的信息，上一时刻的状态值通过与自循环权重 (该权重就是遗忘门的输出) 进行位乘可得到当前时刻状态值的一个加数。遗忘门输出表达式：</p><script type="math/tex; mode=display">f_i^t=sigmoid(b_i^f+\sum U_{i,j}^fx_j^t+\sum W_{i,j}^fh_j^{t-1})</script><p>其中，$h^{t-1}$是包含一个LSTM细胞上一时刻的所有输出，可以被看作是当前隐藏层向量，数量为$j$，$W$是循环权重。要保存长期的记忆，还需要输入门，同样是根据$x^t$、$b^g$和$h^{t-1}$来决定哪些部分将进入当前时刻的状态，输入门的值表示为：</p><script type="math/tex; mode=display">g_i^t=sigmoid(b_i^g+\sum U_{i,j}^gx_j^t+\sum W_{i,j}^gh_j^{t-1})</script><p>进一步计算LSTM结构当前时刻的状态值：</p><script type="math/tex; mode=display">C_i^t=f_i^tC_i^{t-1}+g_i^ttanh(b_i+\sum U_{i,j}x_j^t+\sum W_{i,j}h_j^{t-1})</script><p>其中，$W$值为遗忘门的循环权重。然后得出结构当前时刻的输出值：</p><script type="math/tex; mode=display">h^t=tanh(C_i^t)q_i^t</script><script type="math/tex; mode=display">q_i^t=sigmoid(b_i^q+\sum U_{i,j}^qx_j^t+\sum W_{i,j}^qh_j^{t-1})</script><p>其中，$W^q$为遗忘门的循环权重，$q_i^t$为输出们的输出值。用LSTM作为循环体的RNN网络如下图所示：<br><img src="https://img-blog.csdnimg.cn/20200403192938761.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="LSTM-RNN"></p><p>　　循环神经网络的变种还有：双向循环神经网络和深层循环神经网络。</p><ul><li>至此，关于RNN和LSTM的学习就结束啦~<br>用pytorch实现LSTM的RNN对MNIST数据集分类，以及RNN实现一元二次曲线的回归的代码地址：<a href="https://gitee.com/sparklekk/RNN" target="_blank" rel="noopener">sparklekk</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度学习模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>循环神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>总结经典深度学习网络（pytorch、tensorflow实现）</title>
    <link href="/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/"/>
    <url>/2020/03/31/%E7%BB%93%E5%B8%B8%E7%94%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h2 id="总结经典的深度学习模型"><a href="#总结经典的深度学习模型" class="headerlink" title="总结经典的深度学习模型"></a>总结经典的深度学习模型</h2><p>参考书目：《深度学习算法原理与编程实战》 | 蒋子阳著</p><h3 id="一、LeNet-5"><a href="#一、LeNet-5" class="headerlink" title="一、LeNet-5"></a>一、LeNet-5</h3><p><strong>1、LeNet-5网络简介</strong> </p><p>　　LeNet-5是一个专为手写数字识别而设计的最经典的卷积神经网络，被誉为早期卷积神经网络中最有代表性的实验系统之一。LeNet-5 模型由Yann LeCun教授于1998年提出， 在MNIST数据集上，LeNet-5模型可以达到大约99.4%的准确率。与近几年的卷积神经网络比较，LeNet-5的网络规模比较小，但却包含了构成现代CNN网络的基本组件——卷积层、 Pooling层、全连接层。 &lt;/br&gt;<br><strong>2、模型结构</strong> &lt;/br&gt;<br>　　网络一共有8层(包含输入和输出在内)，基本网络架构如下：<br><img src="https://img-blog.csdnimg.cn/20200401163708326.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="ＬｅＮｅｔ－５"><br>备注：图中C表示卷积层 ，S表示池化层。　&lt;/br&gt;</p><ul><li><p><font color="red"><strong>Layer1</strong></font>：Input层，输入图片大小32×32×1，MNIST数据集大小是28×28，所以输入reshape为32×32是希望高层特征监测感受野的中心能够收集更多潜在的明显特征。</p></li><li><p><font color="red"><strong>Layer2</strong></font>：Conv1层——卷积层，有6个Feature Map，即Convolutions操作时的卷积核的个数为6，卷积核大小为5×5，该层每个单元和输入层的25个单元连接，padding没有使用，stride为1。</p></li><li><p><font color="red"><strong>Layer3</strong></font>：S2为下采样层(Subsampling)，6个14×14的特征图，是上一层的每个特征图经过2×2的最大池化操作得到的，且长宽步长都为2，S2层每个特征图的每一个单元与C3对应特征图中2×2大小的区域连接。</p></li><li><p><font color="red"><strong>Layer4</strong></font>：Conv3层，由上一层的特征图经过卷积操作得到，卷积核大小为5×5，卷积核个数为16，即该层有16个特征图，但并不是与上一层的6个特征图一一对应，而是有固定的连接关系，例如第一个特征图只与Layer3的第1、2、3个特征图的有卷积关系。</p></li><li><p><font color="red"><strong>Layer5</strong></font>：S4层，16个5×5的特征图，每个特征图都是由第四层经过一个2×2的最大池化操作得到的，意义同Layer3。</p></li><li><p><font color="red"><strong>Layer6</strong></font>：Conv5，120个特征图，是由上一层输出经过120个大小为5×5的卷积核得到的，没有padding，stride为1，上一层的16个特征图都连接到该层的每一个单元，所以这里相当于一个全连接层。</p></li><li><p><font color="red"><strong>Layer7</strong></font>：F6是全连接层，有84个神经元，与上一层构成全连接的关系，再经由Sigmoid激活函数传到输出层。</p></li><li><p><font color="red"><strong>Layer8</strong></font>：Output层也是一个全连接层，共有10个单元，对应0~9十个数字。本层单元计算的是径向基函数：$y<em>i =\sum</em>{j}(x-w_{i,j})^2$,RBF的计算与第i个数字的比特图编码有关，对于第i个单元，yi的值越接近0，则表示越接近第i个数字的比特编码，即识别当前输入的结果为第i个数字。</p></li></ul><p><strong>3、pytorch和tensorflow实现LeNet-5网络的MNIST手写数字识别</strong> &lt;/br&gt;<br>　　代码地址：<a href="https://github.com/KK-xi/LeNet-5" target="_blank" rel="noopener">GitHub</a> &lt;/br&gt;<br>　　LeNet-5网络规模比较小，所以它无法很好的处理类似ImageNet的比较大的图像数据集。</p><h3 id="二、AlexNet"><a href="#二、AlexNet" class="headerlink" title="二、AlexNet"></a>二、AlexNet</h3><p>1、<strong>AlexNet网络简介</strong> &lt;/br&gt;<br>　　2012年， Hinton的学生Alex Krizhevsky借助深度学习的相关理论提出了深度卷积神经网络模型AlexNet。卷积层的数量有5个，池化层的数量有3个，也就是说，并不是所有的卷积层后面都连接有池化层。在这些卷积与池化层之后是3个全连层，最后一个全连层的单元数量为1000个，用于完成对ImageNet数据集中的图片完成1000分类（具体分类通过Softmax层实现)。</p><p>2、<strong>模型结构</strong> &lt;/br&gt;　　<br>　　网络结构如下图所示，有两个子网络，可以用<font color="red">GPU分别进行训练</font>(特点1)，两个GPU之间存在通信：<br>　　<img src="https://img-blog.csdnimg.cn/20200401164504200.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="AlexNet"></p><ul><li><p><font color="red"><strong>第一段卷积</strong></font>: 使用了96个11×11卷积核对输入的224×224×3的RGB图像进行卷积操作，长宽移动步长均为4，得到的结果是96个55x55的特征图；得到基本的卷积数据后，第二个操作是<font color="red">ReLU去线性化</font>(这是AlexNet的特点2)；第三个操作是<font color="red">LRN局部归一化</font>（AlexNet首次提出,特点3)；第四个操作是3×3的max pooling，步长为2。</p></li><li><p><font color="red"><strong>第二段卷积</strong></font>：流程类似第一阶段，用到的核大小不同，256个深度为3(因为是三通道图像)的5×5的卷积核，stride为1×1，然后ReLU去线性化，再是LRN局部归一化，最后是3×3的最大池化操作，步长为2。</p></li><li><p><font color="red"><strong>第三段卷积</strong></font>：输入上一阶段的特征图，首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，这一阶段没有LRN和池化。</p></li><li><p><font color="red"><strong>第四段卷积</strong></font>：首先经过384个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化。</p></li><li><p><font color="red"><strong>第五段卷积</strong></font>：首先经过256个3×3的卷积核，步长参数为1，然后再经过ReLU去线性化，最后是3×3的最大池化操作，步长为2。</p></li><li><p><font color="red"><strong>第六段全连接(FC)</strong></font>：以上过程完了以后经过三层全连接层，前两层有4096个单元，与前一层构成全连接关系，然后都经过一个ReLU函数，左后一层是1000个单元的全连接层(Softmax层)，训练时这几个<font color="red">全连接层使用了Dropout</font>(特点4)。</p><p>备注：<font color="red">数据增强的运用</font>(特点5)，在训练的时候模型随机从大小为256×256的原始图像中截取224×224大小的区域，同时还得到图像水平翻转的镜像图，用以增加样本的数量。在测试时，模型会首先截取一张图片的四个角加中间的位置，并进行左右翻转，这样会获得10张图片，将这10张图片作为预测的输入并对得到的10个预测结果求均值，就是这张图片最终的预测结果。</p></li></ul><p><strong>3、pytorch和tensorflow实现</strong> &lt;/br&gt;<br>　　代码地址：<a href="https://github.com/KK-xi/AlexNet" target="_blank" rel="noopener">Github</a></p><h3 id="三、VGGNet"><a href="#三、VGGNet" class="headerlink" title="三、VGGNet"></a>三、VGGNet</h3><p><strong>1、VGGNet网络简介</strong> &lt;/br&gt;<br>　　2014年ILSVRC图像分类竞赛的第二名是VGGNet网络模型，其 top-5 错误率为 7.3%，它对卷积神经网络的深度与其性能之间的关系进行了探索。在将网络迁移到其他图片数据上进行应用时， VGGNet比GoogleNet有着更好的泛化性。此外，VGGNet模型是从图像中提取特征的CNN首选算法。&lt;/br&gt;</p><p><strong>2、模型结构</strong> &lt;/br&gt;<br>　　网络的结构非常简洁，在整个网络中全部使用了大小相同的卷积核 (3×3 )和 最大油化核 (2x2 )。<font color="red">VGGNet模型通过不断地加深网络结构来提升性能</font>，通过重复堆叠的方式，使用这些卷积层和最大池化层成功地搭建了11～19层深的卷积神经网络。下表是这些网络的结构组成层：<br><img src="https://img-blog.csdnimg.cn/20200401200252309.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGGNet"><br>　　表中的conv3表示大小为3×3的卷积核，conv1则是1×1的卷积核，参数量主要集中在全连接层，其他卷积层的参数共享和局部连接降低了参数的数量。表中五个阶段的卷积用于提取特征，每段卷积后面都有最大池化操作，目的是缩小图像尺寸。多个卷积的堆叠可以降低参数量，也有助于学习特征，C级的VGG用到conv1是为了在输入通道数和输出通道数不变(不发生数据降维)的情况下实现线性变换，对非线性提升效果有较好的作用，但是换成conv3效果更好。VGG19的效果只比VGG16好一点点，所以牛津的研究团队就停止在VGG19层了，不再增加更多层数了。下图是VGG-16的网络构架图：<br>　　<img src="https://img-blog.csdnimg.cn/20200401203149754.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="VGG-16"> </p><p><strong>3、pytorch和tensorflow实现</strong> &lt;/br&gt;<br>　　这里实现的是VGG-16网络(pytorch中只是搭建了网络)，代码地址：<a href="https://github.com/KK-xi/VGGNet-16" target="_blank" rel="noopener">GitHub</a><br>　　在通常的网络训练时，<font color="red">VGGNet通过Multi-Scale方法对图像进行数据增强处理</font>，我自己在训练MNIST的时候没有用数据增强。因为卷积神经网络对于图像的缩放有一定的不变性，所以将这种经过 Multi-Scale多尺度缩放裁剪后的图片合在一起输入到卷积神经网络中训练，可以增加网络的这种<font color="red">不变性</font>(不变性的理解：pooling操作时，对局部感受野取其极大值，如果图像在尺度上发生了变化，有一定概率在尺度变化后对应的感受野取到的极大值不变，这样就可以使特征图不变，同样也增加了一定的平移不变性)。在预测时也采用了Multi-Scale的方法，将图像Scale到一个尺寸再裁剪并输入到卷积网络计算。输入到网络中的图片是某一张图片经过缩放裁剪后的多个样本，这样会得到一张图片的多个分类结果，所以紧接着要做的事就是对这些分类结果进行平均以得到最后这张图片的分类结果，这种平均的方式会提高图片数据的利用率并使分类的效果变好。 </p><h3 id="四、InceptionNet-V3"><a href="#四、InceptionNet-V3" class="headerlink" title="四、InceptionNet-V3"></a>四、InceptionNet-V3</h3><p><strong>1、InceptionNet-V3网络简介</strong> &lt;/br&gt;<br>　Google的InceptionNet首次亮相是在2014年的ILSVRC比赛中，称为 Inception-V1，后来又开发了三个版本，其中InceptionNet-V3最具代表性。相比VGGNet, Inception-V1增加了深度，达到了22层，但是其参数却只有500万个左右(SM)，这是远低于AlexNet(60M 左右)和VGGNet (140M 左右)的，是因为该网络将全连层和一般的卷积中采用了<font color="red">稀疏连接的方法(Hebbian原理)</font>。根据相关性高的单元应该被聚集在一起的结论，这些在同一空间位置但在不同通道的卷积核的输出结果也是稀疏的，也可以通过类似将稀疏矩阵聚类为较为密集的子矩阵的方式来提高计算性能。沿着这样的一个思路，Google团队提出了Inception Module结构来实现这样的目标。 </p><p><strong>2、模型结构</strong> &lt;/br&gt;<br>　　网络中主要用到了稀疏连接的思想提出了<font color="red">Inception Module</font>，其借鉴了论文《Network in Network》的做法，即提出的<font color="red">MLPConv</font>——使用MLP对卷积操作得到的特征图进行进一步的操作，从而得到本层的最终输出特征图，这样可以允许在输出通道之间组合信息，以此提升卷积层的表达能力。在InceptionNet-V3中，Inception Module的基本结构如下图：　　<img src="https://img-blog.csdnimg.cn/20200401233024519.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="Inception Module"><br>结构中的<font color="red">Filter Concat</font>是将特征图在深度方向(channel维度)进行串联拼接，这样可以构建出符合Hebbian原理的稀疏结构。conv1的作用是把相关性高、同一空间位置不同通道的特征连接在一起，并且计算量小，<strong>可以增加一层特征变换和非线性化</strong>，最大池化是为了增加网络对不同尺度的适应性。在Inception V2中首次用到了<font color="red">.批标准化(Batch Normalization)</font>，V3在V2基础上的Module<font color="red">改进之处就是在分支中使用分支，将二维卷积拆分为两个非对称一维卷积</font>，这种卷积可以在处理更丰富的空间特征以及增加特征多样性等方面做得比普通卷积更好。多个这种Inception Module堆叠起来就形成了InceptionNet-V3，其网络构架如下图，整个网络的主要思想就是找到一个最优的Inception Module结构，更好的实现局部稀疏的稠密化。<br><img src="https://img-blog.csdnimg.cn/20200401235114326.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="InceptionNet-V3"></p><p><strong>3、使用InceptionNet-V3完成模型迁移学习</strong></p><ul><li><font color="red">迁移学习</font>：随着卷积神经网络模型层数的加深以及复杂度的逐渐增加，训练这些模型所需要的带有标注的数据也越来越多。比如对于ResNet，其深度有 152 层，使用ImageNet数据集中带有标注的 120 万张图片才能将其训练得到 96.5% 的准确率。尽管这是个比较不错的准确率，但是在真实应用中，几乎很难收集到这么多带有标注的图片数据，而且这些数据训练一个复杂的卷积神经网络也要花费很长的时间 。<strong>迁移学习的出现就是为了解决上述标注数据以及训练时间的问题</strong>。所谓迁移学习，就是将一个问题上训练好的模型通过简单的调整使其适用于 一个新的问题，例如在 Google 提供的基于 ImageNet 数据集训练好的 Inception V3 模型的基础上进行简单的修改，使其能够解决基于其他数据集的图片分类任务。被修改的全连接层之前的一个网络层叫做<font color="red">瓶颈层</font>(Bottleneck，这里是V3中最后一个Dropout层)。</li><li>模型迁移学习tensorflow代码地址：<a href="https://github.com/KK-xi/Inception_V3_transfer_learning" target="_blank" rel="noopener">Github</a></li></ul><h3 id="五、ResNet"><a href="#五、ResNet" class="headerlink" title="五、ResNet"></a>五、ResNet</h3><p><strong>1、ResNet网络简介</strong> &lt;/br&gt;<br>　　ResNet (Residual Neural Network）由微软研究院的何情明等 4 名华人提出，网络深度达到了152 层，top-5 错误率 只有3.57% 。虽然ResNet的深度远远高于 VGGNet，但是参数量却比 VGGNet 低，效果也更好。ResNet 中最创新就是<font color="red">残差学习单元（Residual Unit）</font>的引入，它是参考了瑞士教授 Schmidhuber 的论文《Training Very Deep Networks》中提出的 Highway Network。 <font color="red"> Highway Network</font> 的出现是为了解决较深的神经 网络难以训练的问题，主要思想是启发于LSTM的门(Gate)结构，使得有一定比例的前一层的信息没有经过矩阵乘法和非线性变换而是直接传输到下一层，网络要学习的就是原始信息应该以何种比例保留下来。后来残差网络的学习单元就受益于Highway Network加深网络层数的做法。</p><p><strong>2、模型结构</strong> &lt;/br&gt;<br>　　随着网络加深，由于反向传播过程的叠乘可能出现<font color="red">梯度消失</font>，结果就是准确率下降，为此ReNet中引入残差学习单元(如图所示，2层和3层)，思想就是对于一个达到了准确率饱和的较浅网络，在后面加几个全等映射层允许初始信息直接传到下一层(y=x)时，误差不会因此而增加，并且网络要学习的就是原来输出$H(x)$与原始输入$x$的残差$F(x)=H(x)-x$。<br><img src="https://img-blog.csdnimg.cn/20200402093002712.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_7#pic_center" srcset="/img/loading.gif" alt="残差单元"><br>　　将多个残差单元堆叠起来就组成了ResNet网络，如下图所示是一个34层的残差网络：<br><img src="https://img-blog.csdnimg.cn/20200402095125460.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" alt="ResNet"><br>旁边的支线就是将上一层残差单元的输出直接参与到该层的输出，这样的连接方式被称为<font color="red">Shortcut和Skip Connection</font>，这样可以一定程度上保护信息的完整性。最后在改进的ResNet V2中通过该连接使用的激活函数(ReLU)被更换为Identity Mappings ($y=x$)，残差单元都使用了BN归一化处理，使得训练更加容易并且泛化能力更强。</p><p><strong>3、pytorch和tensorflow实现ResNet</strong> &lt;/br&gt;<br>　代码地址：<a href="https://github.com/KK-xi/ResNet" target="_blank" rel="noopener">GitHub</a><br>　这里只是搭建的网络，没有进行任何任务的训练，可以在自己的数据集上训练试一下。</p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度学习模型</category>
      
    </categories>
    
    
    <tags>
      
      <tag>深度学习网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>总结SLAM相关知识点资源库以及企业</title>
    <link href="/2020/03/31/%E8%A7%86%E8%A7%89SLAM%E7%9F%A5%E8%AF%86%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E4%BC%81%E4%B8%9A%E6%80%BB%E7%BB%93/"/>
    <url>/2020/03/31/%E8%A7%86%E8%A7%89SLAM%E7%9F%A5%E8%AF%86%E8%B5%84%E6%BA%90%E7%9B%B8%E5%85%B3%E4%BC%81%E4%B8%9A%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h3 id="视觉SLAM的知识资源以及相关企业总结"><a href="#视觉SLAM的知识资源以及相关企业总结" class="headerlink" title="视觉SLAM的知识资源以及相关企业总结"></a>视觉SLAM的知识资源以及相关企业总结</h3><pre><code>先在这儿记录一下，SLAM涉及的太多了，现在接触的只是冰山一角。</code></pre><p>   声名：我只是微信公众号<font color="red">计算机视觉life</font>的搬运工。<br>   <bar><br>   <bar></bar></bar></p><bar><h4 id="一、SLAM知识库"><a href="#一、SLAM知识库" class="headerlink" title="一、SLAM知识库"></a>一、SLAM知识库</h4><p><strong>1、SLAM框架/算法流程</strong></p><p><a href="img-Nb7dyKIQ-1586231125936"></a><img src="https://img-blog.csdnimg.cn/20200407114420210.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70#pic_center" srcset="/img/loading.gif" width="50%"></p><p><strong>2、应掌握的数学知识</strong></p><ul><li><strong>矩阵</strong>：四则运算、求逆、反对称矩阵；矩阵分解（SVD、QR、Cholesky）。</li><li><strong>李群与李代数</strong>：指数对数映射；李代数求导；扰动模型。</li><li><strong>非线性优化</strong>：梯度下降；牛顿法；高斯牛顿法；LM算法；Bundle Adjustment。</li><li><strong>微积分</strong>：求(偏)导、泰勒展开等。</li></ul><p><strong>3、专业知识</strong> &lt;/br&gt;</p><ul><li><strong>计算机视觉</strong> &lt;/br&gt;<br>　<strong>①</strong> 传感器类型： &lt;/br&gt;<br>　　　激光雷达 &lt;/br&gt;<br>　　　视觉传感器：单目相机、双目相机、RGB-D相机、全景相机、Event相机。 &lt;/br&gt;<br>　　　IMU (Inertial measurement unit，测量物体三轴姿态角及加速度的装置) &lt;/br&gt;<br>　<strong>②</strong> 相机： &lt;/br&gt;<br>　　　针孔相机模型、双目相机模型、RGB-D相机模型、相机标定、<br>去畸变。 &lt;/br&gt;<br>　<strong>③</strong> 特征点 &lt;/br&gt;<br>　　　特征点检测、特征点描述子、特征点匹配、特征点筛选。 &lt;/br&gt;<br>　<strong>④</strong> 多视角几何 &lt;/br&gt;<br>　　　对极约束、本质矩阵、单应矩阵、三角化。 &lt;/br&gt;　　　</li><li><strong>书籍文献</strong>  &lt;/br&gt;<br>　《视觉SLAM十四讲》、《Multiple View Geometry》、《机器人状态估计》</li></ul><p><strong>4、编程环境</strong></p><ul><li>Linux系统操作：推荐Ubuntu16.04 &lt;/br&gt;<br>　　　　　　　　书《鸟哥的Linux私房菜》　　　　　　　　</li><li>开发环境：Clion（JetBrains推出的C/C++跨平台集成开发环境） &lt;/br&gt;<br>　　　　　　Kdevelop（免费的）　　　　　　</li><li><p>编译工具：学习跨平台编译器cmake、电子书《Cmake Practice》</p></li><li><p>第三方库：Opencv(计算机视觉)、Eigen(几何变换)、Sophus(李代数)、Ceres(非线性优化)、G2o(图优化)、OpenGL(计算机图形学)　　　　　　</p></li><li>文档编辑：Gedit、Vim、Nano</li></ul><p><strong>5、应用场景</strong></p><ul><li><p>自动驾驶：作为激光传感器的辅助（百度、腾讯、驭势、图森）</p></li><li><p>增强现实：手机、智能眼镜上结合IMU用于定位（微软、三星、华为、虹软、悉见）</p></li><li><p>机器人：无人机定位、建立地图（大疆）；服务机器人室内定位及导航（思岚）；工业机器人定位导航（阿里、京东AGV仓库运输）</p></li><li><p>三维重建：物体重建（3D打印、3D虚拟试衣）；大场景重建（虚拟全景漫游）</p></li></ul><p><strong>6、公开数据集</strong> &lt;/br&gt;<br>　　TUM RGB-D SLAM Dataset、KITTI Vision Benchmark Suite、EuRoC MAV Dataset.</p><p><strong>7、典型开源方案</strong></p><ul><li>稀疏法：ORB-SLAM2（单目、双目、RGB-D）</li><li>半稠密法：LSD-SLAM（单目、双目、RGB-D）、DSO（单目）</li><li>稠密法：Elastic Fusion（RGB-D）、Bundle Fusion（RGB-D）、<br>　　　　　RGB-D SLAM V2（RGB-D）</li><li>多传感器融合：VINS（单目+IMU）、OKVIS（单目、双目、四目+IMU）</li></ul><h4 id="二、SLAM学习资源总结"><a href="#二、SLAM学习资源总结" class="headerlink" title="二、SLAM学习资源总结"></a>二、SLAM学习资源总结</h4><p><strong>1、公众号</strong> &lt;/br&gt;<br>　　泡泡机器人SLAM、计算机视觉life、3D视觉工坊、小白学视觉、计算机视觉之路、AI算法修炼营、PCL点云。 　</p><p>  <strong>2、视频公开课</strong> &lt;/br&gt;<br>　　① <a href="https://space.bilibili.com/38737757" target="_blank" rel="noopener">泡泡机器人公开课</a> &lt;/br&gt;<br>　　② <a href="https://space.bilibili.com/45189691" target="_blank" rel="noopener">计算机视觉life SLAM研习社直播公开课</a> &lt;/br&gt;<br>　　③ <a href="https://www.dis.uniroma1.it/~labrococo/tutorial_icra_2016/" target="_blank" rel="noopener">SLAM Totorial@ICRA 2016</a> &lt;/br&gt;<br>　　④ <a href="https://www.coursera.org/specializations/robotics" target="_blank" rel="noopener">Robotics-UPenn on Coursera by Vijay Kumar(2016)</a> &lt;/br&gt;<br>　　⑤ <a href="https://vision.in.tum.de/teaching/ss2016/mvg2016" target="_blank" rel="noopener">Computer Vision Ⅱ：Multiple View Geometry-TUM by Daniel Cremers (Spring 2016)</a> &lt;/br&gt;<br>　　⑥ <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa15/" target="_blank" rel="noopener">Advanced Robotics-UCBerkeley by Pieter Abbeel (Fall 2015)</a> &lt;/br&gt;<br>　　⑦ <a href="https://ylatif.github.io/movingsensors/" target="_blank" rel="noopener">The Problem of Mobile Sensors</a> &lt;/br&gt;</p><p><strong>3、主要研究方向</strong> &lt;/br&gt;<br>　　视觉里程计：线特征；点线特征融合；点面特征融合；点线面多特征融合；多鱼眼VO；特征点发直接法结合；抗光照变化。 &lt;/br&gt;</p><p>　　语义SLAM：3D语义地图重定位；语义建图互相促进。 &lt;/br&gt;</p><p>　　多传感器融合：VIO（松耦合、紧耦合）；多传感器在线标定；camera，lidar融合定位；lidar，天花板摄像头融合；lidar，imu融合；TOF辅助双目密集匹配；事件相机SLAM；多机协同SLAM。 &lt;/br&gt;</p><p>优化方法：非线性增量优化；BA改进；离散连续图模型优化；边缘的约束图优化。 &lt;/br&gt;</p><p>　　三维重建：大尺度场景下的实时重建；RGBD室内稠密重建；地图数据关联；MVS；3D物体分类，提升重建质量；多子图融合。 &lt;/br&gt;</p><p>　　结合深度学习：基于深度学习的特征点；结合深度学习的深度估计；深度学习做闭环检测；深度学习估计关键帧深度。</p><p><strong>4、会议期刊</strong></p><ul><li><p>英文期刊 &lt;/br&gt;<br>ICRA（IEEE International Conference on Robotics and Automation） &lt;/br&gt;<br>IROS（IEEE International Conference on Intelligent Robots and Systems） &lt;/br&gt;<br>CVPR（IEEE International Conference on Computer Vision and Pattern Recognition） &lt;/br&gt;<br>RSS（Robotics：Science and Systems） &lt;/br&gt;<br>ECCV（European Conference on Computer Vision） &lt;/br&gt;<br>ISMAR（IEEE and ACM International Symposium on Mixed and Augmented Reality.IEEE） &lt;/br&gt;<br>JFR（Journal of Field Robotics） &lt;/br&gt;<br>IEEE Transactions on Robotics &lt;/br&gt;<br>ACRA（Australian Conference on Robotics and Automation） &lt;/br&gt;<br>ICARCV（International Conference on Cotrol，Automation，Robotics and Vision） &lt;/br&gt;<br>ISR（International Symposium on Robotics） &lt;/br&gt;<br>IEEE Transactions On Pattern Analysis And achine Intelligence &lt;/br&gt;</p></li><li><p>中文期刊 &lt;/br&gt;<br>计算机辅助设计与图形学学报 &lt;/br&gt;<br>机器人 &lt;/br&gt;<br>计算机应用研究 &lt;/br&gt;<br>中国科学：信息科学 &lt;/br&gt;</p></li></ul><p><strong>5、知名研究实验室</strong></p><ul><li><p>欧洲 &lt;/br&gt;<br>苏黎世联邦理工学院的Autonomous System Lab &lt;/br&gt;<br>苏黎世大学Robotics and Perception Group &lt;/br&gt;<br>慕尼黑工业大学The Computer Vision Group &lt;/br&gt;<br>英国伦敦大学帝国理工学院 Dyson 机器人实验室 &lt;/br&gt;<br>英国牛津大学Active Vision Laboratory &lt;/br&gt;<br>德国弗莱堡大学Autonomous Intelligent Systems &lt;/br&gt;<br>西班牙萨拉戈萨大学SLAM实验室 &lt;/br&gt;</p></li><li><p>北美 &lt;/br&gt;<br>麻省理工计算机科学与人工智能实验室（CSAIL）海洋机器人组 &lt;/br&gt;<br>明尼苏达大学Multiple Autonomous Robotics Systems Laboratory &lt;/br&gt;<br>宾夕法尼亚大学GRASP实验室 &lt;/br&gt;<br>华盛顿大学UW Robotics and Estimation Lab &lt;/br&gt;<br>哥伦比亚大学计算机视觉与机器人组 &lt;/br&gt;<br>加拿大谢布鲁克大学IntRoLab &lt;/br&gt;<br>斯坦福大学人工智能实验室自动驾驶团队 &lt;/br&gt;<br>卡内基梅隆大学Robot Perception Lab &lt;/br&gt;<br>特拉华大学Robot Perception and Navigation Group</p></li><li><p>亚洲 &lt;/br&gt;<br>香港科技大学Aerial Robotics Group &lt;/br&gt;<br>浙江大学CAD&amp;CG国家重点实验室Computer Vision Group &lt;/br&gt;<br>清华大学自动化系宽带网络与数字媒体实验室BBNC &lt;/br&gt;<br>中科院自动化研究所国家模式识别实验室Robot Vision Group &lt;/br&gt;<br>上海交通大学感知与导航研究所 &lt;/br&gt;<br>武汉大学Computer Vision &amp; Remote Sensing Lab &lt;/br&gt;<br>日本先进工业科技研究所 &lt;/br&gt;<br>筑波大学智能机器人研究室 &lt;/br&gt;<br>新加坡南洋理工大学HESL实验室 &lt;/br&gt;<br>韩国科学技术研究院 &lt;/br&gt;</p></li><li><p>澳洲 &lt;/br&gt;<br>澳大利亚悉尼科技大数学CAS实验室 &lt;/br&gt;<br>澳大利亚机器学习研究所机器人视觉中心 &lt;/br&gt;</p></li></ul><h4 id="三、SLAM相关企业"><a href="#三、SLAM相关企业" class="headerlink" title="三、SLAM相关企业"></a>三、SLAM相关企业</h4><p><strong>1、移动机器人</strong></p><ul><li><p>扫地机器人 &lt;/br&gt;<br>科沃斯（苏州）、石头（北京）、追觅（上海）、银星智能（深圳）</p></li><li><p>服务机器人 &lt;/br&gt;<br>优必选（深圳）、达闼（北京）、思岚（上海）、高仙（上海）、猎户星空（北京）、速感（北京）、普渡（深圳）、美团（北京）</p></li><li><p>仓储机器人 &lt;/br&gt;<br>旷视（北京、上海）、京东（北京）、顺丰（深圳）、海康威视（杭州）、极智嘉（北京）</p></li></ul><p><strong>2、相机传感器</strong></p><ul><li><p>双目相机 &lt;/br&gt;<br>小觅（无锡）、Indemind（北京）、爱观（上海）、中科慧眼（北京）</p></li><li><p>RGBD相机 &lt;/br&gt;<br>Intel（北京、上海）、奥比中光（深圳）、Pico（青岛、北京）、图漾（上海）、云从（重庆）</p></li><li><p>激光雷达 &lt;/br&gt;<br>禾赛（上海）、镭神智能（深圳）、速腾聚创（深圳）</p></li><li><p>事件相机 &lt;/br&gt;<br>芯仑（上海）</p></li></ul><p><strong>3、无人机</strong> &lt;/br&gt;<br>　　大疆（深圳、上海）、亿航（广州）、臻迪（北京）</p><font color="red">**4、智能驾驶**</font><ul><li><p>无人驾驶 &lt;/br&gt;<br>百度（北京）、阿里菜鸟/达摩院（杭州）、腾讯（北京）、驭势（北京、上海）、momenta（北京）、滴滴（北京）、图森（北京）、飞步科技（杭州）、纽励科技（上海）、小马智行（广州）、Aptiv（上海）、文远知行（广州）</p></li><li><p>辅助驾驶 &lt;/br&gt;<br>纵目科技（上海）、魔视智能（上海）、极目智能（武汉）、虹软（杭州）、商汤（北京）</p></li><li><p>芯片 &lt;/br&gt;<br>NVIDIA（上海）、地平线（北京、南京）</p></li><li><p>高精地图 &lt;/br&gt;<br>高德（北京）、四维图新（北京）</p></li><li><p>汽车厂商 &lt;/br&gt;<br>上汽研究院（上海）、蔚来汽车（上海）、小鹏汽车（广州）、宇通客车（郑州）</p></li></ul><p><strong>5、增强现实</strong></p><ul><li><p>智能眼镜 &lt;/br&gt;<br>联想（上海）、视辰（上海）、肇观电子（上海）、悉见（北京）</p></li><li><p>手机  &lt;/br&gt;<br>华为（上海）、虹软（杭州、上海）、商汤（杭州）、亮风台（上海）、今日头条（北京）</p></li></ul></bar>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>SLAM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>图像处理注意力机制</title>
    <link href="/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2020/03/30/%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h3 id="记录最近所看的注意力机制"><a href="#记录最近所看的注意力机制" class="headerlink" title="记录最近所看的注意力机制"></a>记录最近所看的注意力机制</h3>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>attention mechanism</category>
      
    </categories>
    
    
    <tags>
      
      <tag>attention mechanism</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>视觉SLAM十四讲</title>
    <link href="/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/"/>
    <url>/2020/03/30/SLAM%E5%8D%81%E5%9B%9B%E8%AE%B2/</url>
    
    <content type="html"><![CDATA[<p>第一章 </p>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>SLAM</category>
      
    </categories>
    
    
    <tags>
      
      <tag>视觉SLAM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>seu</title>
    <link href="/2020/03/30/eu/"/>
    <url>/2020/03/30/eu/</url>
    
    <content type="html"><![CDATA[<ul><li>2019.10.06 东大一角</li></ul><p><img src="https://img-blog.csdnimg.cn/20200330115513856.jpg?type_ZmFuZ3poZW5naGVpdGk,shadow_1,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_1,color_FFFFFF,t_10" srcset="/img/loading.gif" alt="seu"></p>]]></content>
    
    
    <categories>
      
      <category>life</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>单目深度估计总结</title>
    <link href="/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/"/>
    <url>/2020/03/29/%E6%B7%B1%E5%BA%A6%E4%BC%B0%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<p><bar><br> <bar></bar></bar></p><p><bar></bar></p><bar><p>《High Quality Monocular Depth Estimation via Transfer Learning》<br>作者：Ibraheem Alhashim and Peter Wonka</p><p>备注：只是一篇总结，不是解读哒~</p><h2 id="一、为什么要看这篇文章？"><a href="#一、为什么要看这篇文章？" class="headerlink" title="一、为什么要看这篇文章？"></a>一、为什么要看这篇文章？</h2><p>   1、因为最近萌生了一个想法，觉得可以用用深度图；<br>   2、多了解一样是一样。</p><h2 id="二、文章提出的出发点"><a href="#二、文章提出的出发点" class="headerlink" title="二、文章提出的出发点"></a>二、文章提出的出发点</h2><p>   1、首先，一张图片2D到3D的深度估计是很多场景理解或者是重建工作中的基础；<br>   2、其次，这篇文章希望提出的深度估计方法能获得高分辨率的深度估计结果。</p><h2 id="三、Framework"><a href="#三、Framework" class="headerlink" title="三、Framework"></a>三、Framework</h2><p>   文章中给出的简化架构：<img src="https://img-blog.csdnimg.cn/20200314154230721.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="看似很简单的网络架构"><br>  <strong>1、Encoder</strong><br>   这里用到的编码器是另一篇文章《Densely Connected Convolutional Networks》里的DenseNet-169（这篇文章就看了一下架构，还没细看，有时间去仔细看看，看了回来补充）。下图是生长率k=4的5层的Dense-block结构图，对这里的k的一种解释是，每个层都可以访问这个块中的所有前面的特征映射，因此也可以访问网络的集体特征。可以将特征图视为网络的全局状态，每层将其自身的k个特征映射添加到该状态，增长率决定了每一层对全局状态贡献多少新信息。一旦写入全局状态，就可以从网络中的任何地方访问全局状态，并且与传统的网络架构不同，不需要从一层复制到另一层。<br> <img src="https://img-blog.csdnimg.cn/20200314160111493.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="5-layer dense block with growth rate of k=4"><br> 然后，为了在网络中进行下采样（改变图像大小），将多个Dense-block连接起来，就形成了下图的样子：<br> <img src="https://img-blog.csdnimg.cn/20200314161447623.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="Dense Net with three dense blocks "><br> 两个相邻块之间的层称为过渡层，并通过卷积和池化来改变特征图的大小，但是本文中是删除了最后的top-layer，因为文章是做depth estimation而不是Classification task。但是呢，深度估计这篇文章用的DensNet-169有4个blocks，网络结构参数如下：<br> <img src="https://img-blog.csdnimg.cn/20200314162029675.png?type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNjM5MDc3,size_16,color_FFFFFF,t_70" srcset="/img/loading.gif" alt="DenseNet architectures for ImageNet"></p><p>   <strong>2、Decoder</strong><br>   对于decoder，从一个1×1的卷积层开始，其输出通道的数量与去掉top-layer的编码器的输出相同。然后依次添加2×2的双线性upsampling块和串接池化层POOL（框架图中并未画出），其后跟两个并列的3×3卷积层，这样的构造重复3次，然后就是2×2的双线性upsampling块和串接卷积层CONV，然后是两个并列的3×3的卷积层，最后经过一个3×3卷积层最终输出通道数为1的图像。</p><p>   这个encoder-decoder带有跳过连接，感觉就是把两部分硬生生连在一起了，整个网络构架不太紧凑，大概的结构就这样啦~~</p><h2 id="四、文章的亮点"><a href="#四、文章的亮点" class="headerlink" title="四、文章的亮点"></a>四、文章的亮点</h2><p>   1、第一点大概就是用了这么一个简单的网络来做深度估计，网络复杂不等于结果好；<br>   2、定义了一个损失函数，通过最小化深度值的差异来平衡重建深度图像之间的关系，同时惩罚深度图的图像域中高频细节的失真：<br><img src="https://img-blog.csdnimg.cn/20200314165259760.png" srcset="/img/loading.gif" alt="损失函数L"><br>3、运用数据增强策略，文章只用了镜像翻转，以及改变颜色通道排列，后者还可以做一个further work；<br>4、提出了一个新的test dataset（用不上，所以没看）。</p><h2 id="五、结果以及改进空间"><a href="#五、结果以及改进空间" class="headerlink" title="五、结果以及改进空间"></a>五、结果以及改进空间</h2><p>   1、在室外场景中没别人的方法表现好，猜想是因为提供的深度图的性质；<br>   2、文章所提的改进空间挺多的，比如用在嵌入式设备上，这个网络存在局限性，以及更清楚地确定不同编码器、数据增强和学习策略对性能和贡献的影响，都是未来工作中值得关注的内容。</p></bar>]]></content>
    
    
    <categories>
      
      <category>work</category>
      
      <category>深度估计</category>
      
    </categories>
    
    
    <tags>
      
      <tag>paper conclusion</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
